# ä¸€ã€ç®€ä»‹

> Spark Streaming æ˜¯ Spark çš„ä¸€ä¸ªå­æ¨¡å—ï¼Œç”¨äºå¿«é€Ÿæ„å»ºå¯æ‰©å±•ï¼Œé«˜ååé‡ï¼Œé«˜å®¹é”™çš„æµå¤„ç†ç¨‹åº





## ç‰¹ç‚¹

- é€šè¿‡é«˜çº§ API æ„å»ºåº”ç”¨ç¨‹åºï¼Œç®€å•æ˜“ç”¨ï¼›
- æ”¯æŒå¤šç§è¯­è¨€ï¼Œå¦‚ Javaï¼ŒScala å’Œ Pythonï¼›
- è‰¯å¥½çš„å®¹é”™æ€§ï¼ŒSpark Streaming æ”¯æŒå¿«é€Ÿä»å¤±è´¥ä¸­æ¢å¤ä¸¢å¤±çš„æ“ä½œçŠ¶æ€ï¼›
- èƒ½å¤Ÿå’Œ Spark å…¶ä»–æ¨¡å—æ— ç¼é›†æˆï¼Œå°†æµå¤„ç†ä¸æ‰¹å¤„ç†å®Œç¾ç»“åˆï¼›
- Spark Streaming å¯ä»¥ä» HDFSï¼ŒFlumeï¼ŒKafkaï¼ŒTwitter å’Œ ZeroMQ è¯»å–æ•°æ®ï¼Œä¹Ÿæ”¯æŒè‡ªå®šä¹‰æ•°æ®æºã€‚

[![img](../image/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f737061726b2d73747265616d696e672d617263682e706e67.png)](https://camo.githubusercontent.com/bdc8fef2e08b4b88cc32da3c22752ef2b7557c5d9df3fd20950ccf1eec5e58be/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f737061726b2d73747265616d696e672d617263682e706e67)





## DStream

> Spark Streaming æä¾›ç§°ä¸ºç¦»æ•£æµ (DStream) çš„é«˜çº§æŠ½è±¡ï¼Œç”¨äºè¡¨ç¤ºè¿ç»­çš„æ•°æ®æµã€‚

-  DStream å¯ä»¥ä»æ¥è‡ª Kafkaï¼ŒFlume å’Œ Kinesis ç­‰æ•°æ®æºçš„è¾“å…¥æ•°æ®æµåˆ›å»ºï¼Œ
- ä¹Ÿå¯ä»¥ç”±å…¶ä»– DStream è½¬åŒ–è€Œæ¥ã€‚==åœ¨å†…éƒ¨ï¼ŒDStream è¡¨ç¤ºä¸ºä¸€ç³»åˆ— RDD==

[![img](../image/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f737061726b2d73747265616d696e672d666c6f772e706e67.png)](https://camo.githubusercontent.com/ddff4927bfe9c2c409b0448817415b87dedc36361e37aca8fb47e79010679e24/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f737061726b2d73747265616d696e672d666c6f772e706e67)



## æ‰¹å¤„ç†ä¸æµå¤„ç†





### æ‰¹å¤„ç†

åœ¨æµå¤„ç†ä¹‹å‰ï¼Œæ•°æ®é€šå¸¸å­˜å‚¨åœ¨æ•°æ®åº“ï¼Œæ–‡ä»¶ç³»ç»Ÿæˆ–å…¶ä»–å½¢å¼çš„å­˜å‚¨ç³»ç»Ÿä¸­ã€‚åº”ç”¨ç¨‹åºæ ¹æ®éœ€è¦æŸ¥è¯¢æ•°æ®æˆ–è®¡ç®—æ•°æ®ã€‚è¿™å°±æ˜¯ä¼ ç»Ÿçš„é™æ€æ•°æ®å¤„ç†æ¶æ„ã€‚Hadoop é‡‡ç”¨ HDFS è¿›è¡Œæ•°æ®å­˜å‚¨ï¼Œé‡‡ç”¨ MapReduce è¿›è¡Œæ•°æ®æŸ¥è¯¢æˆ–åˆ†æï¼Œè¿™å°±æ˜¯å…¸å‹çš„é™æ€æ•°æ®å¤„ç†æ¶æ„ã€‚

[![img](../image/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f30315f646174615f61745f726573745f696e6672617374727563747572652e706e67.png)](https://camo.githubusercontent.com/973240254714d6016213968084d544c49d396a17875790328d6825c0ddd17b6f/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f30315f646174615f61745f726573745f696e6672617374727563747572652e706e67)

###  æµå¤„ç†

è€Œæµå¤„ç†åˆ™æ˜¯ç›´æ¥å¯¹è¿åŠ¨ä¸­çš„æ•°æ®çš„å¤„ç†ï¼Œåœ¨æ¥æ”¶æ•°æ®æ—¶ç›´æ¥è®¡ç®—æ•°æ®ã€‚

å¤§å¤šæ•°æ•°æ®éƒ½æ˜¯è¿ç»­çš„æµï¼šä¼ æ„Ÿå™¨äº‹ä»¶ï¼Œç½‘ç«™ä¸Šçš„ç”¨æˆ·æ´»åŠ¨ï¼Œé‡‘èäº¤æ˜“ç­‰ç­‰ ï¼Œæ‰€æœ‰è¿™äº›æ•°æ®éƒ½æ˜¯éšç€æ—¶é—´çš„æ¨ç§»è€Œåˆ›å»ºçš„ã€‚

æ¥æ”¶å’Œå‘é€æ•°æ®æµå¹¶æ‰§è¡Œåº”ç”¨ç¨‹åºæˆ–åˆ†æé€»è¾‘çš„ç³»ç»Ÿç§°ä¸º**æµå¤„ç†å™¨**ã€‚æµå¤„ç†å™¨çš„åŸºæœ¬èŒè´£æ˜¯ç¡®ä¿æ•°æ®æœ‰æ•ˆæµåŠ¨ï¼ŒåŒæ—¶å…·å¤‡å¯æ‰©å±•æ€§å’Œå®¹é”™èƒ½åŠ›ï¼ŒStorm å’Œ Flink å°±æ˜¯å…¶ä»£è¡¨æ€§çš„å®ç°ã€‚

![img](../image/12345.png)

æµå¤„ç†å¸¦æ¥äº†é™æ€æ•°æ®å¤„ç†æ‰€ä¸å…·å¤‡çš„ä¼—å¤šä¼˜ç‚¹ï¼š

- **åº”ç”¨ç¨‹åºç«‹å³å¯¹æ•°æ®åšå‡ºååº”**
  - é™ä½äº†æ•°æ®çš„æ»åæ€§ï¼Œä½¿å¾—æ•°æ®æ›´å…·æœ‰æ—¶æ•ˆæ€§ï¼Œæ›´èƒ½åæ˜ å¯¹æœªæ¥çš„é¢„æœŸï¼›
- **æµå¤„ç†å¯ä»¥å¤„ç†æ›´å¤§çš„æ•°æ®é‡**ï¼š
  - ç›´æ¥å¤„ç†æ•°æ®æµï¼Œå¹¶ä¸”åªä¿ç•™æ•°æ®ä¸­æœ‰æ„ä¹‰çš„å­é›†ï¼Œå¹¶å°†å…¶ä¼ é€åˆ°ä¸‹ä¸€ä¸ªå¤„ç†å•å…ƒï¼Œé€çº§è¿‡æ»¤æ•°æ®ï¼Œé™ä½éœ€è¦å¤„ç†çš„æ•°æ®é‡ï¼Œä»è€Œèƒ½å¤Ÿæ‰¿å—æ›´å¤§çš„æ•°æ®é‡ï¼›
- **æµå¤„ç†æ›´è´´è¿‘ç°å®çš„æ•°æ®æ¨¡å‹**ï¼š
  - åœ¨å®é™…çš„ç¯å¢ƒä¸­ï¼Œä¸€åˆ‡æ•°æ®éƒ½æ˜¯æŒç»­å˜åŒ–çš„ï¼Œè¦æƒ³èƒ½å¤Ÿé€šè¿‡è¿‡å»çš„æ•°æ®æ¨æ–­æœªæ¥çš„è¶‹åŠ¿ï¼Œå¿…é¡»ä¿è¯æ•°æ®çš„ä¸æ–­è¾“å…¥å’Œæ¨¡å‹çš„ä¸æ–­ä¿®æ­£ï¼Œå…¸å‹çš„å°±æ˜¯é‡‘èå¸‚åœºã€è‚¡ç¥¨å¸‚åœºï¼Œæµå¤„ç†èƒ½æ›´å¥½çš„åº”å¯¹è¿™äº›æ•°æ®çš„è¿ç»­æ€§çš„ç‰¹å¾å’ŒåŠæ—¶æ€§çš„éœ€æ±‚ï¼›
- **æµå¤„ç†åˆ†æ•£å’Œåˆ†ç¦»åŸºç¡€è®¾æ–½**ï¼š
  - æµå¼å¤„ç†å‡å°‘äº†å¯¹å¤§å‹æ•°æ®åº“çš„éœ€æ±‚ã€‚ç›¸åï¼Œæ¯ä¸ªæµå¤„ç†ç¨‹åºé€šè¿‡æµå¤„ç†æ¡†æ¶ç»´æŠ¤äº†è‡ªå·±çš„æ•°æ®å’ŒçŠ¶æ€ï¼Œè¿™ä½¿å¾—æµå¤„ç†ç¨‹åºæ›´é€‚åˆå¾®æœåŠ¡æ¶æ„ã€‚





## Arch

![image-20220705153834280](../image/image-20220705153834280.png)

**ç®€åŒ–ç‰ˆğŸ‘‡**

![image-20220705153903186](../image/image-20220705153903186.png)



## èƒŒå‹æœºåˆ¶

> ä¸€å¥è¯ï¼šåŠ¨æ€æ§åˆ¶æ•°æ®æ¥æ”¶é€Ÿç‡æ¥é€‚é…é›†ç¾¤æ•°æ®å¤„ç†èƒ½åŠ›

**å†å²**

- Spark 1.5ä»¥å‰ç‰ˆæœ¬ï¼Œç”¨æˆ·å¦‚æœè¦é™åˆ¶Receiverçš„æ•°æ®æ¥æ”¶é€Ÿç‡ï¼Œå¯ä»¥é€šè¿‡è®¾ç½®é™æ€é…åˆ¶å‚æ•°â€œ`spark.streaming.receiver.maxRate`â€çš„å€¼æ¥å®ç°ï¼Œæ­¤ä¸¾è™½ç„¶å¯ä»¥é€šè¿‡é™åˆ¶æ¥æ”¶é€Ÿç‡ï¼Œæ¥é€‚é…å½“å‰çš„å¤„ç†èƒ½åŠ›ï¼Œé˜²æ­¢å†…å­˜æº¢å‡ºï¼Œä½†ä¹Ÿä¼šå¼•å…¥å…¶å®ƒé—®é¢˜ã€‚

  æ¯”å¦‚ï¼šproduceræ•°æ®ç”Ÿäº§é«˜äºmaxRateï¼Œå½“å‰é›†ç¾¤å¤„ç†èƒ½åŠ›ä¹Ÿé«˜äºmaxRateï¼Œè¿™å°±ä¼šé€ æˆèµ„æºåˆ©ç”¨ç‡ä¸‹é™ç­‰é—®é¢˜



**ç›®çš„**

- ä¸ºäº†æ›´å¥½çš„åè°ƒæ•°æ®æ¥æ”¶é€Ÿç‡ä¸èµ„æºå¤„ç†èƒ½åŠ›

  1.5ç‰ˆæœ¬å¼€å§‹Spark Streamingå¯ä»¥åŠ¨æ€æ§åˆ¶æ•°æ®æ¥æ”¶é€Ÿç‡æ¥é€‚é…é›†ç¾¤æ•°æ®å¤„ç†èƒ½åŠ›



**è§£å†³**

- [^èƒŒå‹æœºåˆ¶ï¼ˆå³Spark Streaming Backpressureï¼‰]:æ ¹æ®JobScheduleråé¦ˆä½œä¸šçš„æ‰§è¡Œä¿¡æ¯æ¥åŠ¨æ€è°ƒæ•´Receiveræ•°æ®æ¥æ”¶ç‡ã€‚

- **ä½¿ç”¨æ–¹æ³•**

  [^å±æ€§â€œspark.streaming.backpressure.enabledâ€]: æ§åˆ¶æ˜¯å¦å¯ç”¨backpressureæœºåˆ¶ï¼Œé»˜è®¤å€¼falseï¼Œå³ä¸å¯ç”¨ã€‚

  





# äºŒã€DStreamåˆ›å»º

## StreamingContext

> Spark Streaming ç¼–ç¨‹çš„å…¥å£ç±» StreamingContext

Spark æµå¤„ç†æœ¬è´¨æ˜¯å°†æµæ•°æ®æ‹†åˆ†ä¸ºä¸€ä¸ªä¸ªæ‰¹æ¬¡ï¼Œç„¶åè¿›è¡Œå¾®æ‰¹å¤„ç†



**åˆ›å»ºStreamingContextçš„ä¸¤ä¸ªå‚æ•°**

- ```scala
  val sparkConf = new SparkConf().setMaster("local[*]").setAppName("SparkStreaming")
  // ç¬¬ä¸€ä¸ªå‚æ•°è¡¨ç¤ºç¯å¢ƒé…ç½®
  // ç¬¬äºŒä¸ªå‚æ•°è¡¨ç¤ºæ‰¹é‡å¤„ç†çš„å‘¨æœŸï¼ˆé‡‡é›†å‘¨æœŸï¼‰
  val ssc = new StreamingContext(sparkConf, Seconds(3))
  ```

| å‚æ•°è¯´æ˜      |                    |
| ------------- | ------------------ |
| sparkConf     | é…ç½®ä¿¡æ¯           |
| batchDuration | æ‰¹æ¬¡æ‹†åˆ†çš„æ—¶é—´é—´éš” |



[^æ³¨æ„]: ä½¿ç”¨çš„æ˜¯æœ¬åœ°æ¨¡å¼ï¼Œé…ç½®ä¸º `local[2]`ï¼Œè¿™é‡Œä¸èƒ½é…ç½®ä¸º `local[1]`ã€‚è¿™æ˜¯å› ä¸ºå¯¹äºæµæ•°æ®çš„å¤„ç†ï¼ŒSpark å¿…é¡»æœ‰ä¸€ä¸ªç‹¬ç«‹çš„ Executor æ¥æ¥æ”¶æ•°æ®ï¼Œç„¶åå†ç”±å…¶ä»–çš„ Executors æ¥å¤„ç†ï¼Œæ‰€ä»¥ä¸ºäº†ä¿è¯æ•°æ®èƒ½å¤Ÿè¢«å¤„ç†ï¼Œè‡³å°‘è¦æœ‰ 2 ä¸ª Executorsã€‚è¿™é‡Œæˆ‘ä»¬çš„ç¨‹åºåªæœ‰ä¸€ä¸ªæ•°æ®æµï¼Œåœ¨å¹¶è¡Œè¯»å–å¤šä¸ªæ•°æ®æµçš„æ—¶å€™ï¼Œä¹Ÿéœ€è¦ä¿è¯æœ‰è¶³å¤Ÿçš„ Executors æ¥æ¥æ”¶å’Œå¤„ç†æ•°æ®ã€‚



## æœåŠ¡çš„å¯åŠ¨ä¸åœæ­¢



| è¯´æ˜                                    |                              |
| --------------------------------------- | ---------------------------- |
| StreamingContextå¯¹è±¡.start()            | ä»£è¡¨å¯åŠ¨æœåŠ¡                 |
| StreamingContextå¯¹è±¡.awaitTermination() | ä½¿æœåŠ¡å¤„äºç­‰å¾…å’Œå¯ç”¨çš„çŠ¶æ€   |
| StreamingContextå¯¹è±¡.stop()             | å‘ç”Ÿå¼‚å¸¸æˆ–è€…æ‰‹åŠ¨ä½¿ç”¨è¿›è¡Œç»ˆæ­¢ |



## æ•°æ®æ¥æº

### åŸºæœ¬æ•°æ®æº

> åŒ…æ‹¬RDDé˜Ÿåˆ—ã€æ–‡ä»¶ç³»ç»Ÿã€Socket è¿æ¥ç­‰
>

#### RDDé˜Ÿåˆ—

------

```scala
package com.hjc.streaming_bilibili

import org.apache.spark.SparkConf
import org.apache.spark.rdd.RDD
import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream}
import org.apache.spark.streaming.{Seconds, StreamingContext}

import scala.collection.mutable

object SparkStreaming02_Queue {

    def main(args: Array[String]): Unit = {

        // TODO åˆ›å»ºç¯å¢ƒå¯¹è±¡
        // StreamingContextåˆ›å»ºæ—¶ï¼Œéœ€è¦ä¼ é€’ä¸¤ä¸ªå‚æ•°
        // ç¬¬ä¸€ä¸ªå‚æ•°è¡¨ç¤ºç¯å¢ƒé…ç½®
        val sparkConf = new SparkConf().setMaster("local[*]").setAppName("SparkStreaming")
        // ç¬¬äºŒä¸ªå‚æ•°è¡¨ç¤ºæ‰¹é‡å¤„ç†çš„å‘¨æœŸï¼ˆé‡‡é›†å‘¨æœŸï¼‰
        val ssc = new StreamingContext(sparkConf, Seconds(3))

        val rddQueue = new mutable.Queue[RDD[Int]]()
		//							ğŸ‘‡
        val inputStream = ssc.queueStream(rddQueue,oneAtATime = false)
        val mappedStream = inputStream.map((_,1))
        val reducedStream = mappedStream.reduceByKey(_ + _)
        reducedStream.print()

        ssc.start()

        for (i <- 1 to 5) {
            rddQueue += ssc.sparkContext.makeRDD(1 to 300, 10)
            Thread.sleep(2000)
        }

        ssc.awaitTermination()
    }
}

```

[^ssc.queueStream]:ç”¨æ¥åˆ›å»ºDStream



#### Socket æ•°æ®æº

------

> éœ€è¦è‡ªå®šä¹‰æ•°æ®æº  ç»§æ‰¿Receiverï¼Œå¹¶å®ç°onStartã€onStopæ–¹æ³•æ¥è‡ªå®šä¹‰æ•°æ®æºé‡‡é›†

```scala
object SparkStreaming03_CustomReceiver {
  def main(args: Array[String]): Unit = {
    //1. ç”Ÿæˆä¸€ä¸ªDstream
    val sparkConf: SparkConf = new SparkConf()
      .setMaster("local[2]")
      .setAppName("StreamingTest")
    val streamingContext = new StreamingContext(sparkConf, Seconds(4))

    val myStream: ReceiverInputDStream[String] = streamingContext.receiverStream(new MyReceiver)
    myStream
      .flatMap(_.split(" "))
      .map((_,1))
      .reduceByKey(_+_)
      .print()

    //3. è¿è¡Œæµç¨‹åº
    streamingContext.start()
    streamingContext.awaitTermination()
  }
}

class MyReceiver extends Receiver[String](StorageLevel.MEMORY_ONLY) {
  override def onStart(): Unit = {
    new Thread {
      override def run(): Unit = {
        val socket = new Socket("hadoop102", 9999)
        val reader = new BufferedReader(new InputStreamReader(socket.getInputStream, StandardCharsets.UTF_8))
        var line: String = reader.readLine()
        while (line != null) {
          store(line)
          line = reader.readLine()
        }
      }
    }.start()
  }

  override def onStop(): Unit = {

  }
}
```







### é«˜çº§æ•°æ®æº

> åŒ…æ‹¬ Kafkaï¼ŒFlumeï¼ŒKinesis ç­‰



#### Kafkaæ•°æ®æº

------

```scala
package com.hjc.streaming_bilibili

import java.util.Random

import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord}
import org.apache.spark.SparkConf
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.dstream.{InputDStream, ReceiverInputDStream}
import org.apache.spark.streaming.kafka010.{ConsumerStrategies, KafkaUtils, LocationStrategies}
import org.apache.spark.streaming.receiver.Receiver
import org.apache.spark.streaming.{Seconds, StreamingContext}

object SparkStreaming04_Kafka {

    def main(args: Array[String]): Unit = {

        val sparkConf = new SparkConf().setMaster("local[*]").setAppName("SparkStreaming")
        val ssc = new StreamingContext(sparkConf, Seconds(3))

        val kafkaPara: Map[String, Object] = Map[String, Object](
            ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG -> "hadoop102:9092,hadoop103:9092,hadoop104:9092",
            ConsumerConfig.GROUP_ID_CONFIG -> "atguigu",
            "key.deserializer" -> "org.apache.kafka.common.serialization.StringDeserializer",
            "value.deserializer" -> "org.apache.kafka.common.serialization.StringDeserializer"
        )

        val kafkaDataDS: InputDStream[ConsumerRecord[String, String]] = 	
        KafkaUtils.createDirectStream[String, String](
            ssc,
            //  âœ¨
            LocationStrategies.PreferConsistent,
            ConsumerStrategies.Subscribe[String, String](Set("atguiguNew"), kafkaPara)
        )
        kafkaDataDS.map(_.value()).print()


        ssc.start()
        ssc.awaitTermination()
    }
}
```



| âœ¨LocationStrategiesçš„ä¸‰ç§æ¨¡å¼[^é‡‡é›†èŠ‚ç‚¹å’Œå¤„ç†èŠ‚ç‚¹çš„åŒ¹é…ç­–ç•¥] |                            |      |
| ------------------------------------------------------------ | -------------------------- | ---- |
| PreferBrokers                                                | è®©Borkerså»é€‰æ‹©[^ä¸€èˆ¬ä¸ç”¨] |      |
| PreferConsistent                                             | è‡ªåŠ¨é€‰æ‹©                   |      |
| PreferFixed                                                  | æ··åˆæ¨¡å¼                   |      |





# ä¸‰ã€DStreamè½¬æ¢

> DStream çš„ä»»ä½•æ“ä½œéƒ½ä¼šè½¬æ¢ä¸ºåº•å±‚ RDD ä¸Šçš„æ“ä½œã€æ‰€ä»¥ DStream èƒ½å¤Ÿæ”¯æŒ RDD å¤§éƒ¨åˆ†çš„è½¬æ¢ç®—å­

[^æ¦‚å¿µå¼•å…¥]: **çŠ¶æ€**

**ğŸ¤”ä»€ä¹ˆæ˜¯çŠ¶æ€ï¼Ÿ**

[^ğŸŒ°]: åœ¨ç™»é™†ç½‘é¡µæ—¶ï¼Œç¬¬ä¸€æ¬¡éœ€è¦è¾“å…¥è´¦å·å¯†ç   è€Œç¬¬äºŒæ¬¡å¯èƒ½å°±ä¸éœ€è¦äº†  è¿™æ˜¯å› ä¸ºç½‘ç«™ä¿å­˜äº†æˆ‘ä»¬ä¸Šä¸€æ¬¡ç™»é™†çš„çŠ¶æ€ã€



å¸¦å…¥åˆ°Spark Streamingå°±æ˜¯ï¼šæ¯ä¸ªé‡‡é›†å‘¨æœŸæ˜¯å¦å¯¹è¯¥å‘¨æœŸçš„çŠ¶æ€[^æ•°æ®ä»€ä¹ˆçš„]è¿›è¡Œä¿å­˜   ä¾¿äºåç»­çš„ èšåˆ æ±‡æ€»ç­‰æ“ä½œ

ä¸»è¦åˆ†ä¸ºä»¥ä¸‹ä¸¤ç±»ğŸ‘‡

- **æ— çŠ¶æ€è½¬åŒ–**
- **æœ‰çŠ¶æ€è½¬åŒ–**



## æ— çŠ¶æ€è½¬åŒ–

> æ— çŠ¶æ€æ•°æ®æ“ä½œï¼Œåªå¯¹å½“å‰çš„é‡‡é›†å‘¨æœŸå†…çš„æ•°æ®è¿›è¡Œå¤„ç†

### Transform

------

==æ‰§è¡Œä»»æ„çš„RDD-to-RDDå‡½æ•°==

```scala
package com.hjc.streaming_bilibili

import org.apache.spark.SparkConf
import org.apache.spark.streaming.dstream.DStream
import org.apache.spark.streaming.{Seconds, StreamingContext}

object SparkStreaming06_State_Transform {

    def main(args: Array[String]): Unit = {

        val sparkConf = new SparkConf().setMaster("local[*]").setAppName("SparkStreaming")
        val ssc = new StreamingContext(sparkConf, Seconds(3))

        val lines = ssc.socketTextStream("localhost", 9999)



        // Code : Driverç«¯					ğŸ‘‡
        val newDS: DStream[String] = lines.transform(
            rdd => {
                // Code : Driverç«¯ï¼Œï¼ˆå‘¨æœŸæ€§æ‰§è¡Œï¼‰
                rdd.map(
                    str => {
                        // Code : Executorç«¯
                        str
                    }
                )
            }
        )
        // Code : Driverç«¯			       ğŸ‘‡
        val newDS1: DStream[String] = lines.map(
            data => {
                // Code : Executorç«¯
                data
            }
        )

        ssc.start()
        ssc.awaitTermination()
    }

}

```



[^âœ¨]:è¯¥å‡½æ•°æ¯ä¸€æ‰¹æ¬¡è°ƒåº¦ä¸€æ¬¡ã€‚å…¶å®ä¹Ÿå°±æ˜¯å¯¹DStreamä¸­çš„RDDåº”ç”¨è½¬æ¢



### Join

------

> æ‰€è°“çš„DStreamçš„Joinæ“ä½œï¼Œå…¶å®å°±æ˜¯ä¸¤ä¸ªRDDçš„join

[^ğŸ“¢]: ä¸¤ä¸ªæµä¹‹é—´çš„joinéœ€è¦ä¸¤ä¸ªæµçš„æ‰¹æ¬¡å¤§å°ä¸€è‡´ï¼Œè¿™æ ·æ‰èƒ½åšåˆ°åŒæ—¶è§¦å‘è®¡ç®—ã€‚è®¡ç®—è¿‡ç¨‹å°±æ˜¯å¯¹å½“å‰æ‰¹æ¬¡çš„ä¸¤ä¸ªæµä¸­å„è‡ªçš„RDDè¿›è¡Œjoinï¼Œä¸ä¸¤ä¸ªRDDçš„joinæ•ˆæœç›¸åŒ



```scala
package com.hjc.streaming_bilibili

import org.apache.spark.SparkConf
import org.apache.spark.streaming.dstream.DStream
import org.apache.spark.streaming.{Seconds, StreamingContext}

object SparkStreaming06_State_Join {

    def main(args: Array[String]): Unit = {

        val sparkConf = new SparkConf().setMaster("local[*]").setAppName("SparkStreaming")
        val ssc = new StreamingContext(sparkConf, Seconds(5))

        val data9999 = ssc.socketTextStream("localhost", 9999)
        val data8888 = ssc.socketTextStream("localhost", 8888)

        val map9999: DStream[(String, Int)] = data9999.map((_,9))
        val map8888: DStream[(String, Int)] = data8888.map((_,8))

        // æ‰€è°“çš„DStreamçš„Joinæ“ä½œï¼Œå…¶å®å°±æ˜¯ä¸¤ä¸ªRDDçš„join
        val joinDS: DStream[(String, (Int, Int))] = map9999.join(map8888)

        joinDS.print()

        ssc.start()
        ssc.awaitTermination()
    }
}
```





## æœ‰çŠ¶æ€è½¬åŒ–

### UpdateStateByKey

> åœ¨æŸäº›åœºåˆä¸‹ï¼Œéœ€è¦ä¿ç•™æ•°æ®ç»Ÿè®¡ç»“æœï¼ˆçŠ¶æ€ï¼‰ï¼Œå®ç°æ•°æ®çš„æ±‡æ€»  æœ‰ç‚¹åƒç´¯åŠ å™¨

**å®ç°æ­¥éª¤**

- å®šä¹‰çŠ¶æ€ï¼ŒçŠ¶æ€å¯ä»¥æ˜¯ä¸€ä¸ª**ä»»æ„çš„æ•°æ®ç±»å‹**

- å®šä¹‰çŠ¶æ€æ›´æ–°å‡½æ•°ï¼Œç”¨æ­¤å‡½æ•°é˜æ˜å¦‚ä½•ä½¿ç”¨ä¹‹å‰çš„çŠ¶æ€å’Œæ¥è‡ªè¾“å…¥æµçš„æ–°å€¼å¯¹çŠ¶æ€è¿›è¡Œæ›´æ–°

- é…ç½®æ£€æŸ¥ç‚¹ç›®å½•

  

```scala
package com.hjc.streaming_bilibili

import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord}
import org.apache.spark.SparkConf
import org.apache.spark.streaming.dstream.InputDStream
import org.apache.spark.streaming.kafka010.{ConsumerStrategies, KafkaUtils, LocationStrategies}
import org.apache.spark.streaming.{Seconds, StreamingContext}

object SparkStreaming05_State {

    def main(args: Array[String]): Unit = {

        val sparkConf = new SparkConf().setMaster("local[*]").setAppName("SparkStreaming")
        val ssc = new StreamingContext(sparkConf, Seconds(3))
        ssc.checkpoint("cp")

        // åœ¨æŸäº›åœºåˆä¸‹ï¼Œéœ€è¦ä¿ç•™æ•°æ®ç»Ÿè®¡ç»“æœï¼ˆçŠ¶æ€ï¼‰ï¼Œå®ç°æ•°æ®çš„æ±‡æ€»
        // ä½¿ç”¨æœ‰çŠ¶æ€æ“ä½œæ—¶ï¼Œéœ€è¦è®¾å®šæ£€æŸ¥ç‚¹è·¯å¾„
        val datas = ssc.socketTextStream("localhost", 9999)

        val wordToOne = datas.map((_,1))

        //val wordToCount = wordToOne.reduceByKey(_+_)

        // updateStateByKeyï¼šæ ¹æ®keyå¯¹æ•°æ®çš„çŠ¶æ€è¿›è¡Œæ›´æ–°
        // ä¼ é€’çš„å‚æ•°ä¸­å«æœ‰ä¸¤ä¸ªå€¼
        // ç¬¬ä¸€ä¸ªå€¼è¡¨ç¤ºç›¸åŒçš„keyçš„valueæ•°æ®
        // ç¬¬äºŒä¸ªå€¼è¡¨ç¤ºç¼“å­˜åŒºç›¸åŒkeyçš„valueæ•°æ®
        val state = wordToOne.updateStateByKey(
            ( seq:Seq[Int], buff:Option[Int] ) => {
                val newCount = buff.getOrElse(0) + seq.sum
                Option(newCount)
            }
        )

        state.print()

        ssc.start()
        ssc.awaitTermination()
    }

}
```



**è¯¥å‡½æ•°çš„ä¸¤ä¸ªå‚æ•°**

[^å‚æ•°ä¸€]:å°±åƒæŠŠ æ•°æ®mapç»™ä¸ºï¼ˆhelloï¼Œ1ï¼‰çš„é‚£ä¸ª 1
[^å‚æ•°äºŒ]: å°±æ˜¯å¯¹æ¯ä¸ª helloçš„ 1  åšæ“ä½œå çš„æ•°æ®

å‚ç…§ç´¯åŠ å™¨



â€‹																									==ğŸ‘‡æµç¨‹å›¾ğŸ‘‡==

![image-20220705202300600](../image/image-20220705202300600.png)





### çª—å£æ“ä½œ

------

> Window Operationså¯ä»¥è®¾ç½®çª—å£çš„å¤§å°å’Œæ»‘åŠ¨çª—å£çš„é—´éš”æ¥åŠ¨æ€çš„è·å–å½“å‰Steamingçš„å…è®¸çŠ¶æ€ã€‚
>
> æ‰€æœ‰åŸºäºçª—å£çš„æ“ä½œéƒ½éœ€è¦ä¸¤ä¸ªå‚æ•°ï¼Œåˆ†åˆ«ä¸ºçª—å£æ—¶é•¿ä»¥åŠæ»‘åŠ¨æ­¥é•¿ã€‚

- **çª—å£æ—¶é•¿**[^windowLength]ï¼šè®¡ç®—å†…å®¹çš„æ—¶é—´èŒƒå›´
- **æ»‘åŠ¨æ­¥é•¿**[^slideInterval]ï¼šéš”å¤šä¹…è§¦å‘ä¸€æ¬¡è®¡ç®—



==æ³¨æ„ï¼šè¿™ä¸¤è€…éƒ½å¿…é¡»ä¸ºé‡‡é›†å‘¨æœŸå¤§å°çš„æ•´æ•°å€==



#### window

> å‚æ•°	(windowLength, slideInterval)

- è¯´æ˜

  åŸºäºå¯¹æºDStreamçª—åŒ–çš„æ‰¹æ¬¡è¿›è¡Œè®¡ç®—è¿”å›ä¸€ä¸ªæ–°çš„Dstream





#### countByWindow

> å‚æ•°	(windowLength, slideInterval)

- è¯´æ˜

  è¿”å›ä¸€ä¸ªæ»‘åŠ¨çª—å£è®¡æ•°æµä¸­çš„å…ƒç´ ä¸ªæ•°

- ä»£ç 

  ```scala
  object SparkStreaming09_CounyByWindow {
    def main(args: Array[String]): Unit = {
      val context: StreamingContext = StreamingContext.getOrCreate(
        "./ck",
        () => {
          //1. ç”Ÿæˆä¸€ä¸ªDstream
          val sparkConf: SparkConf = new SparkConf()
            .setMaster("local[2]")
            .setAppName("StreamingTest")
          val streamingContext = new StreamingContext(sparkConf, Seconds(3))
  
          streamingContext.checkpoint("./ck")
  
          val dStream: ReceiverInputDStream[String] = streamingContext.socketTextStream("hadoop102", 9999)
  
          //2. è®¡ç®—(wordcount)
          dStream
            .flatMap(_.split(" "))
            .map((_, 1))
            .countByWindow(
              Seconds(12),
              Seconds(6)
            )
            .print()
          streamingContext
        })
  
      context.start()
      context.awaitTermination()
    }
  
  }
  ```





#### reduceByWindow

> å‚æ•°	(func, windowLength, slideInterval)

- è¯´æ˜

  é€šè¿‡ä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°æ•´åˆæ»‘åŠ¨åŒºé—´æµå…ƒç´ æ¥åˆ›å»ºä¸€ä¸ªæ–°çš„å•å…ƒç´ æµ

- ä»£ç 

  ```scala
  object SparkStreaming10_ReduceByWindow {
    def main(args: Array[String]): Unit = {
      //1. ç”Ÿæˆä¸€ä¸ªDStream
      val sparkConf: SparkConf = new SparkConf()
        .setMaster("local[2]")
        .setAppName("StreamingTest")
      val streamingContext = new StreamingContext(sparkConf, Seconds(4))
  
      //2. ç”ŸæˆDStream
      val queue: mutable.Queue[RDD[Int]] = mutable.Queue.empty[RDD[Int]]
      val QueueDStream: InputDStream[Int] = streamingContext.queueStream(queue, oneAtATime = false)
      QueueDStream
        .reduceByWindow(
          (x: Int, y:Int) => x + y,
          Seconds(8),
          Seconds(4)
        )
        .print()
  
  
      //3. è¿è¡Œæµç¨‹åº
      streamingContext.start()
  
      //4. å‘é˜Ÿåˆ—ä¸­æ·»åŠ RDD
      while (true) {
        val rdd: RDD[Int] = streamingContext.sparkContext.makeRDD(Seq(1, 2, 3, 4, 5))
        queue.enqueue(rdd)
        Thread.sleep(2000)
      }
      streamingContext.awaitTermination()
    }
  
  }
  ```





#### reduceByKeyAndWindow

##### ä¸‰ä¸ªå‚æ•°

> (func, windowLength, slideInterval, [numTasks])

- è¯´æ˜

  å½“åœ¨ä¸€ä¸ª(K,V)å¯¹çš„DStreamä¸Šè°ƒç”¨æ­¤å‡½æ•°ï¼Œä¼šè¿”å›ä¸€ä¸ªæ–°(K,V)å¯¹çš„DStreamï¼Œæ­¤å¤„é€šè¿‡å¯¹æ»‘åŠ¨çª—å£ä¸­æ‰¹æ¬¡æ•°æ®ä½¿ç”¨reduceå‡½æ•°æ¥æ•´åˆæ¯ä¸ªkeyçš„valueå€¼

- ä»£ç 

  ```scala
  import org.apache.spark.SparkConf
  import org.apache.spark.rdd.RDD
  import org.apache.spark.streaming.dstream.InputDStream
  import org.apache.spark.streaming.{Seconds, StreamingContext}
  
  import scala.collection.mutable
  
  object SparkStreaming10_ReduceByWindow {
    def main(args: Array[String]): Unit = {
      //1. ç”Ÿæˆä¸€ä¸ªDStream
      val sparkConf: SparkConf = new SparkConf()
        .setMaster("local[2]")
        .setAppName("StreamingTest")
      val streamingContext = new StreamingContext(sparkConf, Seconds(4))
  
      //2. ç”ŸæˆDStream
      val queue: mutable.Queue[RDD[Int]] = mutable.Queue.empty[RDD[Int]]
      val QueueDStream: InputDStream[Int] = streamingContext.queueStream(queue, oneAtATime = false)
      QueueDStream
        .reduceByWindow(
          (x: Int, y:Int) => x + y,
          Seconds(8),
          Seconds(4)
        )
        .print()
  
  
      //3. è¿è¡Œæµç¨‹åº
      streamingContext.start()
  
      //4. å‘é˜Ÿåˆ—ä¸­æ·»åŠ RDD
      while (true) {
        val rdd: RDD[Int] = streamingContext.sparkContext.makeRDD(Seq(1, 2, 3, 4, 5))
        queue.enqueue(rdd)
        Thread.sleep(2000)
      }
      streamingContext.awaitTermination()
    }
  
  }
  ```



##### å››ä¸ªå‚æ•°

> (func, invFunc, windowLength, slideInterval, [numTasks])

[^ç¬¬ä¸€ä¸ªå‡½æ•°]: æ˜¯åœ¨ä¸€ä¸ªæ­¥é•¿åå¯¹è¿›å…¥çš„æ•°æ®åšçš„æ“ä½œ
[^ç¬¬äºŒä¸ªå‡½æ•°]: æ˜¯å¯¹ä¸€ä¸ªæ­¥é•¿åå¯¹ç¦»å¼€çª—å£çš„æ•°æ®åšçš„æ“ä½œ



- è¯´æ˜

  å½“çª—å£èŒƒå›´æ¯”è¾ƒå¤§ï¼Œä½†æ˜¯æ»‘åŠ¨å¹…åº¦æ¯”è¾ƒå°ï¼Œé‚£ä¹ˆå¯ä»¥é‡‡ç”¨å¢åŠ æ•°æ®å’Œåˆ é™¤æ•°æ®çš„æ–¹å¼

- ä»£ç 

  ```scala
  package com.hjc.streaming_bilibili
  
  import org.apache.spark.SparkConf
  import org.apache.spark.streaming.dstream.DStream
  import org.apache.spark.streaming.{Seconds, StreamingContext}
  
  object SparkStreaming06_State_Window1 {
  
      def main(args: Array[String]): Unit = {
  
          val sparkConf = new SparkConf().setMaster("local[*]").setAppName("SparkStreaming")
          val ssc = new StreamingContext(sparkConf, Seconds(3))
          ssc.checkpoint("cp")
  
          val lines = ssc.socketTextStream("localhost", 9999)
          val wordToOne = lines.map((_,1))
  
          
          val windowDS: DStream[(String, Int)] =
              wordToOne.reduceByKeyAndWindow(
                  (x:Int, y:Int) => { x + y},
                  (x:Int, y:Int) => {x - y},
                  Seconds(9), Seconds(3))
  
          windowDS.print()
  
          ssc.start()
          ssc.awaitTermination()
      }
  
  }
  ```





# å››ã€DSteamè¾“å‡º

Spark Streaming æ”¯æŒä»¥ä¸‹è¾“å‡ºæ“ä½œï¼š

| Output Operation                            | Meaning                                                      |
| ------------------------------------------- | ------------------------------------------------------------ |
| **print**()                                 | åœ¨è¿è¡Œæµåº”ç”¨ç¨‹åºçš„ driver èŠ‚ç‚¹ä¸Šæ‰“å° DStream ä¸­æ¯ä¸ªæ‰¹æ¬¡çš„å‰åä¸ªå…ƒç´ ã€‚ç”¨äºå¼€å‘è°ƒè¯•ã€‚ |
| **saveAsTextFiles**(*prefix*, [*suffix*])   | å°† DStream çš„å†…å®¹ä¿å­˜ä¸ºæ–‡æœ¬æ–‡ä»¶ã€‚æ¯ä¸ªæ‰¹å¤„ç†é—´éš”çš„æ–‡ä»¶ååŸºäºå‰ç¼€å’Œåç¼€ç”Ÿæˆï¼šâ€œprefix-TIME_IN_MS [.suffix]â€ã€‚ |
| **saveAsObjectFiles**(*prefix*, [*suffix*]) | å°† DStream çš„å†…å®¹åºåˆ—åŒ–ä¸º Java å¯¹è±¡ï¼Œå¹¶ä¿å­˜åˆ° SequenceFilesã€‚æ¯ä¸ªæ‰¹å¤„ç†é—´éš”çš„æ–‡ä»¶ååŸºäºå‰ç¼€å’Œåç¼€ç”Ÿæˆï¼šâ€œprefix-TIME_IN_MS [.suffix]â€ã€‚ |
| **saveAsHadoopFiles**(*prefix*, [*suffix*]) | å°† DStream çš„å†…å®¹ä¿å­˜ä¸º Hadoop æ–‡ä»¶ã€‚æ¯ä¸ªæ‰¹å¤„ç†é—´éš”çš„æ–‡ä»¶ååŸºäºå‰ç¼€å’Œåç¼€ç”Ÿæˆï¼šâ€œprefix-TIME_IN_MS [.suffix]â€ã€‚ |
| **foreachRDD**(*func*)                      | æœ€é€šç”¨çš„è¾“å‡ºæ–¹å¼ï¼Œå®ƒå°†å‡½æ•° func åº”ç”¨äºä»æµç”Ÿæˆçš„æ¯ä¸ª RDDã€‚æ­¤å‡½æ•°åº”å°†æ¯ä¸ª RDD ä¸­çš„æ•°æ®æ¨é€åˆ°å¤–éƒ¨ç³»ç»Ÿï¼Œä¾‹å¦‚å°† RDD ä¿å­˜åˆ°æ–‡ä»¶ï¼Œæˆ–é€šè¿‡ç½‘ç»œå°†å…¶å†™å…¥æ•°æ®åº“ã€‚ |



## foreachRDD

> å®ç°å¯¹æ•°æ®åº“çš„æ“ä½œ



```scala
reduceDS.foreachRDD(
    rdd => {
        rdd.foreachPartition(
            iter => {
                val conn = JDBCUtil.getConnection
                val pstat = conn.prepareStatement(
                    """
                      | insert into area_city_ad_count ( dt, area, city, adid, count )
                      | values ( ?, ?, ?, ?, ? )
                      | on DUPLICATE KEY
                      | UPDATE count = count + ?
                    """.stripMargin)
                iter.foreach{
                    case ( ( day, area, city, ad ), sum ) => {
                        pstat.setString(1,day )
                        pstat.setString(2,area )
                        pstat.setString(3, city)
                        pstat.setString(4, ad)
                        pstat.setInt(5, sum)
                        pstat.setInt(6,sum )
                        pstat.executeUpdate()
                    }
                }
                pstat.close()
                conn.close()
            }
        )
    }
)
```



==æ³¨æ„==

- è¿æ¥ä¸èƒ½å†™åœ¨driverå±‚é¢ï¼ˆåºåˆ—åŒ–ï¼‰
- å¦‚æœå†™åœ¨foreachåˆ™æ¯ä¸ªRDDä¸­çš„æ¯ä¸€æ¡æ•°æ®éƒ½åˆ›å»ºï¼Œå¾—ä¸å¿å¤±
- å¢åŠ foreachPartitionï¼Œåœ¨åˆ†åŒºåˆ›å»ºï¼ˆè·å–ï¼‰





# äº”ã€å…³é—­

> æµå¼ä»»åŠ¡éœ€è¦7*24å°æ—¶æ‰§è¡Œï¼Œä½†æ˜¯æœ‰æ—¶æ¶‰åŠåˆ°å‡çº§ä»£ç éœ€è¦ä¸»åŠ¨åœæ­¢ç¨‹åºï¼Œä½†æ˜¯åˆ†å¸ƒå¼ç¨‹åºï¼Œæ²¡åŠæ³•åšåˆ°ä¸€ä¸ªä¸ªè¿›ç¨‹å»æ€æ­»ï¼Œæ‰€æœ‰é…ç½®ä¼˜é›…çš„å…³é—­å°±æ˜¾å¾—è‡³å…³é‡è¦äº†ã€‚



**ä½¿ç”¨å¤–éƒ¨æ–‡ä»¶ç³»ç»Ÿæ¥æ§åˆ¶å†…éƒ¨ç¨‹åºå…³é—­**

```scala
package com.hjc.streaming_bilibili

import org.apache.spark.SparkConf
import org.apache.spark.streaming.dstream.DStream
import org.apache.spark.streaming.{Seconds, StreamingContext, StreamingContextState}

object SparkStreaming08_Close {

    def main(args: Array[String]): Unit = {

        /*
           çº¿ç¨‹çš„å…³é—­ï¼š
           val thread = new Thread()
           thread.start()

           thread.stop(); // å¼ºåˆ¶å…³é—­

         */

        val sparkConf = new SparkConf().setMaster("local[*]").setAppName("SparkStreaming")
        val ssc = new StreamingContext(sparkConf, Seconds(3))

        val lines = ssc.socketTextStream("localhost", 9999)
        val wordToOne = lines.map((_,1))

        wordToOne.print()

        ssc.start()

        // å¦‚æœæƒ³è¦å…³é—­é‡‡é›†å™¨ï¼Œé‚£ä¹ˆéœ€è¦åˆ›å»ºæ–°çš„çº¿ç¨‹
        // è€Œä¸”éœ€è¦åœ¨ç¬¬ä¸‰æ–¹ç¨‹åºä¸­å¢åŠ å…³é—­çŠ¶æ€
        new Thread(
            new Runnable {
                override def run(): Unit = {
                    // ä¼˜é›…åœ°å…³é—­
                    // è®¡ç®—èŠ‚ç‚¹ä¸åœ¨æ¥æ”¶æ–°çš„æ•°æ®ï¼Œè€Œæ˜¯å°†ç°æœ‰çš„æ•°æ®å¤„ç†å®Œæ¯•ï¼Œç„¶åå…³é—­
                    // Mysql : Table(stopSpark) => Row => data
                    // Redis : Dataï¼ˆK-Vï¼‰
                    // ZK    : /stopSpark
                    // HDFS  : /stopSpark
                    /*
                    while ( true ) {
                        if (true) {
                            // è·å–SparkStreamingçŠ¶æ€
                            val state: StreamingContextState = ssc.getState()
                            if ( state == StreamingContextState.ACTIVE ) {
                                ssc.stop(true, true)
                            }
                        }
                        Thread.sleep(5000)
                    }
                     */

                    Thread.sleep(5000)
                    val state: StreamingContextState = ssc.getState()
                    if ( state == StreamingContextState.ACTIVE ) {
                        ssc.stop(true, true)
                    }
                    System.exit(0)
                }
            }
        ).start()

        ssc.awaitTermination() // block é˜»å¡mainçº¿ç¨‹


    }

}
```















