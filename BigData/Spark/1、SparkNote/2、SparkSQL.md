# ä¸€ã€ç®€ä»‹

> Spark SQLæ˜¯Sparkç”¨äºç»“æ„åŒ–æ•°æ®(structured data)å¤„ç†çš„Sparkæ¨¡å—

- **å†å²**

  SparkSQLçš„å‰èº«æ˜¯Shark  Sharkæ˜¯ä¼¯å…‹åˆ©å®éªŒå®¤Sparkç”Ÿæ€ç¯å¢ƒçš„ç»„ä»¶ä¹‹ä¸€ï¼Œæ˜¯åŸºäº**Hive**æ‰€å¼€å‘çš„å·¥å…·  

  Sharkå¯¹äºHiveçš„å¤ªå¤šä¾èµ–  åˆ¶çº¦äº†Sparkå„ä¸ªç»„ä»¶çš„ç›¸äº’é›†æˆï¼Œæ‰€ä»¥æå‡ºäº†SparkSQLé¡¹ç›®  SparkSQLæŠ›å¼ƒåŸæœ‰Sharkçš„ä»£ç ï¼Œæ±²å–äº†Sharkçš„ä¸€äº›ä¼˜ç‚¹é‡æ–°å¼€å‘äº†SparkSQLä»£ç   æ‘†è„±äº†å¯¹Hiveçš„ä¾èµ–æ€§

  2014å¹´6æœˆ1æ—¥Sharké¡¹ç›®å’ŒSparkSQLé¡¹ç›®çš„ä¸»æŒäººReynold Xinå®£å¸ƒï¼šåœæ­¢å¯¹Sharkçš„å¼€å‘ï¼Œå›¢é˜Ÿå°†æ‰€æœ‰èµ„æºæ”¾SparkSQLé¡¹ç›®ä¸Šï¼Œè‡³æ­¤ï¼ŒSharkçš„å‘å±•ç”»ä¸Šäº†å¥è¯ï¼Œä½†ä¹Ÿå› æ­¤å‘å±•å‡ºä¸¤ä¸ªæ”¯çº¿ï¼š==SparkSQLå’ŒHive on Spark==





- **æ¶æ„å¯¹æ¯”**

  Spark SQL

  ![CSE6242 wk7 1 3 spark SQL other libraries - YouTube](../image/maxresdefault.jpg)

  Hive

  <img src="../image/images.jpeg" alt="Spark and Shark: Lightning-Fast Analytics over Hadoop and Hive Data" style="zoom:200%;" />

  [^ä¸»è¦ä¿®æ”¹æ¨¡å—]: å†…å­˜ç®¡ç†ã€ç‰©ç†è®¡åˆ’ã€æ‰§è¡Œä¸‰ä¸ªæ¨¡å—

  

## ç‰¹ç‚¹



- èƒ½å¤Ÿå°† SQL æŸ¥è¯¢ä¸ Spark ç¨‹åºæ— ç¼æ··åˆï¼Œå…è®¸æ‚¨ä½¿ç”¨ SQL æˆ– DataFrame API å¯¹ç»“æ„åŒ–æ•°æ®è¿›è¡ŒæŸ¥è¯¢ï¼›
- æ”¯æŒå¤šç§å¼€å‘è¯­è¨€ï¼›
- æ”¯æŒå¤šè¾¾ä¸Šç™¾ç§çš„å¤–éƒ¨æ•°æ®æº[^åŒ…æ‹¬ Hiveï¼ŒAvroï¼ŒParquetï¼ŒORCï¼ŒJSON å’Œ JDBC ç­‰ï¼›]
- æ”¯æŒ HiveQL è¯­æ³•ä»¥åŠ Hive SerDes å’Œ UDFï¼Œå…è®¸ä½ è®¿é—®ç°æœ‰çš„ Hive ä»“åº“ï¼›
- æ”¯æŒæ ‡å‡†çš„ JDBC å’Œ ODBC è¿æ¥ï¼›
- æ”¯æŒä¼˜åŒ–å™¨ï¼Œåˆ—å¼å­˜å‚¨å’Œä»£ç ç”Ÿæˆç­‰ç‰¹æ€§ï¼›
- æ”¯æŒæ‰©å±•å¹¶èƒ½ä¿è¯å®¹é”™



## DataFrameå’ŒRDDå¯¹æ¯”

> DataFrame å’Œ RDDs æœ€ä¸»è¦çš„åŒºåˆ«åœ¨äºä¸€ä¸ªé¢å‘çš„æ˜¯ç»“æ„åŒ–æ•°æ®ï¼Œä¸€ä¸ªé¢å‘çš„æ˜¯éç»“æ„åŒ–æ•°æ®

==DataFrameæ˜¯ä¸€ç§ä»¥RDDä¸ºåŸºç¡€çš„åˆ†å¸ƒå¼æ•°æ®é›†ï¼Œç±»ä¼¼äºä¼ ç»Ÿæ•°æ®åº“ä¸­çš„äºŒç»´è¡¨æ ¼==

![13. Spark SQL â€” Code-Cookbook 0.2 æ–‡æ¡£](../image/image-20201122142741172.png)

DataFrame å†…éƒ¨çš„æœ‰æ˜ç¡® Scheme ç»“æ„ï¼Œå³åˆ—åã€åˆ—å­—æ®µç±»å‹éƒ½æ˜¯å·²çŸ¥çš„ï¼Œ

è¿™å¸¦æ¥çš„å¥½å¤„æ˜¯å¯ä»¥**å‡å°‘æ•°æ®è¯»å–ä»¥åŠæ›´å¥½åœ°ä¼˜åŒ–æ‰§è¡Œè®¡åˆ’ï¼Œä»è€Œä¿è¯æŸ¥è¯¢æ•ˆç‡ã€‚**

**âœŒDataFrame å’Œ RDDs åº”è¯¥å¦‚ä½•é€‰æ‹©ï¼Ÿ**

- å¦‚æœä½ æƒ³ä½¿ç”¨å‡½æ•°å¼ç¼–ç¨‹è€Œä¸æ˜¯ DataFrame APIï¼Œåˆ™ä½¿ç”¨ RDDsï¼›
- å¦‚æœä½ çš„æ•°æ®æ˜¯éç»“æ„åŒ–çš„ (æ¯”å¦‚æµåª’ä½“æˆ–è€…å­—ç¬¦æµ)ï¼Œåˆ™ä½¿ç”¨ RDDsï¼Œ
- å¦‚æœä½ çš„æ•°æ®æ˜¯ç»“æ„åŒ–çš„ (å¦‚ RDBMS ä¸­çš„æ•°æ®) æˆ–è€…åŠç»“æ„åŒ–çš„ (å¦‚æ—¥å¿—)ï¼Œå‡ºäºæ€§èƒ½ä¸Šçš„è€ƒè™‘ï¼Œåº”ä¼˜å…ˆä½¿ç”¨ DataFrameã€‚





## DataSet

> DataSetæ˜¯åˆ†å¸ƒå¼æ•°æ®é›†åˆã€‚DataSetæ˜¯Spark 1.6ä¸­æ·»åŠ çš„ä¸€ä¸ªæ–°æŠ½è±¡



**ç‰¹ç‚¹**

- é›†æˆäº† RDD å’Œ DataFrame çš„ä¼˜ç‚¹ï¼Œå…·å¤‡å¼ºç±»å‹çš„ç‰¹ç‚¹ï¼ŒåŒæ—¶æ”¯æŒ Lambda å‡½æ•°ï¼Œä½†åªèƒ½åœ¨ Scala å’Œ Java è¯­è¨€ä¸­ä½¿ç”¨
- Spark 2.0 åï¼Œä¸ºäº†æ–¹ä¾¿å¼€å‘è€…ï¼ŒSpark å°† DataFrame å’Œ Dataset çš„ API èåˆåˆ°ä¸€èµ·ï¼Œæä¾›äº†ç»“æ„åŒ–çš„ API(Structured API)ï¼Œå³ç”¨æˆ·å¯ä»¥é€šè¿‡ä¸€å¥—æ ‡å‡†çš„ API å°±èƒ½å®Œæˆå¯¹ä¸¤è€…çš„æ“ä½œ
- ç”¨æ ·ä¾‹ç±»æ¥å¯¹DataSetä¸­å®šä¹‰æ•°æ®çš„ç»“æ„ä¿¡æ¯ï¼Œæ ·ä¾‹ç±»ä¸­æ¯ä¸ªå±æ€§çš„åç§°ç›´æ¥æ˜ å°„åˆ°DataSetä¸­çš„å­—æ®µåç§°
- DataFrameæ˜¯DataSetçš„ç‰¹åˆ—ï¼ŒÂ·`DataFrame=DataSet[Row]`



**ç±»å‹é—®é¢˜**

- DataFrame è¢«æ ‡è®°ä¸º Untyped APIï¼Œè€Œ DataSet è¢«æ ‡è®°ä¸º Typed API

![image-20220701212042028](../image/image-20220701212042028.png)



- **å¼•å‡ºç±»å‹å®‰å…¨é—®é¢˜**

  æµç¨‹ï¼š Compile  -->  Runtime

  ![image-20220701212826981](../image/image-20220701212826981.png)

  [^Syntax Errors]:  è¯­æ³•é”™è¯¯    
  [^Analtsis Error]: è§£æé”™è¯¯
  [^ä¸€å¥è¯]: Dataset æœ€ä¸¥æ ¼ï¼Œä½†å¯¹äºå¼€å‘è€…æ¥è¯´æ•ˆç‡æœ€é«˜

  

## å°ç»“

**DataFrame ğŸ”— DataSet ğŸ”— RDDs** 

- RDDs é€‚åˆéç»“æ„åŒ–æ•°æ®çš„å¤„ç†ï¼Œè€Œ DataFrame & DataSet æ›´é€‚åˆç»“æ„åŒ–æ•°æ®å’ŒåŠç»“æ„åŒ–çš„å¤„ç†ï¼›
- DataFrame & DataSet å¯ä»¥é€šè¿‡ç»Ÿä¸€çš„ Structured API è¿›è¡Œè®¿é—®ï¼Œè€Œ RDDs åˆ™æ›´é€‚åˆå‡½æ•°å¼ç¼–ç¨‹çš„åœºæ™¯ï¼›
- ç›¸æ¯”äº DataFrame è€Œè¨€ï¼ŒDataSet æ˜¯å¼ºç±»å‹çš„ (Typed)ï¼Œæœ‰ç€æ›´ä¸ºä¸¥æ ¼çš„é™æ€ç±»å‹æ£€æŸ¥ï¼›
- DataSetsã€DataFramesã€SQL çš„åº•å±‚éƒ½ä¾èµ–äº† RDDs APIï¼Œå¹¶å¯¹å¤–æä¾›ç»“æ„åŒ–çš„è®¿é—®æ¥å£ã€‚



![image-20220701213555810](../image/image-20220701213555810.png)



# äºŒã€SparkSQLæ ¸å¿ƒ

> æ•´ä½“æµç¨‹

1. **åˆ›å»ºç¯å¢ƒ**

   - ```scala
     //æ–¹å¼ä¸€
     val spark = SparkSession.builder().appName("Spark-SQL").master("local[*]").getOrCreate()
     
     //æ–¹å¼äºŒ
     val sparkConf = new SparkConf().setMaster("local[*]").setAppName("SparkSQL01_Basic")
     val spark = SparkSession.builder().config(sparkConf).getOrCreate()
     ```

2. **æ‰§è¡Œé€»è¾‘æ“ä½œé€»è¾‘**

3. **å…³é—­ è¿è¡Œç¯å¢ƒ**

   ```scala
   spark.close()
   ```



**å…¨å±€ä¸´æ—¶è§†å›¾**

- ä½¿ç”¨ `createOrReplaceTempView` åˆ›å»ºçš„æ˜¯ä¼šè¯ä¸´æ—¶è§†å›¾ï¼Œå®ƒçš„ç”Ÿå‘½å‘¨æœŸä»…é™äºä¼šè¯èŒƒå›´ï¼Œä¼šéšä¼šè¯çš„ç»“æŸè€Œç»“æŸã€‚

-  `createGlobalTempView` åˆ›å»ºå…¨å±€ä¸´æ—¶è§†å›¾ï¼Œå…¨å±€ä¸´æ—¶è§†å›¾å¯ä»¥åœ¨æ‰€æœ‰ä¼šè¯ä¹‹é—´å…±äº«ï¼Œå¹¶ç›´åˆ°æ•´ä¸ª Spark åº”ç”¨ç¨‹åºç»ˆæ­¢åæ‰ä¼šæ¶ˆå¤±ã€‚
- å…¨å±€ä¸´æ—¶è§†å›¾è¢«å®šä¹‰åœ¨å†…ç½®çš„ `global_temp` æ•°æ®åº“ä¸‹ï¼Œéœ€è¦ä½¿ç”¨é™å®šåç§°è¿›è¡Œå¼•ç”¨ï¼Œ
- å¦‚ `SELECT * FROM global_temp.view1`ã€‚

```scala
// æ³¨å†Œä¸ºå…¨å±€ä¸´æ—¶è§†å›¾
df.createGlobalTempView("gemp")

// ä½¿ç”¨é™å®šåç§°è¿›è¡Œå¼•ç”¨
spark.sql("SELECT ename,job FROM global_temp.gemp").show()
```



## IDEAå¼€å‘

**æ·»åŠ ä¾èµ–**

```xml
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-sql_2.12</artifactId>
    <version>3.0.0</version>
</dependency>
```

[^Tips]: å»ºè®®åœ¨è¿›è¡Œ spark SQL ç¼–ç¨‹å‰å¯¼å…¥ä¸‹é¢çš„éšå¼è½¬æ¢ï¼Œå› ä¸º DataFrames å’Œ dataSets ä¸­å¾ˆå¤šæ“ä½œéƒ½ä¾èµ–äº†éšå¼è½¬æ¢ import spark.implicits._



### DataFrame

------

```json
{"username": "zhangsan","age": 20}
{"username": "lisi","age": 29}
{"username": "wangwu","age": 49}
```

```scala
    // åˆ›å»ºDataFrame
    val df = spark.read.json("data/user.json")

    // åˆ›å»ºä¸´æ—¶è¡¨
    df.createOrReplaceTempView("user")

    //sqlæŸ¥è¯¢
    spark.sql("select * from user").show()
    spark.sql("select username,age from user").show()
    spark.sql("select avg(age) from user").show()

    // DSLæŸ¥è¯¢
    import spark.implicits._
    df.select("username", "age").show()
    df.select($"age" + 1).show()								   ğŸ‘‡ä¸ºä¸Šä¸‹æ–‡ç¯å¢ƒå¯¹è±¡
				ğŸ‘† å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºç»“æ„åŒ–æ•°æ®çš„åˆ— éœ€å¼•å…¥éšå¼è½¬æ¢è§„åˆ™  import XX.implicits._
    df.select('age + 1).show()
				ğŸ‘†ç®€åŒ–
```



**ç›¸äº’è½¬åŒ–**

- **RDD <-> DataFrame**

  ```scala
  val rdd: RDD[(Int, String, Int)] = spark.sparkContext.makeRDD(List((1, "zhangsan", 20), (2, "lisi", 30), (3, "wangwu", 40)))
  val dataFrame = rdd.toDF("id", "name", "age")
  //					 ğŸ‘†
  val rdd1: RDD[Row] = dataFrame.rdd
  //								ğŸ‘†
  ```







### DataSet

------

```scala
// DataSet
// DF å…¶å®æ˜¯DataSetçš„ä¸€ä¸ªç‰¹ä¾‹ ç‰¹å®šæ³›å‹
val seq = Seq(1, 2, 3, 4)

val ds: Dataset[Int] = seq.toDS()
ds.show()
```



**ç›¸äº’è½¬åŒ–**

- DataSet <--> RDD

  ```scala
  
  val rdd: RDD[(Int, String, Int)] = spark.sparkContext.makeRDD(List((1, "zhangsan", 20), (2, "lisi", 30), (3, "wangwu", 40)))
  
  
  val ds1: Dataset[User] = rdd.map {
    case (id, name, age) => User(id, name, age)
  }.toDS()
  
  
  case class User(id: Int, name: String, age: Int)
  ```

-  DataFrame <-> DataSet

  ```scala
  val ds: Dataset[User] = dataFrame.as[User]
  val dataFrame1 = ds.toDF()
  ```





## DataFrameğŸ”—DataSetğŸ”—RDD

### å…±æ€§

- éƒ½æ˜¯sparkå¹³å°ä¸‹çš„åˆ†å¸ƒå¼å¼¹æ€§æ•°æ®é›†ï¼Œä¸ºå¤„ç†è¶…å¤§å‹æ•°æ®æä¾›ä¾¿åˆ©
- æƒ°æ€§æœºåˆ¶ï¼Œåœ¨è¿›è¡Œåˆ›å»ºã€è½¬æ¢ï¼Œå¦‚mapæ–¹æ³•æ—¶ï¼Œä¸ä¼šç«‹å³æ‰§è¡Œï¼Œåªæœ‰åœ¨é‡åˆ°Actionå¦‚foreachæ—¶ï¼Œä¸‰è€…æ‰ä¼šå¼€å§‹éå†è¿ç®—
- ä¸‰è€…æœ‰è®¸å¤šå…±åŒçš„å‡½æ•°ï¼Œå¦‚filterï¼Œæ’åº
- åœ¨å¯¹DataFrameå’ŒDatasetè¿›è¡Œæ“ä½œè®¸å¤šæ“ä½œéƒ½éœ€è¦è¿™ä¸ªåŒ…:import spark.implicits
- éƒ½ä¼šæ ¹æ® Spark çš„å†…å­˜æƒ…å†µè‡ªåŠ¨ç¼“å­˜è¿ç®—ï¼Œè¿™æ ·å³ä½¿æ•°æ®é‡å¾ˆå¤§ï¼Œä¹Ÿä¸ç”¨æ‹…å¿ƒä¼šå†…å­˜æº¢å‡º
- éƒ½æœ‰partitionçš„æ¦‚å¿µ
- DataFrameå’ŒDataSetå‡å¯ä½¿ç”¨æ¨¡å¼åŒ¹é…è·å–å„ä¸ªå­—æ®µçš„å€¼å’Œç±»å‹



### åŒºåˆ«



| RDD                       | DataFrame                        | DataSet                       |
| ------------------------- | -------------------------------- | ----------------------------- |
| ä¸€èˆ¬å’Œspark mllibåŒæ—¶ä½¿ç”¨ | ä¸€èˆ¬ä¸ä¸ spark mllib åŒæ—¶ä½¿ç”¨    | ä¸€èˆ¬ä¸ä¸ spark mllib åŒæ—¶ä½¿ç”¨ |
| ä¸æ”¯æŒsparksqlæ“ä½œ        | æ”¯æŒsparksqlæ“ä½œ                 | æ”¯æŒsparksqlæ“ä½œ              |
|                           | åªæœ‰é€šè¿‡è§£ææ‰èƒ½è·å–å„ä¸ªå­—æ®µçš„å€¼ |                               |
|                           | æ”¯æŒä¸€äº›ç‰¹åˆ«æ–¹ä¾¿çš„ä¿å­˜æ–¹å¼       | æ”¯æŒä¸€äº›ç‰¹åˆ«æ–¹ä¾¿çš„ä¿å­˜æ–¹å¼    |



### ç›¸äº’è½¬æ¢

![image-20220701224716077](../image/image-20220701224716077.png)







## UDF

[^UDF]:UserDefinedFunction

> ç”¨æˆ·å®šä¹‰å‡½æ•° ï¼Œé€šè¿‡spark.udfåŠŸèƒ½æ·»åŠ è‡ªå®šä¹‰å‡½æ•°ï¼Œå®ç°è‡ªå®šä¹‰åŠŸèƒ½
>

```scala
package com.hjc.sql_15

import org.apache.spark.sql.{DataFrame, SparkSession}

object SparkSQL02_UDF {
  def main(args: Array[String]): Unit = {
    val sparkSession: SparkSession = SparkSession.builder()
      .master("local[2]")
      .appName("SparkSQL")
      .getOrCreate()
    import sparkSession.implicits._

    val df: DataFrame = sparkSession.read.json("user.json")

    df.createTempView("user")

    //å¦‚ä½•è‡ªå®šä¹‰UDFå‡½æ•°
    sparkSession.udf.register("addpre", (x: String, y: String) => y + x)

    sparkSession.sql("select addpre(name, 'æˆ‘æ˜¯å‰ç¼€:') from user").show

    sparkSession.close()
  }
}
```



## UDAF

[^UDAF]: UserDefinedAggregateFunction

> ç”¨æˆ·å®šä¹‰èšåˆå‡½æ•°  Scala æä¾›äº†ä¸¤ç§è‡ªå®šä¹‰èšåˆå‡½æ•°çš„æ–¹æ³•
>

- æœ‰ç±»å‹çš„è‡ªå®šä¹‰èšåˆå‡½æ•°ï¼Œä¸»è¦é€‚ç”¨äº DataSet
- æ— ç±»å‹çš„è‡ªå®šä¹‰èšåˆå‡½æ•°ï¼Œä¸»è¦é€‚ç”¨äº DataFrame



è¿™é‡Œä»¥è®¡ç®—å¹´é¾„çš„å¹³å‡å€¼ä¸ºä¾‹

```json
// åŸæ•°æ®
{"username": "zhangsan","age": 30}
{"username": "lisi","age": 40}
{"username": "wangwu","age": 50}
{"username": "zhaoliu","age": 60}
```

**ä¸¤ç§è‡ªå®šä¹‰æ–¹å¼åˆ†åˆ«å¦‚ä¸‹ğŸ‘‡ï¼š**

### æœ‰ç±»å‹çš„è‡ªå®šä¹‰èšåˆå‡½æ•°

------

```scala
package com.hjc.sql_bili

import org.apache.spark.SparkConf
import org.apache.spark.sql.expressions.{Aggregator, MutableAggregationBuffer, UserDefinedAggregateFunction}
import org.apache.spark.sql.types.{DataType, LongType, StructField, StructType}
import org.apache.spark.sql.{Encoder, Encoders, Row, SparkSession, functions}

object Spark03_SparkSQL_UDAF1 {

    def main(args: Array[String]): Unit = {

        // TODO åˆ›å»ºSparkSQLçš„è¿è¡Œç¯å¢ƒ						ğŸ‘‡ åˆ†åŒºæ•°ä¹Ÿæ˜¯è°ƒç”¨æ ¸æ•°
        val sparkConf = new SparkConf().setMaster("local[2]").setAppName("sparkSQL")
        val spark = SparkSession.builder().config(sparkConf).getOrCreate()

        val df = spark.read.json("datas/user.json")
        df.createOrReplaceTempView("user")

        spark.udf.register("ageAvg", functions.udaf(new MyAvgUDAF()))

        spark.sql("select ageAvg(age) from user").show


        // TODO å…³é—­ç¯å¢ƒ
        spark.close()
    }
    /*
     è‡ªå®šä¹‰èšåˆå‡½æ•°ç±»ï¼šè®¡ç®—å¹´é¾„çš„å¹³å‡å€¼
     1. ç»§æ‰¿org.apache.spark.sql.expressions.Aggregator, å®šä¹‰æ³›å‹
         IN : è¾“å…¥çš„æ•°æ®ç±»å‹ Long
         BUF : ç¼“å†²åŒºçš„æ•°æ®ç±»å‹ Buff
         OUT : è¾“å‡ºçš„æ•°æ®ç±»å‹ Long
     2. é‡å†™æ–¹æ³•(6)
     */
    case class Buff( var total:Long, var count:Long )
    class MyAvgUDAF extends Aggregator[Long, Buff, Long]{
        // z & zero : åˆå§‹å€¼æˆ–é›¶å€¼
        // ç¼“å†²åŒºçš„åˆå§‹åŒ–
        override def zero: Buff = {
            Buff(0L,0L)
        }

        // æ ¹æ®è¾“å…¥çš„æ•°æ®æ›´æ–°ç¼“å†²åŒºçš„æ•°æ®
        override def reduce(buff: Buff, in: Long): Buff = {
            buff.total = buff.total + in
            buff.count = buff.count + 1
            buff
        }

        // åˆå¹¶ç¼“å†²åŒº
        override def merge(buff1: Buff, buff2: Buff): Buff = {
            buff1.total = buff1.total + buff2.total
            buff1.count = buff1.count + buff2.count
            buff1
        }

        //è®¡ç®—ç»“æœ
        override def finish(buff: Buff): Long = {
            buff.total / buff.count
        }

        // ç¼“å†²åŒºçš„ç¼–ç æ“ä½œ
        override def bufferEncoder: Encoder[Buff] = Encoders.product

        // è¾“å‡ºçš„ç¼–ç æ“ä½œ
        override def outputEncoder: Encoder[Long] = Encoders.scalaLong
    }
}

```

[^ç»§æ‰¿ç±»]: Aggregator
[^æ‰§è¡Œå‰éœ€æ³¨å†Œ]: spark.udf.register("ageAvg", 	functions.udaf(new MyAvgUDAF()))
[^bufferEncoder]:è‡ªå®šä¹‰ç±»å‹ Case Class æˆ–è€…å…ƒç»„å°±ä½¿ç”¨ `Encoders.product` æ–¹æ³•
[^outputEncoder]:åŸºæœ¬ç±»å‹å°±ä½¿ç”¨å…¶å¯¹åº”åç§°çš„æ–¹æ³•ï¼Œå¦‚ `scalaByte `ï¼Œ`scalaFloat`ï¼Œ`scalaShort` ç­‰



[^è¯¦ç»†æµç¨‹å›¾]: ğŸ‘‡

![image-20220702174343504](../image/image-20220702174343504.png)



### æ— ç±»å‹çš„è‡ªå®šä¹‰èšåˆå‡½æ•°

------



```scala
package com.hjc.sql_bili

import org.apache.spark.SparkConf
import org.apache.spark.sql.{Row, SparkSession}
import org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction}
import org.apache.spark.sql.types.{DataType, LongType, StructField, StructType}

object Spark03_SparkSQL_UDAF {

    def main(args: Array[String]): Unit = {

        // TODO åˆ›å»ºSparkSQLçš„è¿è¡Œç¯å¢ƒ
        val sparkConf = new SparkConf().setMaster("local[*]").setAppName("sparkSQL")
        val spark = SparkSession.builder().config(sparkConf).getOrCreate()

        val df = spark.read.json("datas/user.json")
        df.createOrReplaceTempView("user")

        spark.udf.register("ageAvg", new MyAvgUDAF())

        spark.sql("select ageAvg(age) from user").show


        // TODO å…³é—­ç¯å¢ƒ
        spark.close()
    }
    /*
     è‡ªå®šä¹‰èšåˆå‡½æ•°ç±»ï¼šè®¡ç®—å¹´é¾„çš„å¹³å‡å€¼
     1. ç»§æ‰¿UserDefinedAggregateFunction
     2. é‡å†™æ–¹æ³•(8)
     */
    class MyAvgUDAF extends UserDefinedAggregateFunction{
        // è¾“å…¥æ•°æ®çš„ç»“æ„ : Int
        override def inputSchema: StructType = {
            StructType(
                Array(
                    StructField("age", LongType)
                )
            )
        }
        // ç¼“å†²åŒºæ•°æ®çš„ç»“æ„ : Buffer
        override def bufferSchema: StructType = {
            StructType(
                Array(
                    StructField("total", LongType),
                    StructField("count", LongType)
                )
            )
        }

        // å‡½æ•°è®¡ç®—ç»“æœçš„æ•°æ®ç±»å‹ï¼šOut
        override def dataType: DataType = LongType

        // å‡½æ•°çš„ç¨³å®šæ€§
        override def deterministic: Boolean = true

        // ç¼“å†²åŒºåˆå§‹åŒ–
        override def initialize(buffer: MutableAggregationBuffer): Unit = {
            //buffer(0) = 0L
            //buffer(1) = 0L

            buffer.update(0, 0L)
            buffer.update(1, 0L)
        }

        // æ ¹æ®è¾“å…¥çš„å€¼æ›´æ–°ç¼“å†²åŒºæ•°æ®
        override def update(buffer: MutableAggregationBuffer, input: Row): Unit = {
            buffer.update(0, buffer.getLong(0)+input.getLong(0))
            buffer.update(1, buffer.getLong(1)+1)
        }

        // ç¼“å†²åŒºæ•°æ®åˆå¹¶
        override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {
            buffer1.update(0, buffer1.getLong(0) + buffer2.getLong(0))
            buffer1.update(1, buffer1.getLong(1) + buffer2.getLong(1))
        }

        // è®¡ç®—å¹³å‡å€¼
        override def evaluate(buffer: Row): Any = {
            buffer.getLong(0)/buffer.getLong(1)
        }
    }
}
```





### æ—©æœŸç‰ˆæœ¬æœ‰ç±»å‹UDAFæ“ä½œ

> æ—©æœŸç‰ˆæœ¬ä¸­ï¼Œsparkä¸èƒ½åœ¨sqlä¸­ä½¿ç”¨å¼ºç±»å‹UDAFæ“ä½œ



```scala
package com.hjc.sql_bili

import org.apache.spark.SparkConf
import org.apache.spark.sql.expressions.Aggregator
import org.apache.spark.sql.{Dataset, Encoder, Encoders, SparkSession, TypedColumn, functions}

object Spark03_SparkSQL_UDAF2 {

    def main(args: Array[String]): Unit = {

        // TODO åˆ›å»ºSparkSQLçš„è¿è¡Œç¯å¢ƒ
        val sparkConf = new SparkConf().setMaster("local[*]").setAppName("sparkSQL")
        val spark = SparkSession.builder().config(sparkConf).getOrCreate()
        import spark.implicits._
        val df = spark.read.json("datas/user.json")

        // æ—©æœŸç‰ˆæœ¬ä¸­ï¼Œsparkä¸èƒ½åœ¨sqlä¸­ä½¿ç”¨å¼ºç±»å‹UDAFæ“ä½œ
        // SQL & DSL
        // æ—©æœŸçš„UDAFå¼ºç±»å‹èšåˆå‡½æ•°ä½¿ç”¨DSLè¯­æ³•æ“ä½œ
        val ds: Dataset[User] = df.as[User]

        // å°†UDAFå‡½æ•°è½¬æ¢ä¸ºæŸ¥è¯¢çš„åˆ—å¯¹è±¡
        val udafCol: TypedColumn[User, Long] = new MyAvgUDAF().toColumn

        ds.select(udafCol).show


        // TODO å…³é—­ç¯å¢ƒ
        spark.close()
    }
    /*
     è‡ªå®šä¹‰èšåˆå‡½æ•°ç±»ï¼šè®¡ç®—å¹´é¾„çš„å¹³å‡å€¼
     1. ç»§æ‰¿org.apache.spark.sql.expressions.Aggregator, å®šä¹‰æ³›å‹
         IN : è¾“å…¥çš„æ•°æ®ç±»å‹ User
         BUF : ç¼“å†²åŒºçš„æ•°æ®ç±»å‹ Buff
         OUT : è¾“å‡ºçš„æ•°æ®ç±»å‹ Long
     2. é‡å†™æ–¹æ³•(6)
     */
    case class User(username:String, age:Long)
    case class Buff( var total:Long, var count:Long )
    class MyAvgUDAF extends Aggregator[User, Buff, Long]{
        // z & zero : åˆå§‹å€¼æˆ–é›¶å€¼
        // ç¼“å†²åŒºçš„åˆå§‹åŒ–
        override def zero: Buff = {
            Buff(0L,0L)
        }

        // æ ¹æ®è¾“å…¥çš„æ•°æ®æ›´æ–°ç¼“å†²åŒºçš„æ•°æ®
        override def reduce(buff: Buff, in: User): Buff = {
            buff.total = buff.total + in.age
            buff.count = buff.count + 1
            buff
        }

        // åˆå¹¶ç¼“å†²åŒº
        override def merge(buff1: Buff, buff2: Buff): Buff = {
            buff1.total = buff1.total + buff2.total
            buff1.count = buff1.count + buff2.count
            buff1
        }

        //è®¡ç®—ç»“æœ
        override def finish(buff: Buff): Long = {
            buff.total / buff.count
        }

        // ç¼“å†²åŒºçš„ç¼–ç æ“ä½œ
        override def bufferEncoder: Encoder[Buff] = Encoders.product

        // è¾“å‡ºçš„ç¼–ç æ“ä½œ
        override def outputEncoder: Encoder[Long] = Encoders.scalaLong
    }
}
```

æ­¥éª¤

1. è½¬æ¢ä¸ºDataset

   ```scala
   val ds: Dataset[User] = df.as[User]
   ```

2. å°†å‡½æ•°è½¬æ¢ä¸ºåˆ—å¯¹è±¡

   ```scala
   val udafCol: TypedColumn[User, Long] = new MyAvgUDAF().toColumn
   ```

3. æ‰§è¡Œ

   ```scala
   ds.select(udafCol).show
   ```

   



## è¯»å†™æ•°æ®

> Spark æ”¯æŒä»¥ä¸‹å…­ä¸ªæ ¸å¿ƒæ•°æ®æºï¼ŒåŒæ—¶ Spark ç¤¾åŒºè¿˜æä¾›äº†å¤šè¾¾ä¸Šç™¾ç§æ•°æ®æºçš„è¯»å–æ–¹å¼ï¼Œèƒ½å¤Ÿæ»¡è¶³ç»å¤§éƒ¨åˆ†ä½¿ç”¨åœºæ™¯

- CSV
- JSON
- Parquet
- ORC
- JDBC/ODBC connections
- Plain-text files



**é¢„å®šä¹‰æ•°æ®æ ¼å¼**

- å®šä¹‰æ ¼å¼

  ```scala
  val myManualSchema = new StructType(Array(
      StructField("deptno", LongType, nullable = false),
      StructField("dname", StringType,nullable = true),
      StructField("loc", StringType,nullable = true)
  ))
  ```

- ä½¿ç”¨æ–¹å¼

  ```scala
  .schema(myManualSchema) 
  ```

  



### è¯»æ•°æ®æ ¼å¼

æ‰€æœ‰è¯»å– API éµå¾ªä»¥ä¸‹è°ƒç”¨æ ¼å¼ï¼š

```scala
// æ ¼å¼
DataFrameReader.format(...).option("key", "value").schema(...).load()

// ç¤ºä¾‹
spark.read.format("csv")
.option("mode", "FAILFAST")          // è¯»å–æ¨¡å¼
.option("inferSchema", "true")       // æ˜¯å¦è‡ªåŠ¨æ¨æ–­ schema
.option("path", "path/to/file(s)")   // æ–‡ä»¶è·¯å¾„
.schema(someSchema)                  // ä½¿ç”¨é¢„å®šä¹‰çš„ schema      
.load()
```



[^è¯»å–æ¨¡å¼ä¸‰ç§é€‰é¡¹]:ğŸ‘‡

| è¯»æ¨¡å¼          | æè¿°                                                         |
| --------------- | ------------------------------------------------------------ |
| `permissive`    | å½“é‡åˆ°æŸåçš„è®°å½•æ—¶ï¼Œå°†å…¶æ‰€æœ‰å­—æ®µè®¾ç½®ä¸º nullï¼Œå¹¶å°†æ‰€æœ‰æŸåçš„è®°å½•æ”¾åœ¨åä¸º _corruption t_record çš„å­—ç¬¦ä¸²åˆ—ä¸­ |
| `dropMalformed` | åˆ é™¤æ ¼å¼ä¸æ­£ç¡®çš„è¡Œ                                           |
| `failFast`      | é‡åˆ°æ ¼å¼ä¸æ­£ç¡®çš„æ•°æ®æ—¶ç«‹å³å¤±è´¥                               |

### å†™æ•°æ®æ ¼å¼

```scala
// æ ¼å¼
DataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save()

//ç¤ºä¾‹
dataframe.write.format("csv")
.option("mode", "OVERWRITE")         //å†™æ¨¡å¼
.option("dateFormat", "yyyy-MM-dd")  //æ—¥æœŸæ ¼å¼
.option("path", "path/to/file(s)")
.mode(SaveMode.Append)
.save()
```



[^å†™æ•°æ®æ¨¡å¼å››ç§é€‰é¡¹ï¼š]:ğŸ‘‡

| Scala/Java               | æè¿°                                                         |
| ------------------------ | ------------------------------------------------------------ |
| `SaveMode.ErrorIfExists` | å¦‚æœç»™å®šçš„è·¯å¾„å·²ç»å­˜åœ¨æ–‡ä»¶ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸ï¼Œè¿™æ˜¯å†™æ•°æ®é»˜è®¤çš„æ¨¡å¼ |
| `SaveMode.Append`        | æ•°æ®ä»¥è¿½åŠ çš„æ–¹å¼å†™å…¥                                         |
| `SaveMode.Overwrite`     | æ•°æ®ä»¥è¦†ç›–çš„æ–¹å¼å†™å…¥                                         |
| `SaveMode.Ignore`        | å¦‚æœç»™å®šçš„è·¯å¾„å·²ç»å­˜åœ¨æ–‡ä»¶ï¼Œåˆ™ä¸åšä»»ä½•æ“ä½œ                   |





### CSV

> CSV æ˜¯ä¸€ç§å¸¸è§çš„æ–‡æœ¬æ–‡ä»¶æ ¼å¼ï¼Œå…¶ä¸­æ¯ä¸€è¡Œè¡¨ç¤ºä¸€æ¡è®°å½•ï¼Œè®°å½•ä¸­çš„æ¯ä¸ªå­—æ®µç”¨é€—å·åˆ†éš”ã€‚

**è¯»å–CSVæ–‡ä»¶**

- è‡ªåŠ¨æ¨æ–­ç±»å‹è¯»å–è¯»å–ç¤ºä¾‹ï¼š

```scala
spark.read.format("csv")
.option("header", "false")        // æ–‡ä»¶ä¸­çš„ç¬¬ä¸€è¡Œæ˜¯å¦ä¸ºåˆ—çš„åç§°
.option("mode", "FAILFAST")      // æ˜¯å¦å¿«é€Ÿå¤±è´¥
.option("inferSchema", "true")   // æ˜¯å¦è‡ªåŠ¨æ¨æ–­ schema
.load("/usr/file/csv/dept.csv")
.show()
```



**å†™å…¥CSVæ–‡ä»¶**

- å¸¸è§„å†™å…¥

  ```scala
  df.write.format("csv").mode("overwrite").save("/tmp/csv/dept2")
  ```

- æŒ‡å®šå…·ä½“çš„åˆ†éš”ç¬¦

  ```scala
  df.write.format("csv").mode("overwrite").option("sep", "\t").save("/tmp/csv/dept2")
  ```





**ğŸ¤³å¯é€‰é…ç½®**

| è¯»\å†™æ“ä½œ |           é…ç½®é¡¹            |                          å¯é€‰å€¼                          |           é»˜è®¤å€¼           |                             æè¿°                             |
| :-------: | :-------------------------: | :------------------------------------------------------: | :------------------------: | :----------------------------------------------------------: |
|   Both    |             seq             |                         ä»»æ„å­—ç¬¦                         |         ,   (é€—å·)         |                            åˆ†éš”ç¬¦                            |
|   Both    |           header            |                       true, false                        |           false            |                æ–‡ä»¶ä¸­çš„ç¬¬ä¸€è¡Œæ˜¯å¦ä¸ºåˆ—çš„åç§°ã€‚                |
|   Read    |           escape            |                         ä»»æ„å­—ç¬¦                         |             \              |                           è½¬ä¹‰å­—ç¬¦                           |
|   Read    |         inferSchema         |                       true, false                        |           false            |                      æ˜¯å¦è‡ªåŠ¨æ¨æ–­åˆ—ç±»å‹                      |
|   Read    |   ignoreLeadingWhiteSpace   |                       true, false                        |           false            |                     æ˜¯å¦è·³è¿‡å€¼å‰é¢çš„ç©ºæ ¼                     |
|   Both    |  ignoreTrailingWhiteSpace   |                       true, false                        |           false            |                     æ˜¯å¦è·³è¿‡å€¼åé¢çš„ç©ºæ ¼                     |
|   Both    |          nullValue          |                         ä»»æ„å­—ç¬¦                         |             â€œâ€             |                  å£°æ˜æ–‡ä»¶ä¸­å“ªä¸ªå­—ç¬¦è¡¨ç¤ºç©ºå€¼                  |
|   Both    |          nanValue           |                         ä»»æ„å­—ç¬¦                         |            NaN             |                å£°æ˜å“ªä¸ªå€¼è¡¨ç¤º NaN æˆ–è€…ç¼ºçœå€¼                 |
|   Both    |         positiveInf         |                         ä»»æ„å­—ç¬¦                         |            Inf             |                            æ­£æ— ç©·                            |
|   Both    |         negativeInf         |                         ä»»æ„å­—ç¬¦                         |            -Inf            |                            è´Ÿæ— ç©·                            |
|   Both    |    compression or codec     | None, uncompressed, bzip2, deflate, gzip, lz4, or snappy |            none            |                         æ–‡ä»¶å‹ç¼©æ ¼å¼                         |
|   Both    |         dateFormat          |      ä»»ä½•èƒ½è½¬æ¢ä¸º Java çš„ SimpleDataFormat çš„å­—ç¬¦ä¸²      |         yyyy-MM-dd         |                           æ—¥æœŸæ ¼å¼                           |
|   Both    |       timestampFormat       |      ä»»ä½•èƒ½è½¬æ¢ä¸º Java çš„ SimpleDataFormat çš„å­—ç¬¦ä¸²      | yyyy-MMddâ€™Tâ€™HH:mm:ss.SSSZZ |                          æ—¶é—´æˆ³æ ¼å¼                          |
|   Read    |         maxColumns          |                         ä»»æ„æ•´æ•°                         |           20480            |                     å£°æ˜æ–‡ä»¶ä¸­çš„æœ€å¤§åˆ—æ•°                     |
|   Read    |      maxCharsPerColumn      |                         ä»»æ„æ•´æ•°                         |          1000000           |                  å£°æ˜ä¸€ä¸ªåˆ—ä¸­çš„æœ€å¤§å­—ç¬¦æ•°ã€‚                  |
|   Read    |        escapeQuotes         |                       true, false                        |            true            |                   æ˜¯å¦åº”è¯¥è½¬ä¹‰è¡Œä¸­çš„å¼•å·ã€‚                   |
|   Read    | maxMalformedLogPerPartition |                         ä»»æ„æ•´æ•°                         |             10             | å£°æ˜æ¯ä¸ªåˆ†åŒºä¸­æœ€å¤šå…è®¸å¤šå°‘æ¡æ ¼å¼é”™è¯¯çš„æ•°æ®ï¼Œè¶…è¿‡è¿™ä¸ªå€¼åæ ¼å¼é”™è¯¯çš„æ•°æ®å°†ä¸ä¼šè¢«è¯»å– |
|   Write   |          quoteAll           |                       true, false                        |           false            | æŒ‡å®šæ˜¯å¦åº”è¯¥å°†æ‰€æœ‰å€¼éƒ½æ‹¬åœ¨å¼•å·ä¸­ï¼Œè€Œä¸åªæ˜¯è½¬ä¹‰å…·æœ‰å¼•å·å­—ç¬¦çš„å€¼ã€‚ |
|   Read    |          multiLine          |                       true, false                        |           false            |                 æ˜¯å¦å…è®¸æ¯æ¡å®Œæ•´è®°å½•è·¨åŸŸå¤šè¡Œ                 |



### JSON

------

**è¯»å–JSONæ–‡ä»¶**

> éœ€è¦æ³¨æ„çš„æ˜¯ï¼šé»˜è®¤ä¸æ”¯æŒä¸€æ¡æ•°æ®è®°å½•è·¨è¶Šå¤šè¡Œ    å¯ä»¥é€šè¿‡é…ç½® multiLine ä¸º trueæ¥è¿›è¡Œæ›´æ”¹ï¼Œå…¶é»˜è®¤å€¼ä¸º false

```scala
spark.read.format("json").option("mode", "FAILFAST").load("/usr/file/json/dept.json").show(5)
```



```scala
// é»˜è®¤æ”¯æŒå•è¡Œ
{"DEPTNO": 10,"DNAME": "ACCOUNTING","LOC": "NEW YORK"}

//é»˜è®¤ä¸æ”¯æŒå¤šè¡Œ
{
  "DEPTNO": 10,
  "DNAME": "ACCOUNTING",
  "LOC": "NEW YORK"
}
```



**å†™å…¥JSONæ–‡ä»¶**

```scala
df.write.format("json").mode("overwrite").save("/tmp/spark/json/dept")
```



**ğŸ¤³å¯é€‰é…ç½®**

| è¯»\å†™æ“ä½œ | é…ç½®é¡¹                             | å¯é€‰å€¼                                                   | é»˜è®¤å€¼                           |
| --------- | ---------------------------------- | -------------------------------------------------------- | -------------------------------- |
| Both      | compression or codec               | None, uncompressed, bzip2, deflate, gzip, lz4, or snappy | none                             |
| Both      | dateFormat                         | ä»»ä½•èƒ½è½¬æ¢ä¸º Java çš„ SimpleDataFormat çš„å­—ç¬¦ä¸²           | yyyy-MM-dd                       |
| Both      | timestampFormat                    | ä»»ä½•èƒ½è½¬æ¢ä¸º Java çš„ SimpleDataFormat çš„å­—ç¬¦ä¸²           | yyyy-MMddâ€™Tâ€™HH:mm:ss.SSSZZ       |
| Read      | primitiveAsString                  | true, false                                              | false                            |
| Read      | allowComments                      | true, false                                              | false                            |
| Read      | allowUnquotedFieldNames            | true, false                                              | false                            |
| Read      | allowSingleQuotes                  | true, false                                              | true                             |
| Read      | allowNumericLeadingZeros           | true, false                                              | false                            |
| Read      | allowBackslashEscapingAnyCharacter | true, false                                              | false                            |
| Read      | columnNameOfCorruptRecord          | true, false                                              | Value of spark.sql.column&NameOf |
| Read      | multiLine                          | true, false                                              | false                            |



### Parquet

------

> Parquet æ˜¯ä¸€ä¸ªå¼€æºçš„é¢å‘åˆ—çš„æ•°æ®å­˜å‚¨ï¼Œå®ƒæä¾›äº†å¤šç§å­˜å‚¨ä¼˜åŒ–ï¼Œå…è®¸è¯»å–å•ç‹¬çš„åˆ—éæ•´ä¸ªæ–‡ä»¶ï¼Œè¿™ä¸ä»…èŠ‚çœäº†å­˜å‚¨ç©ºé—´è€Œä¸”æå‡äº†è¯»å–æ•ˆç‡ï¼Œå®ƒæ˜¯ Spark æ˜¯é»˜è®¤çš„æ–‡ä»¶æ ¼å¼ã€‚

- **è¯»å–Parquetæ–‡ä»¶**

  ```scala
  spark.read.format("parquet").load("/usr/file/parquet/dept.parquet").show(5)
  ```

  

- **å†™å…¥Parquetæ–‡ä»¶**

  ```scala
  df.write.format("parquet").mode("overwrite").save("/tmp/spark/parquet/dept")
  ```

  

- **å¯é€‰é…ç½®**

  Parquet æ–‡ä»¶æœ‰ç€è‡ªå·±çš„å­˜å‚¨è§„åˆ™ï¼Œå› æ­¤å…¶å¯é€‰é…ç½®é¡¹æ¯”è¾ƒå°‘ï¼Œå¸¸ç”¨çš„æœ‰å¦‚ä¸‹ä¸¤ä¸ªï¼š

| è¯»å†™æ“ä½œ | é…ç½®é¡¹               | å¯é€‰å€¼                                                   | é»˜è®¤å€¼                                       | æè¿°                                                         |
| -------- | -------------------- | -------------------------------------------------------- | -------------------------------------------- | ------------------------------------------------------------ |
| Write    | compression or codec | None, uncompressed, bzip2, deflate, gzip, lz4, or snappy | None                                         | å‹ç¼©æ–‡ä»¶æ ¼å¼                                                 |
| Read     | mergeSchema          | true, false                                              | å–å†³äºé…ç½®é¡¹ `spark.sql.parquet.mergeSchema` | å½“ä¸ºçœŸæ—¶ï¼ŒParquet æ•°æ®æºå°†æ‰€æœ‰æ•°æ®æ–‡ä»¶æ”¶é›†çš„ Schema åˆå¹¶åœ¨ä¸€èµ·ï¼Œå¦åˆ™å°†ä»æ‘˜è¦æ–‡ä»¶ä¸­é€‰æ‹© Schemaï¼Œå¦‚æœæ²¡æœ‰å¯ç”¨çš„æ‘˜è¦æ–‡ä»¶ï¼Œåˆ™ä»éšæœºæ•°æ®æ–‡ä»¶ä¸­é€‰æ‹© Schemaã€‚ |

[æ›´å¤šé…ç½®ç‚¹æˆ‘](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html)



### ORC

> ORC æ˜¯ä¸€ç§è‡ªæè¿°çš„ã€ç±»å‹æ„ŸçŸ¥çš„åˆ—æ–‡ä»¶æ ¼å¼ï¼Œå®ƒé’ˆå¯¹å¤§å‹æ•°æ®çš„è¯»å†™è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä¹Ÿæ˜¯å¤§æ•°æ®ä¸­å¸¸ç”¨çš„æ–‡ä»¶æ ¼å¼ã€‚

- è¯»å–ORCæ–‡ä»¶

  ```scala
  spark.read.format("orc").load("/usr/file/orc/dept.orc").show(5)
  ```

  

- å†™å…¥ORCæ–‡ä»¶

  ```scala
  csvFile.write.format("orc").mode("overwrite").save("/tmp/spark/orc/dept")
  ```

  



### MYSQL

> Spark åŒæ ·æ”¯æŒä¸ä¼ ç»Ÿçš„å…³ç³»å‹æ•°æ®åº“è¿›è¡Œæ•°æ®è¯»å†™ã€‚ä½†æ˜¯ Spark ç¨‹åºé»˜è®¤æ˜¯æ²¡æœ‰æä¾›æ•°æ®åº“é©±åŠ¨çš„ï¼Œæ‰€ä»¥åœ¨ä½¿ç”¨å‰éœ€è¦å°†å¯¹åº”çš„æ•°æ®åº“é©±åŠ¨ä¸Šä¼ åˆ°å®‰è£…ç›®å½•ä¸‹çš„ `jars` ç›®å½•ä¸­ã€‚ä¸‹é¢ç¤ºä¾‹ä½¿ç”¨çš„æ˜¯ Mysql æ•°æ®åº“ï¼Œä½¿ç”¨å‰éœ€è¦å°†å¯¹åº”çš„ `mysql-connector-java-x.x.x.jar` ä¸Šä¼ åˆ° `jars` ç›®å½•ä¸‹ã€‚
>

 **è¯»å†™æ•°æ®** 

- å¯¼å…¥ä¾èµ–

  ```xml
  <dependency>
      <groupId>mysql</groupId>
      <artifactId>mysql-connector-java</artifactId>
      <version>5.1.27</version>
  </dependency>
  
  ```

- ç¼–å†™ä»£ç 

  ```scala
  package com.hjc.sql_bili
  
  import org.apache.spark.SparkConf
  import org.apache.spark.sql.expressions.Aggregator
  import org.apache.spark.sql._
  
  object Spark04_SparkSQL_JDBC {
  
      def main(args: Array[String]): Unit = {
  
          // TODO åˆ›å»ºSparkSQLçš„è¿è¡Œç¯å¢ƒ
          val sparkConf = new SparkConf().setMaster("local[*]").setAppName("sparkSQL")
          val spark = SparkSession.builder().config(sparkConf).getOrCreate()
          import spark.implicits._
  
          // è¯»å–MySQLæ•°æ®
          val df = spark.read
                  .format("jdbc")
                  .option("url", "jdbc:mysql://linux1:3306/spark-sql")
                  .option("driver", "com.mysql.jdbc.Driver")
                  .option("user", "root")
                  .option("password", "123123")
                  .option("dbtable", "user")
                  .load()
          //df.show
  
          // ä¿å­˜æ•°æ®
          df.write
                  .format("jdbc")
                  .option("url", "jdbc:mysql://linux1:3306/spark-sql")
                  .option("driver", "com.mysql.jdbc.Driver")
                  .option("user", "root")
                  .option("password", "123123")
                  .option("dbtable", "user1")
                  .mode(SaveMode.Append)
                  .save()
  
  
          // TODO å…³é—­ç¯å¢ƒ
          spark.close()
      }
  }
  ```

  

**ğŸ¤³å¯é€‰é…ç½®**

- |                å±æ€§åç§°                |                             å«ä¹‰                             |
  | :------------------------------------: | :----------------------------------------------------------: |
  |                  url                   |                          æ•°æ®åº“åœ°å€                          |
  |                dbtable                 |                            è¡¨åç§°                            |
  |                 driver                 |                          æ•°æ®åº“é©±åŠ¨                          |
  | partitionColumn, lowerBound, upperBoun |                     åˆ†åŒºæ€»æ•°ï¼Œä¸Šç•Œï¼Œä¸‹ç•Œ                     |
  |             numPartitions              | å¯ç”¨äºè¡¨è¯»å†™å¹¶è¡Œæ€§çš„æœ€å¤§åˆ†åŒºæ•°ã€‚å¦‚æœè¦å†™çš„åˆ†åŒºæ•°é‡è¶…è¿‡è¿™ä¸ªé™åˆ¶ï¼Œé‚£ä¹ˆå¯ä»¥è°ƒç”¨ coalesce(numpartition) é‡ç½®åˆ†åŒºæ•°ã€‚ |
  |               fetchsize                |      æ¯æ¬¡å¾€è¿”è¦è·å–å¤šå°‘è¡Œæ•°æ®ã€‚æ­¤é€‰é¡¹ä»…é€‚ç”¨äºè¯»å–æ•°æ®ã€‚      |
  |               batchsize                | æ¯æ¬¡å¾€è¿”æ’å…¥å¤šå°‘è¡Œæ•°æ®ï¼Œè¿™ä¸ªé€‰é¡¹åªé€‚ç”¨äºå†™å…¥æ•°æ®ã€‚é»˜è®¤å€¼æ˜¯ 1000ã€‚ |
  |             isolationLevel             | äº‹åŠ¡éš”ç¦»çº§åˆ«ï¼šå¯ä»¥æ˜¯ NONEï¼ŒREAD_COMMITTED, READ_UNCOMMITTEDï¼ŒREPEATABLE_READ æˆ– SERIALIZABLEï¼Œå³æ ‡å‡†äº‹åŠ¡éš”ç¦»çº§åˆ«ã€‚ é»˜è®¤å€¼æ˜¯ READ_UNCOMMITTEDã€‚è¿™ä¸ªé€‰é¡¹åªé€‚ç”¨äºæ•°æ®è¯»å–ã€‚ |
  |           createTableOptions           |               å†™å…¥æ•°æ®æ—¶è‡ªå®šä¹‰åˆ›å»ºè¡¨çš„ç›¸å…³é…ç½®               |
  |         createTableColumnTypes         |                å†™å…¥æ•°æ®æ—¶è‡ªå®šä¹‰åˆ›å»ºåˆ—çš„åˆ—ç±»å‹                |



[æ›´å¤šé…ç½®ç‚¹æˆ‘](https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html)





### Text

> Text æ–‡ä»¶åœ¨è¯»å†™æ€§èƒ½æ–¹é¢å¹¶æ²¡æœ‰ä»»ä½•ä¼˜åŠ¿ï¼Œä¸”ä¸èƒ½è¡¨è¾¾æ˜ç¡®çš„æ•°æ®ç»“æ„ï¼Œæ‰€ä»¥å…¶ä½¿ç”¨çš„æ¯”è¾ƒå°‘ï¼Œè¯»å†™æ“ä½œå¦‚ä¸‹ï¼š
>

- è¯»å–Textæ•°æ®

- ```
  spark.read.textFile("/usr/file/txt/dept.txt").show()
  ```

-  å†™å…¥Textæ•°æ®

- ```
  df.write.text("/tmp/spark/txt/dept")
  ```






### Hive

------

> spark-shellé»˜è®¤æ˜¯æ”¯æŒHive



#### å†…åµŒç‰ˆ[^sparkå†…åµŒçš„hiveä¸€èˆ¬ä¸ç”¨]

- å¯åŠ¨Sparkshell

- åˆ›å»ºä¸´æ—¶è¡¨

  ```scala
  scala> spark.sql("show tables").show
  ã€‚ã€‚ã€‚
  +--------+---------+-----------+
  |database|tableName|isTemporary|
  +--------+---------+-----------+
  +--------+---------+-----------+
  
  scala> spark.sql("create table aa(id int)")
  
  ã€‚ã€‚ã€‚
  
  scala> spark.sql("show tables").show
  +--------+---------+-----------+
  |database|tableName|isTemporary|
  +--------+---------+-----------+
  | default|       aa|      false|
  +--------+---------+-----------+
  
  ```

- è£…è½½æ•°æ®

  ```scala
  scala> spark.sql("load data local inpath 'input/ids.txt' into table aa")
  
  ã€‚ã€‚ã€‚
  
  scala> spark.sql("select * from aa").show
  +---+
  | id|
  +---+
  |  1|
  |  2|
  |  3|
  |  4|
  +---+
  ```

  

[^PS]: åœ¨å®é™…ä½¿ç”¨ä¸­, å‡ ä¹æ²¡æœ‰ä»»ä½•äººä¼šä½¿ç”¨å†…ç½®çš„ Hive



#### å¤–åµŒç‰ˆ

------

==æ³¨æ„ï¼šä½¿ç”¨SparkSQL    HIVEå¤–åµŒæ“ä½œ  éœ€è¦å¼€å¯çš„åº”ç”¨==



**ğŸ‘‰å‡†å¤‡å·¥ä½œ**

- å°†Hiveçš„hive-site.xmlæ‹·è´åˆ°sparkçš„ conf/ç›®å½•ä¸‹

- Mysqlçš„é©±åŠ¨copyåˆ°spark  jars/ç›®å½•ä¸‹

- æŒ‰é¡ºåºå¯åŠ¨

  zk	ğŸ‘‰	Hadoopé›†ç¾¤	ğŸ‘‰	hiveservices.sh	ğŸ‘‰	spark-shell



**ä¸ºäº†æ–¹ä¾¿æŸ¥è¯¢å¯ä»¥å¯åŠ¨SparkSQL  æ¥ä»£æ›¿spark-shell**

- ```sh
  $bin/spark-sql
  ```





**ä¸ºäº†æ— ç¼å…¼å®¹HiveServer2	è¿è¡ŒSpark beelineæ¥ä»£æ›¿SparkSQL[^SparkSQLä¸»è¦æ˜¯åœ¨æŸ¥è¯¢æ—¶è§†å›¾ä¸å¤Ÿå®Œç¾]**

- åœ¨å‡†å¤‡å·¥ä½œçš„åŸºç¡€ä¸Š

  å¦‚æœè®¿é—®ä¸åˆ°hdfsï¼Œåˆ™éœ€è¦æŠŠcore-site.xmlå’Œhdfs-site.xmlæ‹·è´åˆ°conf/ç›®å½•ä¸‹

- å¯åŠ¨Thrift Server

  ```sh
  sbin/start-thriftserver.sh
  ```

- ä½¿ç”¨beelineè¿æ¥Thrift Server

  ```sh
  bin/beeline -u jdbc:hive2://hadoop102:10000 -n root
  ```

  



#### ä»£ç ç‰ˆ

------

**å¯¼å…¥ä¾èµ–**

- ```xml
  <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-hive_2.12</artifactId>
      <version>3.0.0</version>
  </dependency>
  
  <dependency>
      <groupId>org.apache.hive</groupId>
      <artifactId>hive-exec</artifactId>
      <version>1.2.1</version>
  </dependency>
  <dependency>
      <groupId>mysql</groupId>
      <artifactId>mysql-connector-java</artifactId>
      <version>5.1.27</version>
  </dependency>
  ```

- å°†hive-site.xmlæ–‡ä»¶æ‹·è´åˆ°é¡¹ç›®çš„resourcesç›®å½•ä¸­

- ä»£ç å®ç°

  ```scala
  package com.hjc.sql_bili
  
  import org.apache.spark.SparkConf
  import org.apache.spark.sql._
  
  object Spark05_SparkSQL_Hive {
  
      def main(args: Array[String]): Unit = {
          //										 ğŸ‘‡æ­¤å¤„è‡ªå·±é›†ç¾¤çš„ç”¨æˆ·å
          System.setProperty("HADOOP_USER_NAME", "root")
          // TODO åˆ›å»ºSparkSQLçš„è¿è¡Œç¯å¢ƒ
          val sparkConf = new SparkConf().setMaster("local[*]").setAppName("sparkSQL")
          val spark = SparkSession.builder().enableHiveSupport().config(sparkConf).getOrCreate()
  
          // ä½¿ç”¨SparkSQLè¿æ¥å¤–ç½®çš„Hive
          // 1. æ‹·è´Hive-size.xmlæ–‡ä»¶åˆ°classpathä¸‹
          // 2. å¯ç”¨Hiveçš„æ”¯æŒ
          // 3. å¢åŠ å¯¹åº”çš„ä¾èµ–å…³ç³»ï¼ˆåŒ…å«MySQLé©±åŠ¨ï¼‰
          spark.sql("show tables").show
  
          // TODO å…³é—­ç¯å¢ƒ
          spark.close()
      }
  }
  ```

  















































