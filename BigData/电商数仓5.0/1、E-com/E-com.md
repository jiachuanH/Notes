

# ä¸€ã€å‰ç½®çŸ¥è¯†ä»‹ç»

## ç®€ä»‹

> æ•°æ®ä»“åº“ï¼ˆ Data Warehouse ï¼‰ï¼Œæ˜¯ä¸ºä¼ä¸šæ‰€æœ‰å†³ç­–åˆ¶å®šè¿‡ç¨‹ï¼Œæä¾›æ‰€æœ‰ç³»ç»Ÿæ•°æ®æ”¯æŒçš„æˆ˜ç•¥é›†åˆã€‚

![image-20220712172549790](../image/image-20220712172549790.png)

é€šè¿‡å¯¹æ•°æ®ä»“åº“ä¸­æ•°æ®çš„åˆ†æï¼Œå¯ä»¥å¸®åŠ©ä¼ä¸šï¼Œæ”¹è¿›ä¸šåŠ¡æµç¨‹ã€æ§åˆ¶æˆæœ¬ã€æé«˜äº§å“è´¨é‡ç­‰ã€‚

==æ•°æ®ä»“åº“ï¼Œå¹¶ä¸æ˜¯æ•°æ®çš„æœ€ç»ˆç›®çš„åœ°ï¼Œè€Œæ˜¯ä¸ºæ•°æ®æœ€ç»ˆçš„ç›®çš„åœ°åšå¥½å‡†å¤‡ã€‚==

è¿™äº›å‡†å¤‡åŒ…æ‹¬å¯¹æ•°æ®çš„ï¼š

- æ¸…æ´—	è½¬ä¹‰	åˆ†ç±»	é‡ç»„	åˆå¹¶	æ‹†åˆ†	ç»Ÿè®¡ç­‰ç­‰



## éœ€æ±‚åŠæ¡†æ¶

### é¡¹ç›®éœ€æ±‚

- ç”¨æˆ·è¡Œä¸ºæ•°æ®	ä¸šåŠ¡æ•°æ® é‡‡é›†å¹³å°æ­å»º
- **æ•°ä»“ç»´åº¦å»ºæ¨¡**
- åˆ†ææŒ‡æ ‡
- **ä½¿ç”¨å³å¸­æŸ¥è¯¢å·¥å…·  éšæ—¶åˆ†ææŒ‡æ ‡**
- **å¯¹é›†ç¾¤æ€§èƒ½ç›‘æ§  å¼‚å¸¸æ—¶æŠ¥è­¦**
- **å…ƒæ•°æ®ç®¡ç†**
- **è´¨é‡ç›‘æ§**



### é¡¹ç›®æ¡†æ¶

#### ğŸ‘‡æŠ€æœ¯é€‰å‹

> æŠ€æœ¯é€‰å‹ä¸»è¦è€ƒè™‘å› ç´ ï¼šæ•°æ®é‡å¤§å°ã€ä¸šåŠ¡éœ€æ±‚ã€è¡Œä¸šå†…ç»éªŒã€æŠ€æœ¯æˆç†Ÿåº¦ã€å¼€å‘ç»´æŠ¤æˆæœ¬ã€æ€»æˆæœ¬é¢„ç®—



| å„å¤§æ¨¡å—       | æ¡†æ¶                                |
| -------------- | ----------------------------------- |
| æ•°æ®é‡‡é›†ä¼ è¾“ï¼š | Flumeï¼ŒDataXï¼ŒMaxwellï¼ŒSqoopï¼ŒKafka |
| æ•°æ®å­˜å‚¨ï¼š     | MySqlï¼ŒHDFSï¼ŒHBaseï¼ŒRedisï¼ŒMongoDB  |
| æ•°æ®è®¡ç®—ï¼š     | Hiveï¼ŒTezï¼Œ Sparkï¼Œ Flinkï¼ŒStorm    |
| æ•°æ®æŸ¥è¯¢ï¼š     | Prestoï¼ŒKylin ï¼ŒImpalaï¼ŒDruid       |
| æ•°æ®å¯è§†åŒ–ï¼š   | Supersetã€QuickBIã€DataV            |
| ä»»åŠ¡è°ƒåº¦ï¼š     | DolphinSchedulerã€Azkabanã€Oozie    |
| é›†ç¾¤ç›‘æ§ï¼š     | Zabbix                              |
| å…ƒæ•°æ®ç®¡ç†     | Atlas                               |

#### ğŸ‘‡æ•°æ®æµç¨‹è®¾è®¡

![image-20220712175510321](../image/image-20220712175510321.png)



#### ğŸ‘‡æ¡†æ¶ç‰ˆæœ¬é€‰æ‹©

- **Apacheï¼š**è¿ç»´éº»çƒ¦ï¼Œç»„ä»¶é—´å…¼å®¹æ€§éœ€è¦è‡ªå·±è°ƒç ”ã€‚[^ï¼ˆä¸€èˆ¬å¤§å‚ä½¿ç”¨ï¼ŒæŠ€æœ¯å®åŠ›é›„åšï¼Œæœ‰ä¸“ä¸šçš„è¿ç»´äººå‘˜ï¼‰ **ï¼ˆ**å»ºè®®ä½¿ç”¨ï¼‰]
- **CDHï¼š**å›½å†…ä½¿ç”¨æœ€å¤šçš„ç‰ˆæœ¬ï¼Œä½†CMä¸å¼€æºï¼Œä»Šå¹´å¼€å§‹è¦æ”¶è´¹ï¼Œä¸€ä¸ªèŠ‚ç‚¹1ä¸‡ç¾é‡‘ã€‚
- **HDPï¼š**å¼€æºï¼Œå¯ä»¥è¿›è¡ŒäºŒæ¬¡å¼€å‘ï¼Œä½†æ˜¯æ²¡æœ‰CDHç¨³å®šï¼Œå›½å†…ä½¿ç”¨è¾ƒå°‘

**ğŸ‘‰ç¡®å®šç‰ˆæœ¬å‹å·**

| **äº§å“**      | **ç‰ˆæœ¬** |
| ------------- | -------- |
| **Java**      | 1.8      |
| **Hadoop**    | 3.1.3    |
| **Hive**      | 3.1.2    |
| **Flume**     | 1.9.0    |
| **Zookeeper** | 3.5.7    |
| **Kafka**     | 2.4.1    |
| **DataX**     | 3.0      |
| **Maxwell**   | 1.29.2   |



#### ğŸ‘‡æœåŠ¡å™¨é€‰å‹

- **ç‰©ç†æœº**

  | 128G | 20æ ¸CPU | 8T æœºæ¢°HDD | 2Tå›ºæ€SSD | æˆ´å°” |
  | ---- | ------- | ---------- | --------- | ---- |

  å”®ä»·  4W   å¯¿å‘½  5 å¹´  	è¿ç»´ä¸€æœˆ  1 W

- **äº‘ä¸»æœº**

  ä¸€å¹´   5W  	ç£ç›˜è´µ     ä¸éœ€è¦è¿ç»´

- **å¦‚ä½•é€‰æ‹©ï¼ŸğŸ¤”**

  - æœ‰é’±çš„å’Œ  ä¸ä¸»æœºå•†  æ— å†²çªé€‰   äº‘
  - ä¸­å°å‹       ä¸ºäº†ä¸Šå¸‚é€‰  äº‘   ä¸Šå¸‚æ‹‰åˆ°èèµ„é€‰  ç‰©ç†
  - æœ‰é•¿æœŸæ‰“ç®—èµ„é‡‘å……è¶³é€‰   ç‰©ç†æœº





#### ğŸ‘‡é›†ç¾¤èµ„æºè§„åˆ’

##### ç¡®è®¤è§„æ¨¡

**å¦‚ä½•ç¡®è®¤é›†ç¾¤è§„æ¨¡ï¼ŸğŸ˜¶**

1. **è®¡ç®—æ•°æ®é‡**

   DAU = 100Wäºº   ä¸€äºº100æ¡		100W*100  =1äº¿æ¡

2. **ç¡®è®¤æ•°æ®æ€»å¤§å°**

   ä¸€æ¡ 1K  ä¸€å¤©1äº¿æ¡		1äº¿  /1024 /1024=  å¤§çº¦100G

3. **è®¡ç®— ä¸€å¹´æˆ–åŠå¹´é‡**

   åŠå¹´ä¸æ‰©å®¹   	100G*180 =18T

4. **ä¹˜ä¸Šå‰¯æœ¬æ•°é‡**

   18T*3  = 54T

5. **é¢„ç•™20%~30%**

   54T  /   0.7  =77T

6. **æ±‡æ€»**

   å¤§çº¦éœ€è¦   8T  çš„æœºå™¨   10 å°

   

##### å…·ä½“è§„åˆ’

â€‹																						**ä»¥ä¸‹ä¸ºæµ‹è¯•è§„åˆ’ğŸ‘‡	**



| æœåŠ¡åç§°           | å­æœåŠ¡           | æœåŠ¡å™¨  hadoop102 | æœåŠ¡å™¨  hadoop103 | æœåŠ¡å™¨  hadoop104 |
| ------------------ | ---------------- | ----------------- | ----------------- | ----------------- |
| HDFS               | NameNode         | âˆš                 |                   |                   |
| DataNode           | âˆš                | âˆš                 | âˆš                 |                   |
| SecondaryNameNode  |                  |                   | âˆš                 |                   |
| Yarn               | NodeManager      | âˆš                 | âˆš                 | âˆš                 |
| Resourcemanager    |                  | âˆš                 |                   |                   |
| Zookeeper          | Zookeeper Server | âˆš                 | âˆš                 | âˆš                 |
| Flume(é‡‡é›†æ—¥å¿—)    | Flume            | âˆš                 | âˆš                 |                   |
| Kafka              | Kafka            | âˆš                 | âˆš                 | âˆš                 |
| Flumeï¼ˆæ¶ˆè´¹Kafkaï¼‰ | Flume            |                   |                   | âˆš                 |
| Hive               | Hive             | âˆš                 |                   |                   |
| MySQL              | MySQL            | âˆš                 |                   |                   |
| DataX              | DataX            | âˆš                 |                   |                   |
| Maxwell            | Maxwell          | âˆš                 |                   |                   |
| Presto             | Coordinator      | âˆš                 |                   |                   |
| Worker             | âˆš                | âˆš                 | âˆš                 |                   |
| DolphinScheduler   | MasterServer     | âˆš                 |                   |                   |
| WorkerServer       | âˆš                | âˆš                 | âˆš                 |                   |
| Druid              | Druid            | âˆš                 | âˆš                 | âˆš                 |
| Kylin              |                  | âˆš                 |                   |                   |
| Hbase              | HMaster          | âˆš                 |                   |                   |
| HRegionServer      | âˆš                | âˆš                 | âˆš                 |                   |
| Superset           |                  | âˆš                 |                   |                   |
| Atlas              |                  | âˆš                 |                   |                   |
| Solr               | Jar              | âˆš                 | âˆš                 | âˆš                 |



# äºŒã€æ­å»ºæµç¨‹

## æ—¥å¿—æ•°æ®é‡‡é›†æ¨¡å—

### ç”¨æˆ·è¡Œä¸ºæ—¥å¿—

#### æ¦‚è¿°

> ç”¨æˆ·è¡Œä¸ºæ—¥å¿—çš„å†…å®¹ï¼Œä¸»è¦åŒ…æ‹¬ç”¨æˆ·çš„å„é¡¹**è¡Œä¸ºä¿¡æ¯**ä»¥åŠè¡Œä¸ºæ‰€å¤„çš„**ç¯å¢ƒä¿¡æ¯**ã€‚æ”¶é›†è¿™äº›ä¿¡æ¯çš„ä¸»è¦ç›®çš„æ˜¯ä¼˜åŒ–äº§å“å’Œä¸ºå„é¡¹åˆ†æç»Ÿè®¡æŒ‡æ ‡æä¾›æ•°æ®æ”¯æ’‘ã€‚

**ğŸ‘‰æ”¶é›†è¿™äº›ä¿¡æ¯çš„æ‰‹æ®µé€šå¸¸ä¸ºåŸ‹ç‚¹ã€‚**

ç›®å‰ä¸»æµçš„åŸ‹ç‚¹æ–¹å¼æœ‰

- ä»£ç åŸ‹ç‚¹ï¼ˆå‰ç«¯/åç«¯ï¼‰
- å¯è§†åŒ–åŸ‹ç‚¹
- å…¨åŸ‹ç‚¹ç­‰

|  åŸ‹ç‚¹æ–¹å¼  |                             ä»‹ç»                             |
| :--------: | :----------------------------------------------------------: |
|  ä»£ç åŸ‹ç‚¹  | æ˜¯é€šè¿‡è°ƒç”¨åŸ‹ç‚¹SDKå‡½æ•°ï¼Œåœ¨éœ€è¦åŸ‹ç‚¹çš„ä¸šåŠ¡é€»è¾‘åŠŸèƒ½ä½ç½®è°ƒç”¨æ¥å£ï¼Œä¸ŠæŠ¥åŸ‹ç‚¹æ•°æ®ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯¹é¡µé¢ä¸­çš„æŸä¸ªæŒ‰é’®åŸ‹ç‚¹åï¼Œå½“è¿™ä¸ªæŒ‰é’®è¢«ç‚¹å‡»æ—¶ï¼Œå¯ä»¥åœ¨è¿™ä¸ªæŒ‰é’®å¯¹åº”çš„ OnClick å‡½æ•°é‡Œé¢è°ƒç”¨SDKæä¾›çš„æ•°æ®å‘é€æ¥å£ï¼Œæ¥å‘é€æ•°æ® |
| å¯è§†åŒ–åŸ‹ç‚¹ | åªéœ€è¦ç ”å‘äººå‘˜é›†æˆé‡‡é›† SDKï¼Œä¸éœ€è¦å†™åŸ‹ç‚¹ä»£ç ï¼Œä¸šåŠ¡äººå‘˜å°±å¯ä»¥é€šè¿‡è®¿é—®åˆ†æå¹³å°çš„â€œåœˆé€‰â€åŠŸèƒ½ï¼Œæ¥â€œåœˆâ€å‡ºéœ€è¦å¯¹ç”¨æˆ·è¡Œä¸ºè¿›è¡Œæ•æ‰çš„æ§ä»¶ï¼Œå¹¶å¯¹è¯¥äº‹ä»¶è¿›è¡Œå‘½åã€‚åœˆé€‰å®Œæ¯•åï¼Œè¿™äº›é…ç½®ä¼šåŒæ­¥åˆ°å„ä¸ªç”¨æˆ·çš„ç»ˆç«¯ä¸Šï¼Œç”±é‡‡é›† SDK æŒ‰ç…§åœˆé€‰çš„é…ç½®è‡ªåŠ¨è¿›è¡Œç”¨æˆ·è¡Œä¸ºæ•°æ®çš„é‡‡é›†å’Œå‘é€ã€‚ |
|   å…¨åŸ‹ç‚¹   | æ˜¯é€šè¿‡åœ¨äº§å“ä¸­åµŒå…¥SDKï¼Œå‰ç«¯è‡ªåŠ¨é‡‡é›†é¡µé¢ä¸Šçš„å…¨éƒ¨ç”¨æˆ·è¡Œä¸ºäº‹ä»¶ï¼Œä¸ŠæŠ¥åŸ‹ç‚¹æ•°æ®ï¼Œç›¸å½“äºåšäº†ä¸€ä¸ªç»Ÿä¸€çš„åŸ‹ç‚¹ã€‚ç„¶åå†é€šè¿‡ç•Œé¢é…ç½®å“ªäº›æ•°æ®éœ€è¦åœ¨ç³»ç»Ÿé‡Œé¢è¿›è¡Œåˆ†æã€‚ |



#### æ—¥å¿—å†…å®¹

------



|   ä¸»è¦å†…å®¹   |                             ä»‹ç»                             |
| :----------: | :----------------------------------------------------------: |
| é¡µé¢æµè§ˆè®°å½• | é¡µé¢æµè§ˆè®°å½•ï¼Œè®°å½•çš„æ˜¯è®¿å®¢å¯¹é¡µé¢çš„æµè§ˆè¡Œä¸ºï¼Œè¯¥è¡Œä¸ºçš„ç¯å¢ƒä¿¡æ¯ä¸»è¦æœ‰ç”¨æˆ·ä¿¡æ¯ã€æ—¶é—´ä¿¡æ¯ã€åœ°ç†ä½ç½®ä¿¡æ¯ã€è®¾å¤‡ä¿¡æ¯ã€åº”ç”¨ä¿¡æ¯ã€æ¸ é“ä¿¡æ¯åŠé¡µé¢ä¿¡æ¯ç­‰ã€‚ |
|   åŠ¨ä½œè®°å½•   | åŠ¨ä½œè®°å½•ï¼Œè®°å½•çš„æ˜¯ç”¨æˆ·çš„ä¸šåŠ¡æ“ä½œè¡Œä¸ºï¼Œè¯¥è¡Œä¸ºçš„ç¯å¢ƒä¿¡æ¯ä¸»è¦æœ‰ç”¨æˆ·ä¿¡æ¯ã€æ—¶é—´ä¿¡æ¯ã€åœ°ç†ä½ç½®ä¿¡æ¯ã€è®¾å¤‡ä¿¡æ¯ã€åº”ç”¨ä¿¡æ¯ã€æ¸ é“ä¿¡æ¯åŠåŠ¨ä½œç›®æ ‡å¯¹è±¡ä¿¡æ¯ç­‰ã€‚ |
|   æ›å…‰è®°å½•   | æ›å…‰è®°å½•ï¼Œè®°å½•çš„æ˜¯æ›å…‰è¡Œä¸ºï¼Œè¯¥è¡Œä¸ºçš„ç¯å¢ƒä¿¡æ¯ä¸»è¦æœ‰ç”¨æˆ·ä¿¡æ¯ã€æ—¶é—´ä¿¡æ¯ã€åœ°ç†ä½ç½®ä¿¡æ¯ã€è®¾å¤‡ä¿¡æ¯ã€åº”ç”¨ä¿¡æ¯ã€æ¸ é“ä¿¡æ¯åŠæ›å…‰å¯¹è±¡ä¿¡æ¯ç­‰ã€‚ |
|   å¯åŠ¨è®°å½•   | å¯åŠ¨è®°å½•ï¼Œè®°å½•çš„æ˜¯ç”¨æˆ·å¯åŠ¨åº”ç”¨çš„è¡Œä¸ºï¼Œè¯¥è¡Œä¸ºçš„ç¯å¢ƒä¿¡æ¯ä¸»è¦æœ‰ç”¨æˆ·ä¿¡æ¯ã€æ—¶é—´ä¿¡æ¯ã€åœ°ç†ä½ç½®ä¿¡æ¯ã€è®¾å¤‡ä¿¡æ¯ã€åº”ç”¨ä¿¡æ¯ã€æ¸ é“ä¿¡æ¯ã€å¯åŠ¨ç±»å‹åŠå¼€å±å¹¿å‘Šä¿¡æ¯ç­‰ã€‚ |
|   é”™è¯¯è®°å½•   | å¯åŠ¨è®°å½•ï¼Œè®°å½•çš„æ˜¯ç”¨æˆ·åœ¨ä½¿ç”¨åº”ç”¨è¿‡ç¨‹ä¸­çš„æŠ¥é”™è¡Œä¸ºï¼Œè¯¥è¡Œä¸ºçš„ç¯å¢ƒä¿¡æ¯ä¸»è¦æœ‰ç”¨æˆ·ä¿¡æ¯ã€æ—¶é—´ä¿¡æ¯ã€åœ°ç†ä½ç½®ä¿¡æ¯ã€è®¾å¤‡ä¿¡æ¯ã€åº”ç”¨ä¿¡æ¯ã€æ¸ é“ä¿¡æ¯ã€ä»¥åŠå¯èƒ½ä¸æŠ¥é”™ç›¸å…³çš„é¡µé¢ä¿¡æ¯ã€åŠ¨ä½œä¿¡æ¯ã€æ›å…‰ä¿¡æ¯å’ŒåŠ¨ä½œä¿¡æ¯ã€‚ |





#### æ—¥å¿—æ ¼å¼

------

**æˆ‘ä»¬çš„æ—¥å¿—ç»“æ„å¤§è‡´å¯åˆ†ä¸ºä¸¤ç±»**

- **é¡µé¢æ—¥å¿—**

  é¡µé¢æ—¥å¿—ï¼Œä»¥é¡µé¢æµè§ˆä¸ºå•ä½ï¼Œå³ä¸€ä¸ªé¡µé¢æµè§ˆè®°å½•ï¼Œç”Ÿæˆä¸€æ¡é¡µé¢åŸ‹ç‚¹æ—¥å¿—ã€‚

  ä¸€æ¡**å®Œæ•´**çš„é¡µé¢æ—¥å¿—åŒ…å«ï¼Œä¸€ä¸ªé¡µé¢æµè§ˆè®°å½•ï¼Œè‹¥å¹²ä¸ªç”¨æˆ·åœ¨è¯¥é¡µé¢æ‰€åšçš„åŠ¨ä½œè®°å½•ï¼Œè‹¥å¹²ä¸ªè¯¥é¡µé¢çš„æ›å…‰è®°å½•ï¼Œä»¥åŠä¸€ä¸ªåœ¨è¯¥é¡µé¢å‘ç”Ÿçš„æŠ¥é”™è®°å½•ã€‚

  é™¤ä¸Šè¿°è¡Œä¸ºä¿¡æ¯ï¼Œé¡µé¢æ—¥å¿—è¿˜åŒ…å«äº†è¿™äº›è¡Œä¸ºæ‰€å¤„çš„å„ç§ç¯å¢ƒä¿¡æ¯ï¼Œ

  åŒ…æ‹¬ç”¨æˆ·ä¿¡æ¯ã€æ—¶é—´ä¿¡æ¯ã€åœ°ç†ä½ç½®ä¿¡æ¯ã€è®¾å¤‡ä¿¡æ¯ã€åº”ç”¨ä¿¡æ¯ã€æ¸ é“ä¿¡æ¯ç­‰ã€‚

- **å¯åŠ¨æ—¥å¿—**

  å¯åŠ¨æ—¥å¿—ä»¥å¯åŠ¨ä¸ºå•ä½ï¼ŒåŠä¸€æ¬¡å¯åŠ¨è¡Œä¸ºï¼Œç”Ÿæˆä¸€æ¡å¯åŠ¨æ—¥å¿—ã€‚

  ä¸€æ¡å®Œæ•´çš„å¯åŠ¨æ—¥å¿—åŒ…æ‹¬ä¸€ä¸ªå¯åŠ¨è®°å½•ï¼Œä¸€ä¸ªæœ¬æ¬¡å¯åŠ¨æ—¶çš„æŠ¥é”™è®°å½•ï¼Œ

  ä»¥åŠå¯åŠ¨æ—¶æ‰€å¤„çš„ç¯å¢ƒä¿¡æ¯ï¼ŒåŒ…æ‹¬ç”¨æˆ·ä¿¡æ¯ã€æ—¶é—´ä¿¡æ¯ã€åœ°ç†ä½ç½®ä¿¡æ¯ã€è®¾å¤‡ä¿¡æ¯ã€åº”ç”¨ä¿¡æ¯ã€æ¸ é“ä¿¡æ¯ç­‰ã€‚

  











â€‹																							**æ•°æ®é€šé“å›¾ğŸ‘‡**

![image-20220712210902790](../image/image-20220712210902790.png)



### ç¯å¢ƒå‡†å¤‡

------

> ä¸‰å°è™šæ‹Ÿæœº    			æ¨¡æ‹Ÿæ•°æ® 				å¸¸ç”¨è„šæœ¬

#### è™šæ‹Ÿæœº

- æ¨¡æ¿æœºå™¨å…‹éš†   æ³¨æ„ï¼š[^ç½‘æ®µå’Œ	ç½‘å…³  windowsçš„hostsæ–‡ä»¶   è™šæ‹Ÿç½‘ç»œç¼–è¾‘å™¨  æ–‡ä»¶å¤¹   é˜²ç«å¢™    ç”¨æˆ·æƒé™ ]

- ä¿®æ”¹ä¸‰å°çš„çš„  IP  hostname  é‡å¯

- é…ç½®å…å¯†  

- å¸è½½JDK  é‡è£…å¹¶é…ç½®JDK

  - ä¸Šä¼ å¹¶è§£å‹

    ```sh
    # tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/
    ```

  - é…ç½®ç¯å¢ƒå˜é‡ 

    ```sh
    # sudo vim /etc/profile.d/my_env.sh
    
    #JAVA_HOME
    export JAVA_HOME=/opt/module/jdk1.8.0_212
    export PATH=$PATH:$JAVA_HOME/bin
    ```

  - åˆ†å‘  å¹¶ä½¿å…¶ç”Ÿæ•ˆ

    ```sh
    $xsync /opt/module/jdk1.8.0_212/
    
    $xcall source /etc/profile.d/my_env.sh
    ```

[xsyncç‚¹æˆ‘](#####xsync)						[xcallç‚¹æˆ‘](#####xcall)







#### æ¨¡æ‹Ÿæ•°æ®

------

> ä¸»è¦åœ¨102  å’Œ103  ä¸Šç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®

- åˆ›å»ºæ–‡ä»¶å¤¹  å¹¶ä¸Šä¼ æ¨¡æ‹Ÿå™¨æ–‡ä»¶

  ```sh
  $ mkdir /opt/module/applog
  
  #å°†application.ymlã€
  #gmall2020-mock-log-2021-10-10.jarã€
  #path.jsonã€
  #logback.xml
  #ä¸Šä¼ åˆ°hadoop102çš„/opt/module/applogç›®å½•ä¸‹
  ```

- ä¿®æ”¹é…ç½®æ–‡ä»¶

  - [^path.json]: è¯¥æ–‡ä»¶ç”¨æ¥é…ç½®è®¿é—®è·¯å¾„ï¼Œå¯ä»¥æ ¹æ®éœ€æ±‚ï¼Œçµæ´»é…ç½®ç”¨æˆ·è®¿é—®è·¯å¾„ã€‚

  - **logbacké…ç½®æ–‡ä»¶**ï¼šé…ç½®æ—¥å¿—ç”Ÿæˆè·¯å¾„

    ```xml
    <?xml version="1.0" encoding="UTF-8"?>
    <configuration>
          <!--				ğŸ‘‡					ğŸ‘‡ -->
        <property name="LOG_HOME" value="/opt/module/applog/log" />
        <appender name="console" class="ch.qos.logback.core.ConsoleAppender">
            <encoder>
                <pattern>%msg%n</pattern>
            </encoder>
        </appender>
    
        <appender name="rollingFile" class="ch.qos.logback.core.rolling.RollingFileAppender">
            <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
                <fileNamePattern>${LOG_HOME}/app.%d{yyyy-MM-dd}.log</fileNamePattern>
            </rollingPolicy>
            <encoder>
                <pattern>%msg%n</pattern>
            </encoder>
        </appender>
        <!-- å°†æŸä¸€ä¸ªåŒ…ä¸‹æ—¥å¿—å•ç‹¬æ‰“å°æ—¥å¿— -->
        <logger name="com.atguigu.gmall2020.mock.log.util.LogUtil"
                level="INFO" additivity="false">
            <appender-ref ref="rollingFile" />
             <appender-ref ref="console" />
        </logger>
    
        <root level="error"  >
            <appender-ref ref="console" />
            <!-- <appender-ref ref="async-rollingFile" />  -->
        </root>
    </configuration>
    
    ```

  - ç”Ÿæˆæ—¥å¿—

    ```sh
    applog]$ java -jar gmall2020-mock-log-2021-10-10.jar
    ```
    
  - ç¼–å†™è‡ªåŠ¨ç”Ÿæˆè„šæœ¬
  
    [lg.sh](#####lg)
  
  



#### å¸¸ç”¨è„šæœ¬

------

> æ³¨æ„  è¦åœ¨ç”¨æˆ·çš„binä¸‹åˆ›å»ºè„šæœ¬  
>
>    åˆ›å»ºåä¿å­˜é€€å‡º   èµ‹æƒ  
>
> ```sh
> chown  +x    $è„šæœ¬åå­—$
> ```

##### xsync

```sh
#!/bin/bash

#1. åˆ¤æ–­å‚æ•°ä¸ªæ•°
if [ $# -lt 1 ]
then
    echo Not Enough Arguement!
    exit;
fi

#2. éå†é›†ç¾¤æ‰€æœ‰æœºå™¨
for host in hadoop102 hadoop103 hadoop104
do
    echo ====================  $host  ====================
    #3. éå†æ‰€æœ‰ç›®å½•ï¼ŒæŒ¨ä¸ªå‘é€

    for file in $@
    do
        #4. åˆ¤æ–­æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if [ -e $file ]
            then
                #5. è·å–çˆ¶ç›®å½•
                pdir=$(cd -P $(dirname $file); pwd)

                #6. è·å–å½“å‰æ–‡ä»¶çš„åç§°
                fname=$(basename $file)
                ssh $host "mkdir -p $pdir"
                rsync -av $pdir/$fname $host:$pdir
            else
                echo $file does not exists!
        fi
    done
done

```

##### xcall

```sh
#!/bin/bash

# è·å–æ§åˆ¶å°æŒ‡ä»¤

cmd=$*

# åˆ¤æ–­æŒ‡ä»¤æ˜¯å¦ä¸ºç©º
if [ ! -n "$cmd" ]
then
        echo "command can not be null !"
        exit
fi

# è·å–å½“å‰ç™»å½•ç”¨æˆ·
user=`whoami`

# åœ¨ä»æœºæ‰§è¡ŒæŒ‡ä»¤,è¿™é‡Œéœ€è¦æ ¹æ®ä½ å…·ä½“çš„é›†ç¾¤æƒ…å†µé…ç½®ï¼Œhostä¸å…·ä½“ä¸»æœºåä¸€è‡´ï¼ŒåŒä¸Š
for (( host=2;host<=4;host++ ))
do
        echo "================current host is hadoop10$host================="
        echo "--> excute command \"$cmd\""
        ssh $user@hadoop10$host $cmd
done

echo "excute successfully !"
```

##### lg

```sh
#!/bin/bash
for i in hadoop102 hadoop103; do
    echo "========== $i =========="
    ssh $i "cd /opt/module/applog/; java -jar gmall2020-mock-log-2021-10-10.jar >/dev/null 2>&1 &"
done 

```

##### jpsall

```sh
#! /bin/bash
 
for i in hadoop102 hadoop103 hadoop104
do
    echo --------- $i ----------
    ssh $i "jps $@ | grep -v Jps"
done

```



### é‡‡é›†æ¨¡å—æ¡†æ¶å®‰è£…

> ä¸Šä¼   è§£å‹  é…ç½®ç¯å¢ƒ  ä¸äºˆèµ˜è¿°

#### Hadoop

------

**é¡¹ç›®ç»éªŒ**

- **HDSFå­˜å‚¨å¤šç›®å½•**[^æœ¬é¡¹ç›®ä¸å¼€]

  - æŸ¥çœ‹ç£ç›˜ä½¿ç”¨æƒ…å†µ

    ```sh
    $df -h
    ```

  - åœ¨`hdfs-site.xml`æ–‡ä»¶ä¸­é…ç½®å¤šç›®å½•

    ```xml
    <property>
        <name>dfs.datanode.data.dir</name>        
     				   <value>file:///dfs/data1,file:///hd2/dfs/data2,file:///hd3/dfs/data3,file:///hd4/dfs/data4</value>
    </property>
    ```

    ==æ³¨æ„ï¼šæ¯å°æœåŠ¡å™¨æŒ‚è½½çš„ç£ç›˜ä¸ä¸€æ ·ï¼Œæ‰€ä»¥æ¯ä¸ªèŠ‚ç‚¹çš„å¤šç›®å½•é…ç½®å¯ä»¥ä¸ä¸€è‡´ã€‚å•ç‹¬é…ç½®å³å¯ã€‚==

  

- **é›†ç¾¤æ•°æ®å‡è¡¡**

  - èŠ‚ç‚¹é—´æ•°æ®å‡è¡¡

    ```sh
    #å¼€å¯
    start-balancer.sh -threshold 10
    #åœæ­¢
    stop-balancer.sh
    ```
    
    [^10]: ä»£è¡¨çš„æ˜¯é›†ç¾¤ä¸­å„ä¸ªèŠ‚ç‚¹çš„ç£ç›˜ç©ºé—´åˆ©ç”¨ç‡ç›¸å·®ä¸è¶…è¿‡10%ï¼Œå¯æ ¹æ®å®é™…æƒ…å†µè¿›è¡Œè°ƒæ•´ã€‚
    
  - ç£ç›˜é—´æ•°æ®å‡è¡¡[^ä¸€å—ç£ç›˜ä¸ç”¨å¼€]
  
    ```sh
    ç”Ÿæˆå‡è¡¡è®¡åˆ’ï¼ˆæˆ‘ä»¬åªæœ‰ä¸€å—ç£ç›˜ï¼Œä¸ä¼šç”Ÿæˆè®¡åˆ’ï¼‰
    hdfs diskbalancer -plan hadoop103
    æ‰§è¡Œå‡è¡¡è®¡åˆ’
    hdfs diskbalancer -execute hadoop103.plan.json
    æŸ¥çœ‹å½“å‰å‡è¡¡ä»»åŠ¡çš„æ‰§è¡Œæƒ…å†µ
    hdfs diskbalancer -query hadoop103
    å–æ¶ˆå‡è¡¡ä»»åŠ¡
    hdfs diskbalancer -cancel hadoop103.plan.json
    ```



- **å‚æ•°è°ƒä¼˜**

  - HDFSå‚æ•°è°ƒä¼˜hdfs-site.xml

    ```xml
    NameNodeæœ‰ä¸€ä¸ªå·¥ä½œçº¿ç¨‹æ± ï¼Œç”¨æ¥å¤„ç†ä¸åŒDataNodeçš„å¹¶å‘å¿ƒè·³ä»¥åŠå®¢æˆ·ç«¯å¹¶å‘çš„å…ƒæ•°æ®æ“ä½œã€‚
    å¯¹äºå¤§é›†ç¾¤æˆ–è€…æœ‰å¤§é‡å®¢æˆ·ç«¯çš„é›†ç¾¤æ¥è¯´ï¼Œé€šå¸¸éœ€è¦å¢å¤§å‚æ•°dfs.namenode.handler.countçš„é»˜è®¤å€¼10ã€‚
    <property>
        <name>dfs.namenode.handler.count</name>
        <value>10</value>
    </property>
    ```

    ä½¿ç”¨Pythonè®¡ç®—è¯¥å€¼

    â€‹																			**å…¬å¼å¦‚ä¸‹ğŸ‘‡**
    $$
    dfs.namenode.handler.count=20\times\log_e(é›†ç¾¤æœºå™¨æ€»æ•°)
    $$

    ```sh
    $ python
    >>> import math
    >>> print int(20*math.log(8))
    41
    >>> quit()
    ```

  - YARNå‚æ•°è°ƒä¼˜yarn-site.xml

    - **èƒŒæ™¯**ï¼š7å°æœºå™¨ï¼Œæ¯å¤©å‡ äº¿æ¡æ•°æ®ï¼Œæ•°æ®æº->Flume->Kafka->HDFS->Hive

    - **é—®é¢˜**ï¼šæ•°æ®ç»Ÿè®¡ä¸»è¦ç”¨HiveSQLï¼Œæ²¡æœ‰æ•°æ®å€¾æ–œï¼Œå°æ–‡ä»¶å·²ç»åšäº†åˆå¹¶å¤„ç†ï¼Œå¼€å¯çš„JVMé‡ç”¨ï¼Œè€Œä¸”IOæ²¡æœ‰é˜»å¡ï¼Œ==å†…å­˜ç”¨äº†ä¸åˆ°50%==ã€‚ä½†æ˜¯è¿˜æ˜¯è·‘çš„éå¸¸æ…¢ï¼Œè€Œä¸”æ•°æ®é‡æ´ªå³°è¿‡æ¥æ—¶ï¼Œæ•´ä¸ªé›†ç¾¤éƒ½ä¼šå®•æ‰ã€‚åŸºäºè¿™ç§æƒ…å†µæœ‰æ²¡æœ‰ä¼˜åŒ–æ–¹æ¡ˆã€‚

    - **è§£å†³**

      [^yarn.nodemanager.resource.memory-mb]:è¡¨ç¤ºè¯¥èŠ‚ç‚¹ä¸ŠYARNå¯ä½¿ç”¨çš„ç‰©ç†å†…å­˜æ€»é‡ï¼Œé»˜è®¤æ˜¯8192ï¼ˆMBï¼‰ï¼Œæ³¨æ„ï¼Œå¦‚æœä½ çš„èŠ‚ç‚¹å†…å­˜èµ„æºä¸å¤Ÿ8GBï¼Œåˆ™éœ€è¦è°ƒå‡å°è¿™ä¸ªå€¼ï¼Œè€ŒYARNä¸ä¼šæ™ºèƒ½çš„æ¢æµ‹èŠ‚ç‚¹çš„ç‰©ç†å†…å­˜æ€»é‡ã€‚
      [^yarn.scheduler.maximum-allocation-mb]:å•ä¸ªä»»åŠ¡å¯ç”³è¯·çš„æœ€å¤šç‰©ç†å†…å­˜é‡ï¼Œé»˜è®¤æ˜¯8192ï¼ˆMBï¼‰





#### ZK

------

**å¯åœè„šæœ¬**

- ```sh
  #!/bin/bash
  
  case $1 in
  "start"){
  	for i in hadoop102 hadoop103 hadoop104
  	do
          echo ---------- zookeeper $i å¯åŠ¨ ------------
  		ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh start"
  	done
  };;
  "stop"){
  	for i in hadoop102 hadoop103 hadoop104
  	do
          echo ---------- zookeeper $i åœæ­¢ ------------    
  		ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh stop"
  	done
  };;
  "status"){
  	for i in hadoop102 hadoop103 hadoop104
  	do
          echo ---------- zookeeper $i çŠ¶æ€ ------------    
  		ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh status"
  	done
  };;
  esac
  
  ```

  


#### Kafka

------

> Kafka çš„è¿è¡Œä¾èµ–äº zookeeperï¼Œéœ€è¦é¢„å…ˆå¯åŠ¨

**å¯åœè„šæœ¬**

- ```sh
  s#! /bin/bash
  
  case $1 in
  "start"){
      for i in hadoop102 hadoop103 hadoop104
      do
          echo " --------å¯åŠ¨ $i Kafka-------"
          ssh $i "/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties"
      done
  };;
  "stop"){
      for i in hadoop102 hadoop103 hadoop104
      do
          echo " --------åœæ­¢ $i Kafka-------"
          ssh $i "/opt/module/kafka/bin/kafka-server-stop.sh stop"
      done
  };;
  esac
  
  ```



**å¸¸ç”¨å‘½ä»¤**

- æŸ¥çœ‹Topic

  ```sh
  bin/kafka-topics.sh  --bootstrap-server hadoop102:9092 \
   					  --list
  ```

- åˆ›å»ºTopic

  ```sh
  bin/kafka-topics.sh --create \
                      --bootstrap-server hadoop102:9092 \
                       --replication-factor 1 --partitions 1 \
                       --topic Hello-Kafka
  ```

- åˆ é™¤Topic

  ```sh
  bin/kafka-topics.sh --bootstrap-server hadoop102:9092 \
  					--delete \
  					--topic topic_log
  ```

- ä¿®æ”¹Topicå±æ€§

  ```sh
  ä¿®æ”¹åˆ†åŒºæ•°ï¼ˆæ³¨æ„ï¼šåˆ†åŒºæ•°åªèƒ½å¢åŠ ï¼Œä¸èƒ½å‡å°‘ï¼‰
  bin/kafka-topics.sh --bootstrap-server hadoop102:9092 \
  					  --alter \
  					  --topic first \
  					  --partitions 3
  					  
  æŸ¥çœ‹è¯¦æƒ…
  bin/kafka-topics.sh --bootstrap-server E-comp102:9092 \
  					  --describe \
                        --topic first
  ```

  

- æ¨¡æ‹Ÿç”Ÿäº§è€…ç”Ÿäº§æ¶ˆæ¯

  ```sh
  bin/kafka-console-producer.sh --bootstrap-server hadoop102:9092 --topic first
  ```

- æ¨¡æ‹Ÿæ¶ˆè´¹è€…æ¶ˆè´¹æ¶ˆæ¯

  ```sh
  #ä¸ä»å¤´æ¶ˆè´¹
  bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first
  #ä»å¤´æ¶ˆè´¹
  bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic first
  ```

  

#### Flume

------

> åˆ é™¤guavaåŒ…  è§£å†³  æ–°æ—§åŒ…å†²çªé—®é¢˜
>
> ```sh
> $ rm /opt/module/flume/lib/guava-11.0.2.jar
> ```
>
> æ³¨æ„ï¼šåˆ é™¤guava-11.0.2.jarçš„æœåŠ¡å™¨èŠ‚ç‚¹ï¼Œä¸€å®šè¦é…ç½®hadoopç¯å¢ƒå˜é‡ã€‚å¦åˆ™ä¼šæŠ¥å¼‚å¸¸ã€‚
>
> ç¯å¢ƒä¸ç”¨é…ç½®  ç›´æ¥åˆ†å‘



**é¡¹ç›®ç»éªŒ**

- å †å†…å­˜è°ƒæ•´[^è™šæ‹Ÿæœºä¸è¦é…]

  ```xml
  ä¿®æ”¹/opt/module/flume/conf/flume-env.shæ–‡ä»¶ï¼Œé…ç½®å¦‚ä¸‹å‚æ•°
  export JAVA_OPTS="-Xms4096m -Xmx4096m -Dcom.sun.management.jmxremote"
  ```

  [^-Xms]: è¡¨ç¤ºJVM Heap(å †å†…å­˜)æœ€å°å°ºå¯¸ï¼Œåˆå§‹åˆ†é…
  [^-Xmx]: è¡¨ç¤ºJVM Heap(å †å†…å­˜)æœ€å¤§å…è®¸çš„å°ºå¯¸ï¼ŒæŒ‰éœ€åˆ†é…





### Flumeé‡‡é›†å’Œæ¶ˆè´¹é…ç½®

------

> éœ€è¦åœ¨hadoop102ï¼Œhadoop103ä¸¤å°èŠ‚ç‚¹é…ç½®æ—¥å¿—é‡‡é›†Flume

é€‰æ‹©  TaildirSource	+	KafkaChannel	å¹¶é…ç½®æ—¥å¿—æ ¡éªŒæ‹¦æˆªå™¨

**Sourceå¯¹æ¯”**

[^TaildirSource]:æ–­ç‚¹ç»­ä¼ ã€å¤šç›®å½•ã€‚Flume1.6ä»¥å‰éœ€è¦è‡ªå·±è‡ªå®šä¹‰Sourceè®°å½•æ¯æ¬¡è¯»å–æ–‡ä»¶ä½ç½®ï¼Œå®ç°æ–­ç‚¹ç»­ä¼ 
[^ExecSource]: å¯ä»¥å®æ—¶æœé›†æ•°æ®ï¼Œä½†æ˜¯åœ¨Flumeä¸è¿è¡Œæˆ–è€…Shellå‘½ä»¤å‡ºé”™çš„æƒ…å†µä¸‹ï¼Œæ•°æ®å°†ä¼šä¸¢å¤±ã€‚
[^SpoolingDirectorySource]: ç›‘æ§ç›®å½•ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 



**ä½¿ç”¨Kafka ChannelåŸå› **

==é‡‡ç”¨Kafka Channelï¼Œçœå»äº†Sinkï¼Œæé«˜äº†æ•ˆç‡==



#### é‡‡é›†é…ç½®

------

- åˆ›å»ºé…ç½®æ–‡ä»¶

  ```sh
  [atguigu@hadoop104 flume]$ mkdir job
  [atguigu@hadoop104 flume]$ vim job/file_to_kafka.conf 
  ```

- å†™å…¥å†…å®¹

  ```sh
  #ä¸ºå„ç»„ä»¶å‘½å
  a1.sources = r1
  a1.channels = c1
  
  #æè¿°source
  a1.sources.r1.type = TAILDIR
  a1.sources.r1.filegroups = f1
  a1.sources.r1.filegroups.f1 = /opt/module/applog/log/app.*
  a1.sources.r1.positionFile = /opt/module/flume/taildir_position.json
  a1.sources.r1.interceptors =  i1
  a1.sources.r1.interceptors.i1.type = com.atguigu.flume.interceptor.ETLInterceptor$Builder
  
  #æè¿°channel
  a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
  a1.channels.c1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092
  a1.channels.c1.kafka.topic = topic_log
  a1.channels.c1.parseAsFlumeEvent = false
  
  #ç»‘å®šsourceå’Œchannelä»¥åŠsinkå’Œchannelçš„å…³ç³»
  a1.sources.r1.channels = c1
  
  ```

- ç¼–å†™æ‹¦æˆªå™¨

  - åˆ›å»ºmaven  åˆ›å»ºåŒ…

  - æ·»åŠ pom

    ```xml
    <dependencies>
        <dependency>
            <groupId>org.apache.flume</groupId>
            <artifactId>flume-ng-core</artifactId>
            <version>1.9.0</version>
            <scope>provided</scope>
        </dependency>
    
        <dependency>
            <groupId>com.alibaba</groupId>
            <artifactId>fastjson</artifactId>
            <version>1.2.62</version>
        </dependency>
    </dependencies>
    
    <build>
        <plugins>
            <plugin>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>2.3.2</version>
                <configuration>
                    <source>1.8</source>
                    <target>1.8</target>
                </configuration>
            </plugin>
            <plugin>
                <artifactId>maven-assembly-plugin</artifactId>
                <configuration>
                    <descriptorRefs>
                        <descriptorRef>jar-with-dependencies</descriptorRef>
                    </descriptorRefs>
                </configuration>
                <executions>
                    <execution>
                        <id>make-assembly</id>
                        <phase>package</phase>
                        <goals>
                            <goal>single</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
    
    ```

    [^Bulidä¸­æ’ä»¶ä½œç”¨]: ä¸é…ç½®æ’ä»¶æ‰“åŒ…æ²¡æœ‰ä¾èµ–
    
  - åœ¨com.Ryan.flume.interceptoråŒ…ä¸‹åˆ›å»ºJSONUtilsç±»  [^è¯¥å·¥å…·ç±»æŠŠåˆ¤æ–­æ˜¯å¦ä¸ºJSON è¿›è¡Œå°è£…]
  
    ```java
    package com.Ryan.flume.interceptor;
    
    import com.alibaba.fastjson.JSONObject;
    
    public class JSONUtils {
        public static boolean isJSON(String str) {
            try {
                JSONObject.parseObject(str);
                return true;
            } catch (Exception e) {
                return false;
            }
        }
    }
    
    ```
  
  - åœ¨com.Ryan.flume.interceptoråŒ…ä¸‹åˆ›å»ºETLInterceptorç±»
  
    ```java
    package com.Ryan.flume.interceptor;
    
    
    import org.apache.flume.Context;
    import org.apache.flume.Event;
    import org.apache.flume.interceptor.Interceptor;
    
    import java.nio.charset.StandardCharsets;
    import java.util.List;
    
    public class ETLInterceptor implements Interceptor {
    
    
        @Override
        public void initialize() {
    
        }
    
        @Override
        public Event intercept(Event event) {
            //TODO 1 è·å–eventçš„body
            byte[] body = event.getBody();
    
            //TODO 2 è®¾ç½®eventçš„bodyç¼–ç 
            String log = new String(body, StandardCharsets.UTF_8);
    
            //TODO 3 åˆ¤æ–­logæ˜¯å¦jsonæ ¼å¼
            boolean flag = JSONUtils.isJSON(log);
            return flag ? event : null;
    
    
        }
    
    
    
    
        @Override
        public List<Event> intercept(List<Event> events) {
    
            //TODO åˆ é™¤ä¸ºnullçš„event ä½¿ç”¨è¿­ä»£å™¨
            // å¯ä»¥ä½¿ç”¨lamdaè¡¨è¾¾å¼ä»£æ›¿
            events.removeIf(event -> intercept(event) == null);
    
    
            return events;
        }
    
        @Override
        public void close() {
    
        }
    
        public static class Builder implements Interceptor.Builder{
    
            @Override
            public Interceptor build() {
                return new ETLInterceptor();
            }
    
    
    
            @Override
            public void configure(Context context) {
    
            }
        }
    }
    
    ```
  
  - æ‰“åŒ…
  
    ![image-20220713163634637](../image/image-20220713163634637.png)
  
  - ä¸Šä¼ è‡³102 çš„     /opt/module/flume/libæ–‡ä»¶å¤¹ä¸‹é¢
  
  - åˆ†å‘  flumeé…ç½®æ–‡ä»¶å’Œåˆšä¸Šä¼ çš„  æ‹¦æˆªå™¨jaråŒ…

##### å¯åœè„šæœ¬

------

- åœ¨102   ç”¨æˆ·binä¸‹åˆ›å»ºf1.sh

  ```sh
  bin]$ vim f1.sh
  ```

- å¡«å…¥å¦‚ä¸‹å†…å®¹

  ```sh
  #!/bin/bash
  
  case $1 in
  "start"){
          for i in hadoop102 hadoop103
          do
                  echo " --------å¯åŠ¨ $i é‡‡é›†flume-------"
                  ssh $i "nohup /opt/module/flume/bin/flume-ng agent -n a1 -c /opt/module/flume/conf/ -f /opt/module/flume/job/file_to_kafka.conf >/dev/null 2>&1 &"
          done
  };; 
  "stop"){
          for i in hadoop102 hadoop103
          do
                  echo " --------åœæ­¢ $i é‡‡é›†flume-------"
                  ssh $i "ps -ef | grep file_to_kafka.conf | grep -v grep |awk  '{print \$2}' | xargs -n1 kill -9 "
          done
  
  };;
  esac
  
  ```

  [^æ³¨æ„]: åˆ«å¿˜äº†èµ‹æƒ

  

#### æ¶ˆè´¹é…ç½®

------

> æ­¤å¤„é€‰æ‹©KafkaSourceã€FileChannelã€HDFSSinkã€‚

- **åˆ›å»ºFlumeé…ç½®æ–‡ä»¶å¹¶å†™å…¥å†…å®¹**

  ```properties
  #åœ¨hadoop102èŠ‚ç‚¹çš„Flumeçš„jobç›®å½•ä¸‹åˆ›å»ºkafka_to_hdfs.conf  å¹¶å†™å…¥ä¸€ä¸‹å†…å®¹
  
  # Name the components on this agent
  a1.sources = r1
  a1.sinks = k1
  a1.channels = c1
  
  # Describe/configure the source
  a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
  a1.sources.r1.kafka.bootstrap.servers = E-com102:9092
  a1.sources.r1.kafka.consumer.group.id = flume2
  a1.sources.r1.kafka.topics = topic_log
  
  a1.sources.r1.batchSize = 5000
  a1.sources.r1.batchDurationMillis = 2000
  
  a1.sources.r1.interceptors = i1
  a1.sources.r1.interceptors.i1.type = com.ryan.flume.interceptor.TimestampInterceptor$Builder
  
  
  # Describe the sink
  a1.sinks.k1.type = hdfs
  a1.sinks.k1.hdfs.path=/origin_data/gmall/log/topic_log/%Y-%m-%d
  a1.sinks.k1.hdfs.filePrefix=log-
  
  a1.sinks.k1.hdfs.rollInterval = 10
  a1.sinks.k1.hdfs.rollSize = 134217728
  a1.sinks.k1.hdfs.rollCount = 0
  
  a1.sinks.k1.hdfs.fileType=CompressedStream
  a1.sinks.k1.hdfs.codeC=gzip
  
  # Use a channel which buffers events in memory
  a1.channels.c1.type = file
  a1.channels.c1.dataDirs = /opt/module/flume/data/behavior1/
  a1.channels.c1.checkpointDir = /opt/module/flume/checkpoint/behavior1
  
  
  # Bind the source and sink to the channel
  a1.sources.r1.channels = c1
  a1.sinks.k1.channel = c1
  ```

  [^FileChannelä¼˜åŒ–]: é…ç½®dataDirs    checkpointDirå’ŒbackupCheckpointDiræŒ‡  å‘å¤šä¸ªè·¯å¾„ï¼Œæ¯ä¸ªè·¯å¾„å¯¹åº”ä¸åŒçš„ç¡¬ç›˜ï¼Œå¢å¤§Flumeååé‡ä¿è¯é«˜å¯ç”¨
  [^HDFS Sinkä¼˜åŒ–]: é…ç½®hdfs.rollInterval=3600ï¼Œhdfs.rollSize=134217728ï¼Œhdfs.rollCount =0   æ–‡ä»¶åœ¨è¾¾åˆ°128Mæ—¶ä¼šæ»šåŠ¨ç”Ÿæˆæ–°æ–‡ä»¶  æ–‡ä»¶åˆ›å»ºè¶…3600ç§’æ—¶ä¼šæ»šåŠ¨ç”Ÿæˆæ–°æ–‡ä»¶

- **ç¼–å†™Flumeæ‹¦æˆªå™¨**

  [^è§£å†³é›¶ç‚¹æ¼‚ç§»é—®é¢˜]: å°±æ˜¯æ—¶é—´æˆ³ä½¿ç”¨æ•°æ®å†…çš„æ—¶é—´æˆ³    ä½¿ç”¨å½“å‰æ—¶é—´æˆ³

  åœ¨com.ryan.flume.interceptoråŒ…ä¸‹åˆ›å»ºTimestampInterceptorç±»

  ```java
  package com.ryan.flume.interceptor;
  
  import com.alibaba.fastjson.JSONObject;
  import org.apache.flume.Context;
  import org.apache.flume.Event;
  import org.apache.flume.interceptor.Interceptor;
  
  import java.nio.charset.StandardCharsets;
  import java.util.List;
  import java.util.Map;
  
  public class TimestampInterceptor implements Interceptor {
      @Override
      public void initialize() {
  
      }
  
      @Override
      public Event intercept(Event event) {
  
          byte[] body = event.getBody();
  
          String log = new String(body, StandardCharsets.UTF_8);
  
          JSONObject jsonObject = JSONObject.parseObject(log);
          String ts = jsonObject.getString("ts");
  
  
          Map<String, String> headers = event.getHeaders();
          headers.put("timestamp",ts);
          return event;
      }
  
      @Override
      public List<Event> intercept(List<Event> events) {
  
          for (Event event : events) {
              intercept(event);
          }
          return events;
      }
  
      @Override
      public void close() {
  
      }
  
  
      public static class Builder implements Interceptor.Builder{
  
          @Override
          public Interceptor build() {
              return new TimestampInterceptor();
          }
  
          @Override
          public void configure(Context context) {
  
          }
      }
  }
  
  ```

- **é‡æ–°æ‰“åŒ…æ”¾å…¥102 libä¸‹**
- **åˆ†å‘é…ç½®æ–‡ä»¶å’ŒjaråŒ…**





**æµ‹è¯•**

- æŒ‰ç…§é¡ºåºå¯åŠ¨  zk  kafka  hadoop  

- 102  å¯åŠ¨  flume  ğŸ‘‰  kafka

  ```sh
  $f1.sh start
  ```

- 103 å¯åŠ¨kafkaæ¶ˆè´¹è€…

  ```sh
  $kafka-console-consumer --bootstrap-server E-com102:9092 --topic topic_log
  ```

- 104  å¯åŠ¨   kafka  ğŸ‘‰  hdfs

  ```sh
  flume]$ bin/flume-ng agent -n a1 -c conf/ -f job/kafka_to_hdfs.conf -Dflume.root.logger=info,console
  ```

- ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®

  ```sh
  $lg.sh
  ```

- æŸ¥çœ‹HDFS  æ˜¯å¦ç”Ÿæˆæ•°æ®





##### å¯åœè„šæœ¬

```sh
#!/bin/bash

case $1 in
        "start")
        
                echo " --------å¯åŠ¨æ¶ˆè´¹æ—¥å¿—flume-------"


                echo " --------E-com104-------"                
                ssh E-com104 "nohup /opt/module/flume/bin/flume-ng agent -n a1 -c /opt/module/flume/conf/ -f /opt/module/flume/job/kafka_to_hdfs.conf >/dev/null 2>&1 &"
        
        ;; 
        "stop")
        
                echo " --------åœæ­¢æ¶ˆè´¹æ—¥å¿—flume-------"

                echo " --------E-com104-------"
                ssh $i "ps -ef | grep kafka_to_hdfs.conf | grep -v grep |awk  '{print \$2}' | xargs -n1 kill -9 "
        

        ;;
esac


```



### é‡‡é›†é€šé“å¯åœè„šæœ¬

```sh
#!/bin/bash

case $1 in
"start"){
        echo ================== å¯åŠ¨ é›†ç¾¤ ==================

        #å¯åŠ¨ Zookeeperé›†ç¾¤
        zk.sh start

        #å¯åŠ¨ Hadoopé›†ç¾¤
        myhadoop.sh start

        #å¯åŠ¨ Kafkaé‡‡é›†é›†ç¾¤
        kf.sh start

        #å¯åŠ¨ Flumeé‡‡é›†é›†ç¾¤
        f1.sh start

        #å¯åŠ¨ Flumeæ¶ˆè´¹é›†ç¾¤
        f2.sh start

        };;
"stop"){
        echo ================== åœæ­¢ é›†ç¾¤ ==================

        #åœæ­¢ Flumeæ¶ˆè´¹é›†ç¾¤
        f2.sh stop

        #åœæ­¢ Flumeé‡‡é›†é›†ç¾¤
        f1.sh stop

        #åœæ­¢ Kafkaé‡‡é›†é›†ç¾¤
        kf.sh stop

        #åœæ­¢ Hadoopé›†ç¾¤
        myhadoop.sh stop

        #åœæ­¢ Zookeeperé›†ç¾¤
        zk.sh stop

};;
esac

```



## ä¸šåŠ¡æ•°æ®é‡‡é›†æ¨¡å—

> ä¸»è¦å¯¹Mysql æ•°æ®åº“ä¸­çš„ä¸šåŠ¡è¡¨æ“ä½œ



[^SKU=Stock Keeping Unit]:ï¼ˆåº“å­˜é‡åŸºæœ¬å•ä½ï¼‰
[^SPUï¼ˆStandard Product Unitï¼‰]:æ˜¯å•†å“ä¿¡æ¯èšåˆçš„æœ€å°å•ä½ï¼Œæ˜¯ä¸€ç»„å¯å¤ç”¨ã€æ˜“æ£€ç´¢çš„æ ‡å‡†åŒ–ä¿¡æ¯é›†åˆ
[^ä¾‹å­ï¼š]: iPhoneXæ‰‹æœºå°±æ˜¯SPUã€‚ä¸€å°é“¶è‰²ã€128Gå†…å­˜çš„ã€æ”¯æŒè”é€šç½‘ç»œçš„iPhoneXï¼Œå°±æ˜¯SKU

==SPUè¡¨ç¤ºä¸€ç±»å•†å“  åŒæ„SPUå•†å“å…¬ç”¨å•†å“å›¾ç‰‡ã€æµ·æŠ¥ç­‰==



### ç¯å¢ƒå‡†å¤‡

------

#### æ¨¡æ‹Ÿæ•°æ®

------

##### Mysqlå®‰è£…

- **å¸è½½è‡ªå¸¦mysql-libs**

  ```sh
   rpm -qa | grep -i -E mysql\|mariadb | xargs -n1 sudo rpm -e --nodeps
  ```

- **åœ¨/opt/softwareä¸‹åˆ›å»ºmysqlæ–‡ä»¶å¤¹å¹¶ä¸Šä¼ é©±åŠ¨å’Œ5ä¸ªå®‰è£…åŒ…**

- **ä½¿ç”¨rpmå‘½ä»¤æŒ‰ç…§é¡ºåºå®‰è£…**

  ```sh
  $ sudo rpm -ivh 01_mysql-community-common-5.7.16-1.el7.x86_64.rpm
  ```

- **å¯åŠ¨mysql**

  ```sh
  $ sudo systemctl start mysqld
  ```

- **æŸ¥çœ‹mysqlå¯†ç **

  ```sh
  $ sudo cat /var/log/mysqld.log | grep password
  ```

  

##### Mysqlé…ç½®

- **ä½¿ç”¨åˆšæ‰æŸ¥åˆ°çš„å¯†ç ç™»é™†**

  ```sh
  $ mysql -uroot -p'password'
  ```

- **è®¾ç½®å¤æ‚å¯†ç (ç”±äºmysqlå¯†ç ç­–ç•¥ï¼Œæ­¤å¯†ç å¿…é¡»è¶³å¤Ÿå¤æ‚)**

  ```mysql
  mysql> set password=password("Qs23=zs32");
  ```

- **æ›´æ”¹å¯†ç ç­–ç•¥**

  ```mysql
  					ğŸ‘‡å¯†ç æœ€å°é•¿åº¦
  mysql> set global validate_password_length=4;
  mysql> set global validate_password_policy=0;
  					 ğŸ‘†å…è®¸çš„ä¸åŒå­—ç¬¦ä¸ªæ•°  0è¡¨ç¤ºä¸é™åˆ¶
  ```

- **è®¾ç½®ç®€å•å¥½è®°çš„å¯†ç **

  ```sh
  mysql> set password=password("123456");
  ```

- **é…ç½®ç¬¬ä¸‰æ–¹è®¿é—®**

  ```mysql
  mysql> use mysql
  mysql> select user, host from user;
  mysql> update user set host="%" where user="root";
  mysql> flush privileges;
  ```

- **é€€å‡º**

  ```mysql
  mysql> quit;
  ```

  





##### ä¸šåŠ¡æ•°æ®ç”Ÿæˆ

- **é€šè¿‡navicatè¿æ¥è‡³mysqlæ•°æ®åº“**[^æ³¨æ„ä¸è¦è¿é”™äº†]

- **åˆ›å»ºgmallæ•°æ®åº“è®¾ç½®ç¼–ç ä¸ºutf-8ï¼Œæ’åºè§„åˆ™ä¸ºutf8_general_ci**

- **å¯¼å…¥gmall.sql**

- **åœ¨/opt/moduleåˆ›å»ºdb_logæ–‡ä»¶å¤¹**

  ```sh
  module]$ mkdir db_log/
  ```

- **ä¸Šä¼ gmall2020-mock-db-2021-11-14.jarå’Œapplication.propertiesè‡³è¯¥æ–‡ä»¶å¤¹**

- **ä¿®æ”¹application.propertiesç›¸å…³é…ç½®**ã€

  ```properties
  logging.level.root=info
  
  spring.datasource.driver-class-name=com.mysql.jdbc.Driver
  #										ğŸ‘‡æ³¨æ„ä¿®æ”¹
  spring.datasource.url=jdbc:mysql://E-com102:3306/gmall?useUnicode=true&characterEncoding=utf-8&useSSL=false&serverTimezone=GMT%2B8
  spring.datasource.username=root
  spring.datasource.password=123456
  
  logging.pattern.console=%m%n
  
  
  mybatis-plus.global-config.db-config.field-strategy=not_null
  mybatis.mapperLocations=classpath:mapper/*.xml
  
  #ä¸šåŠ¡æ—¥æœŸ
  mock.date=2020-06-14
  #ğŸš©æ˜¯å¦é‡ç½®ï¼Œé¦–æ—¥é¡»è®¾ç½®ä¸º1
  mock.clear=1
  #ğŸš©æ˜¯å¦é‡ç½®ç”¨æˆ·ï¼Œé¦–æ—¥é¡»è®¾ç½®ä¸º1
  mock.clear.user=1
  
  #ç”Ÿæˆæ–°ç”¨æˆ·æ•°é‡
  mock.user.count=200
  #ç”·æ€§æ¯”ä¾‹
  mock.user.male-rate=20
  #ç”¨æˆ·æ•°æ®å˜åŒ–æ¦‚ç‡
  mock.user.update-rate:20
  
  #æ”¶è—å–æ¶ˆæ¯”ä¾‹
  mock.favor.cancel-rate=10
  #æ”¶è—æ•°é‡
  mock.favor.count=100
  
  #æ¯ä¸ªç”¨æˆ·æ·»åŠ è´­ç‰©è½¦çš„æ¦‚ç‡
  mock.cart.user-rate=10
  #æ¯æ¬¡æ¯ä¸ªç”¨æˆ·æœ€å¤šæ·»åŠ å¤šå°‘ç§å•†å“è¿›è´­ç‰©è½¦
  mock.cart.max-sku-count=8 
  #æ¯ä¸ªå•†å“æœ€å¤šä¹°å‡ ä¸ª
  mock.cart.max-sku-num=3 
  
  #è´­ç‰©è½¦æ¥æº  ç”¨æˆ·æŸ¥è¯¢ï¼Œå•†å“æ¨å¹¿ï¼Œæ™ºèƒ½æ¨è, ä¿ƒé”€æ´»åŠ¨
  mock.cart.source-type-rate=60:20:10:10
  
  #ç”¨æˆ·ä¸‹å•æ¯”ä¾‹
  mock.order.user-rate=30
  #ç”¨æˆ·ä»è´­ç‰©ä¸­è´­ä¹°å•†å“æ¯”ä¾‹
  mock.order.sku-rate=50
  #æ˜¯å¦å‚åŠ æ´»åŠ¨
  mock.order.join-activity=1
  #æ˜¯å¦ä½¿ç”¨è´­ç‰©åˆ¸
  mock.order.use-coupon=1
  #è´­ç‰©åˆ¸é¢†å–äººæ•°
  mock.coupon.user-count=100
  
  #æ”¯ä»˜æ¯”ä¾‹
  mock.payment.rate=70
  #æ”¯ä»˜æ–¹å¼ æ”¯ä»˜å®ï¼šå¾®ä¿¡ ï¼šé“¶è”
  mock.payment.payment-type=30:60:10
  
  #è¯„ä»·æ¯”ä¾‹ å¥½ï¼šä¸­ï¼šå·®ï¼šè‡ªåŠ¨
  mock.comment.appraise-rate=30:10:10:50
  
  #é€€æ¬¾åŸå› æ¯”ä¾‹ï¼šè´¨é‡é—®é¢˜ å•†å“æè¿°ä¸å®é™…æè¿°ä¸ä¸€è‡´ ç¼ºè´§ å·ç ä¸åˆé€‚ æ‹é”™ ä¸æƒ³ä¹°äº† å…¶ä»–
  mock.refund.reason-rate=30:10:20:5:15:5:5
  
  logging.level.com.atguigu.gmall2020.mock.db.mapper=debug
  ```

- **åœ¨è¯¥ç›®å½•ä¸‹æ‰§è¡Œ**

  ```sh
  $ java -jar gmall2020-mock-db-2021-11-14.jar
  ```

- **æŸ¥çœ‹gmallæ•°æ®åº“**



### åº”ç”¨å®‰è£…éƒ¨ç½²

------

#### Hive

> **apache-hive-3.1.2-bin.tar.gz**

##### å®‰è£…

------



- ä¸Šä¼ 	è§£å‹	æ”¹å

- é…ç½®ç¯å¢ƒå˜é‡  å¹¶ source

- è§£å†³JARåŒ…å†²çª

  ```sh
  #è¿›å…¥/opt/module/hive/lib
  $ mv log4j-slf4j-impl-2.10.0.jar log4j-slf4j-impl-2.10.0.jar.bak
  ```

  

##### é…ç½®

------

**Hiveå…ƒæ•°æ®é…ç½®åˆ°MySQL**

- æ‹·è´é©±åŠ¨

  ```sh
  $ cp /opt/software/mysql-connector-java-5.1.27.jar /opt/module/hive/lib/
  ```

- é…ç½®MySQLä½œä¸ºå…ƒæ•°æ®å­˜å‚¨

  - åœ¨hiveçš„libä¸‹æ–°å»º	`hive-site.xml`  å†™å…¥ä»¥ä¸‹å†…å®¹

  - ```xml
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>javax.jdo.option.ConnectionURL</name>
            <value>jdbc:mysql://hadoop102:3306/metastore?useSSL=false</value>
        </property>
    
        <property>
            <name>javax.jdo.option.ConnectionDriverName</name>
            <value>com.mysql.jdbc.Driver</value>
        </property>
    
        <property>
            <name>javax.jdo.option.ConnectionUserName</name>
            <value>root</value>
        </property>
    
        <property>
            <name>javax.jdo.option.ConnectionPassword</name>
            <value>123456</value>
        </property>
    
        <property>
            <name>hive.metastore.warehouse.dir</name>
            <value>/user/hive/warehouse</value>
        </property>
    
        <property>
            <name>hive.metastore.schema.verification</name>
            <value>false</value>
        </property>
    
        <property>
        <name>hive.server2.thrift.port</name>
        <value>10000</value>
        </property>
    
        <property>
            <name>hive.server2.thrift.bind.host</name>
            <value>hadoop102</value>
        </property>
    
        <property>
            <name>hive.metastore.event.db.notification.api.auth</name>
            <value>false</value>
        </property>
        
        <property>
            <name>hive.cli.print.header</name>
            <value>true</value>
        </property>
    
        <property>
            <name>hive.cli.print.current.db</name>
            <value>true</value>
        </property>
    </configuration>
    
    ```

**åˆå§‹åŒ–å…ƒæ•°æ®åº“**

- ç™»é™†Mysql

- æ–°å»ºå…ƒæ•°æ®åº“

  - ```mysql
    mysql> create database metastore;
    mysql> quit;
    ```

- ==åˆå§‹åŒ–Hiveå…ƒæ•°æ®åº“==

  - ```sh
    $ schematool -initSchema -dbType mysql -verbose
    ```

    



##### å¯åŠ¨

------

- å¯åŠ¨Hiveå®¢æˆ·ç«¯

  - ```sh
    $bin/hive
    ```

- æŸ¥çœ‹æ•°æ®åº“

  - ```mysql
    hive (default)> show databases;
    OK
    database_name
    default
    ```







##### ä¿®æ”¹å…ƒæ•°æ®åº“å­—ç¬¦é›†

------

> Hiveå…ƒæ•°æ®åº“çš„å­—ç¬¦é›†é»˜è®¤ä¸ºLatin1ï¼Œç”±äºå…¶ä¸æ”¯æŒä¸­æ–‡å­—ç¬¦ï¼Œæ•…è‹¥å»ºè¡¨è¯­å¥ä¸­åŒ…å«ä¸­æ–‡æ³¨é‡Šï¼Œä¼šå‡ºç°ä¹±ç ç°è±¡
>

**ä¿®æ”¹Hiveå…ƒæ•°æ®åº“ä¸­å­˜å‚¨æ³¨é‡Šçš„å­—æ®µçš„å­—ç¬¦é›†ä¸ºutf-8**

- å­—æ®µæ³¨é‡Š

  - ```mysql
    mysql> alter table COLUMNS_V2 modify column COMMENT varchar(256) character set utf8;
    ```

- è¡¨æ³¨é‡Š

  - ```mysql
    mysql> alter table TABLE_PARAMS modify column PARAM_VALUE mediumtext character set utf8;
    ```

- ==ä¿®æ”¹hive-site.xmlä¸­JDBC URL==

  - ```xml
    <property>
            <name>javax.jdo.option.ConnectionURL</name>
            <value>jdbc:mysql://hadoop102:3306/metastore?useSSL=false&amp;useUnicode=true&amp;characterEncoding=UTF-8</value>
        </property>
    ```

    

### é‡‡é›†æ¨¡å—

------

> æ•°æ®çš„åŒæ­¥ç­–ç•¥æœ‰**å…¨é‡åŒæ­¥**å’Œ**å¢é‡åŒæ­¥**

[^å…¨é‡åŒæ­¥]: æ¯å¤©éƒ½å°†ä¸šåŠ¡æ•°æ®åº“ä¸­çš„å…¨éƒ¨æ•°æ®åŒæ­¥ä¸€ä»½åˆ°æ•°æ®ä»“åº“ï¼Œè¿™æ˜¯ä¿è¯ä¸¤ä¾§æ•°æ®åŒæ­¥çš„æœ€ç®€å•çš„æ–¹å¼
[^å¢é‡åŒæ­¥]: æ¯å¤©åªå°†ä¸šåŠ¡æ•°æ®ä¸­çš„æ–°å¢åŠå˜åŒ–æ•°æ®åŒæ­¥åˆ°æ•°æ®ä»“åº“ã€‚é‡‡ç”¨æ¯æ—¥å¢é‡åŒæ­¥çš„è¡¨ï¼Œé€šå¸¸éœ€è¦åœ¨é¦–æ—¥å…ˆè¿›è¡Œä¸€æ¬¡å…¨é‡åŒæ­¥



**å¯¹æ¯”**

| **åŒæ­¥ç­–ç•¥** |            **ä¼˜ç‚¹**            |                           **ç¼ºç‚¹**                           |
| :----------: | :----------------------------: | :----------------------------------------------------------: |
| **å…¨é‡åŒæ­¥** |            é€»è¾‘ç®€å•            | åœ¨æŸäº›æƒ…å†µä¸‹æ•ˆç‡è¾ƒä½ã€‚ä¾‹å¦‚æŸå¼ è¡¨æ•°æ®é‡è¾ƒå¤§ï¼Œä½†æ˜¯æ¯å¤©æ•°æ®çš„å˜åŒ–æ¯”ä¾‹å¾ˆä½ï¼Œè‹¥å¯¹å…¶é‡‡ç”¨æ¯æ—¥å…¨é‡åŒæ­¥ï¼Œåˆ™ä¼šé‡å¤åŒæ­¥å’Œå­˜å‚¨å¤§é‡ç›¸åŒçš„æ•°æ®ã€‚ |
| **å¢é‡åŒæ­¥** | æ•ˆç‡é«˜ï¼Œæ— éœ€åŒæ­¥å’Œå­˜å‚¨é‡å¤æ•°æ® | é€»è¾‘å¤æ‚ï¼Œéœ€è¦å°†æ¯æ—¥çš„æ–°å¢åŠå˜åŒ–æ•°æ®åŒåŸæ¥çš„æ•°æ®è¿›è¡Œæ•´åˆï¼Œæ‰èƒ½ä½¿ç”¨ |

==æ€»ç»“ï¼šå¤§è¡¨å…¨é‡ï¼Œå°è¡¨å¢é‡==



**æ•°æ®åŒæ­¥å·¥å…·**

------



- DataXã€Sqoopä¸ºä»£è¡¨çš„åŸºäºSelectæŸ¥è¯¢çš„ç¦»çº¿ã€æ‰¹é‡åŒæ­¥å·¥å…·
- Maxwellã€Canalä¸ºä»£è¡¨çš„åŸºäºæ•°æ®åº“æ•°æ®å˜æ›´æ—¥å¿—çš„å®æ—¶æµå¼åŒæ­¥å·¥å…·

==å¯¹æ¯”==

| **å¢é‡åŒæ­¥æ–¹æ¡ˆ**   | **DataX/Sqoop**                                              | **Maxwell/Canal**                                            |
| ------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **å¯¹æ•°æ®åº“çš„è¦æ±‚** | åŸç†æ˜¯åŸºäºæŸ¥è¯¢ï¼Œæ•…è‹¥æƒ³é€šè¿‡selectæŸ¥è¯¢è·å–æ–°å¢åŠå˜åŒ–æ•°æ®ï¼Œå°±è¦æ±‚æ•°æ®è¡¨ä¸­å­˜åœ¨create_timeã€update_timeç­‰å­—æ®µï¼Œç„¶åæ ¹æ®è¿™äº›å­—æ®µè·å–å˜æ›´æ•°æ®ã€‚ | è¦æ±‚æ•°æ®åº“è®°å½•å˜æ›´æ“ä½œï¼Œä¾‹å¦‚MySQLéœ€å¼€å¯binlogã€‚              |
| **æ•°æ®çš„ä¸­é—´çŠ¶æ€** | ç”±äºæ˜¯ç¦»çº¿æ‰¹é‡åŒæ­¥ï¼Œæ•…è‹¥ä¸€æ¡æ•°æ®åœ¨ä¸€å¤©ä¸­å˜åŒ–å¤šæ¬¡ï¼Œè¯¥æ–¹æ¡ˆåªèƒ½è·å–æœ€åä¸€ä¸ªçŠ¶æ€ï¼Œä¸­é—´çŠ¶æ€æ— æ³•è·å–ã€‚ | ç”±äºæ˜¯å®æ—¶è·å–æ‰€æœ‰çš„æ•°æ®å˜æ›´æ“ä½œï¼Œæ‰€ä»¥å¯ä»¥è·å–å˜æ›´æ•°æ®çš„æ‰€æœ‰ä¸­é—´çŠ¶æ€ã€‚ |

#### é…ç½®ç¬”è®°

##### DataX

[DataX](..\..\DataX\1ã€DataX\DataX.md)

##### Maxwell

[Maxwell](..\..\Maxwell\1ã€Maxwell\Maxwell.md)



#### å…¨é‡åŒæ­¥

> ä½¿ç”¨DataX  å°† Mysqlæ•°æ®å¯¼å…¥åˆ°HDFS

â€‹																						**æµç¨‹å›¾ğŸ‘‡**

![image-20220726202600969](../image/image-20220726202600969.png)



##### é…ç½®æ–‡ä»¶

------

â€‹																						**ä»¥activity_infoä¸ºä¾‹**

```json
{
    "job": {
        "content": [
            {
                "reader": {
                    "name": "mysqlreader",
                    "parameter": {
                        "column": [
                            "id",
                            "activity_name",
                            "activity_type",
                            "activity_desc",
                            "start_time",
                            "end_time",
                            "create_time"
                        ],
                        "connection": [
                            {
                                "jdbcUrl": [
                                    "jdbc:mysql://hadoop102:3306/gmall"
                                ],
                                "table": [
                                    "activity_info"
                                ]
                            }
                        ],
                        "password": "123456",
                        "splitPk": "",
                        "username": "root"
                    }
                },
                "writer": {
                    "name": "hdfswriter",
                    "parameter": {
                        "column": [
                            {
                                "name": "id",
                                "type": "bigint"
                            },
                            {
                                "name": "activity_name",
                                "type": "string"
                            },
                            {
                                "name": "activity_type",
                                "type": "string"
                            },
                            {
                                "name": "activity_desc",
                                "type": "string"
                            },
                            {
                                "name": "start_time",
                                "type": "string"
                            },
                            {
                                "name": "end_time",
                                "type": "string"
                            },
                            {
                                "name": "create_time",
                                "type": "string"
                            }
                        ],
                        "compress": "gzip",
                        "defaultFS": "hdfs://hadoop102:8020",
                        "fieldDelimiter": "\t",
                        "fileName": "activity_info",
                        "fileType": "text",
                        "path": "${targetdir}",
                        "writeMode": "append"
                    }
                }
            }
        ],
        "setting": {
            "speed": {
                "channel": 1
            }
        }
    }
}

```



##### é…ç½®æ–‡ä»¶ç”Ÿæˆè„šæœ¬

------

> **åˆ†ä¸ºå•ä¸ªå’Œæ‰¹é‡**

###### ==å•ä¸ª==

- **åœ¨~/binç›®å½•ä¸‹åˆ›å»ºgen_import_config.pyè„šæœ¬**

  ```sh
  $ vim ~/bin/gen_import_config.py 
  ```

- **è„šæœ¬å†…å®¹å¦‚ä¸‹**

  ```python
  # coding=utf-8
  import json
  import getopt
  import os
  import sys
  import MySQLdb
  
  #MySQLç›¸å…³é…ç½®ï¼Œéœ€æ ¹æ®å®é™…æƒ…å†µä½œå‡ºä¿®æ”¹
  mysql_host = "hadoop102"
  mysql_port = "3306"
  mysql_user = "root"
  mysql_passwd = "123456"
  
  #HDFS NameNodeç›¸å…³é…ç½®ï¼Œéœ€æ ¹æ®å®é™…æƒ…å†µä½œå‡ºä¿®æ”¹
  hdfs_nn_host = "hadoop102"
  hdfs_nn_port = "8020"
  
  #ç”Ÿæˆé…ç½®æ–‡ä»¶çš„ç›®æ ‡è·¯å¾„ï¼Œå¯æ ¹æ®å®é™…æƒ…å†µä½œå‡ºä¿®æ”¹
  output_path = "/opt/module/datax/job/import"
  
  #è·å–mysqlè¿æ¥
  def get_connection():
      return MySQLdb.connect(host=mysql_host, port=int(mysql_port), user=mysql_user, passwd=mysql_passwd)
  
  #è·å–è¡¨æ ¼çš„å…ƒæ•°æ®  åŒ…å«åˆ—åå’Œæ•°æ®ç±»å‹
  def get_mysql_meta(database, table):
      connection = get_connection()
      cursor = connection.cursor()
      sql = "SELECT COLUMN_NAME,DATA_TYPE from information_schema.COLUMNS WHERE TABLE_SCHEMA=%s AND TABLE_NAME=%s ORDER BY ORDINAL_POSITION"
      cursor.execute(sql, [database, table])
      fetchall = cursor.fetchall()
      cursor.close()
      connection.close()
      return fetchall
  
  #è·å–mysqlè¡¨çš„åˆ—å
  def get_mysql_columns(database, table):
      return map(lambda x: x[0], get_mysql_meta(database, table))
  
  #å°†è·å–çš„å…ƒæ•°æ®ä¸­mysqlçš„æ•°æ®ç±»å‹è½¬æ¢ä¸ºhiveçš„æ•°æ®ç±»å‹  å†™å…¥åˆ°hdfswriterä¸­
  def get_hive_columns(database, table):
      def type_mapping(mysql_type):
          mappings = {
              "bigint": "bigint",
              "int": "bigint",
              "smallint": "bigint",
              "tinyint": "bigint",
              "decimal": "string",
              "double": "double",
              "float": "float",
              "binary": "string",
              "char": "string",
              "varchar": "string",
              "datetime": "string",
              "time": "string",
              "timestamp": "string",
              "date": "string",
              "text": "string"
          }
          return mappings[mysql_type]
  
      meta = get_mysql_meta(database, table)
      return map(lambda x: {"name": x[0], "type": type_mapping(x[1].lower())}, meta)
  
  #ç”Ÿæˆjsonæ–‡ä»¶
  def generate_json(source_database, source_table):
      job = {
          "job": {
              "setting": {
                  "speed": {
                      "channel": 3
                  },
                  "errorLimit": {
                      "record": 0,
                      "percentage": 0.02
                  }
              },
              "content": [{
                  "reader": {
                      "name": "mysqlreader",
                      "parameter": {
                          "username": mysql_user,
                          "password": mysql_passwd,
                          "column": get_mysql_columns(source_database, source_table),
                          "splitPk": "",
                          "connection": [{
                              "table": [source_table],
                              "jdbcUrl": ["jdbc:mysql://" + mysql_host + ":" + mysql_port + "/" + source_database]
                          }]
                      }
                  },
                  "writer": {
                      "name": "hdfswriter",
                      "parameter": {
                          "defaultFS": "hdfs://" + hdfs_nn_host + ":" + hdfs_nn_port,
                          "fileType": "text",
                          "path": "${targetdir}",
                          "fileName": source_table,
                          "column": get_hive_columns(source_database, source_table),
                          "writeMode": "append",
                          "fieldDelimiter": "\t",
                          "compress": "gzip"
                      }
                  }
              }]
          }
      }
      if not os.path.exists(output_path):
          os.makedirs(output_path)
      with open(os.path.join(output_path, ".".join([source_database, source_table, "json"])), "w") as f:
          json.dump(job, f)
  
  
  def main(args):
      source_database = ""
      source_table = ""
  
      options, arguments = getopt.getopt(args, '-d:-t:', ['sourcedb=', 'sourcetbl='])
      for opt_name, opt_value in options:
          if opt_name in ('-d', '--sourcedb'):
              source_database = opt_value
          if opt_name in ('-t', '--sourcetbl'):
              source_table = opt_value
  
      generate_json(source_database, source_table)
  
  
  if __name__ == '__main__':
      main(sys.argv[1:])
  
  ```

- **ä½¿ç”¨è¯´æ˜**

  - **å®‰è£…Pythonè®¿é—®Mysqlæ•°æ®åº“é©±åŠ¨**

    ```sh
    $ sudo yum install -y MySQL-python
    ```

  - ```python
    python gen_import_config.py -d database -t table
    ```

    [^å‚æ•°è¯´æ˜]: é€šè¿‡-dä¼ å…¥æ•°æ®åº“åï¼Œ-tä¼ å…¥è¡¨åï¼Œæ‰§è¡Œä¸Šè¿°å‘½ä»¤å³å¯ç”Ÿæˆè¯¥è¡¨çš„DataXåŒæ­¥é…ç½®æ–‡ä»¶
    
    

###### ==æ‰¹é‡==

- **åœ¨~/binç›®å½•ä¸‹åˆ›å»ºgen_import_config.shè„šæœ¬**

  ```sh
  $ vim ~/bin/gen_import_config.sh
  ```

- **è¾“å…¥å¦‚ä¸‹å†…å®¹**

  ```sh
  #!/bin/bash
  
  python ~/bin/gen_import_config.py -d gmall -t activity_info
  python ~/bin/gen_import_config.py -d gmall -t activity_rule
  python ~/bin/gen_import_config.py -d gmall -t base_category1
  python ~/bin/gen_import_config.py -d gmall -t base_category2
  python ~/bin/gen_import_config.py -d gmall -t base_category3
  python ~/bin/gen_import_config.py -d gmall -t base_dic
  python ~/bin/gen_import_config.py -d gmall -t base_province
  python ~/bin/gen_import_config.py -d gmall -t base_region
  python ~/bin/gen_import_config.py -d gmall -t base_trademark
  python ~/bin/gen_import_config.py -d gmall -t cart_info
  python ~/bin/gen_import_config.py -d gmall -t coupon_info
  python ~/bin/gen_import_config.py -d gmall -t sku_attr_value
  python ~/bin/gen_import_config.py -d gmall -t sku_info
  python ~/bin/gen_import_config.py -d gmall -t sku_sale_attr_value
  python ~/bin/gen_import_config.py -d gmall -t spu_info
  
  ```

- **æ‰§è¡Œ**

  ```sh
  $ gen_import_config.sh
  ```

- **åœ¨/opt/module/datax/job/import/ä¸‹æŸ¥çœ‹æ–‡ä»¶**



##### å…¨é‡è¡¨æ•°æ®åŒæ­¥è„šæœ¬

------

>   ä¸ºäº†è§£å†³æ‰§è¡Œé…ç½®æ–‡ä»¶å‰éœ€è¦åœ¨HDFSåˆ›å»ºæ–‡ä»¶ å’Œ  æ‰‹åŠ¨ä¸€ä¸ªä¸€ä¸ªæ‰§è¡Œ

- **åœ¨~/binç›®å½•åˆ›å»ºmysql_to_hdfs_full.sh**

  ```sh
  $ vim ~/bin/mysql_to_hdfs_full.sh
  ```

- **å†…å®¹å¦‚ä¸‹**

  ```sh
  #!/bin/bash
  
  DATAX_HOME=/opt/module/datax
  
  # å¦‚æœä¼ å…¥æ—¥æœŸåˆ™do_dateç­‰äºä¼ å…¥çš„æ—¥æœŸï¼Œå¦åˆ™ç­‰äºå‰ä¸€å¤©æ—¥æœŸ
  if [ -n "$2" ] ;then
      do_date=$2
  else
      do_date=`date -d "-1 day" +%F`
  fi
  
  #å¤„ç†ç›®æ ‡è·¯å¾„ï¼Œæ­¤å¤„çš„å¤„ç†é€»è¾‘æ˜¯ï¼Œå¦‚æœç›®æ ‡è·¯å¾„ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºï¼›è‹¥å­˜åœ¨ï¼Œåˆ™æ¸…ç©ºï¼Œç›®çš„æ˜¯ä¿è¯åŒæ­¥ä»»åŠ¡å¯é‡å¤æ‰§è¡Œ
  handle_targetdir() {
    hadoop fs -test -e $1
    if [[ $? -eq 1 ]]; then
      echo "è·¯å¾„$1ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»º......"
      hadoop fs -mkdir -p $1
    else
      echo "è·¯å¾„$1å·²ç»å­˜åœ¨"
      fs_count=$(hadoop fs -count $1)
      content_size=$(echo $fs_count | awk '{print $3}')
      if [[ $content_size -eq 0 ]]; then
        echo "è·¯å¾„$1ä¸ºç©º"
      else
        echo "è·¯å¾„$1ä¸ä¸ºç©ºï¼Œæ­£åœ¨æ¸…ç©º......"
        hadoop fs -rm -r -f $1/*
      fi
    fi
  }
  
  #æ•°æ®åŒæ­¥
  import_data() {
    datax_config=$1
    target_dir=$2
  
    handle_targetdir $target_dir
    python $DATAX_HOME/bin/datax.py -p"-Dtargetdir=$target_dir" $datax_config
  }
  
  case $1 in
  "activity_info")
    import_data /opt/module/datax/job/import/gmall.activity_info.json /origin_data/gmall/db/activity_info_full/$do_date
    ;;
  "activity_rule")
    import_data /opt/module/datax/job/import/gmall.activity_rule.json /origin_data/gmall/db/activity_rule_full/$do_date
    ;;
  "base_category1")
    import_data /opt/module/datax/job/import/gmall.base_category1.json /origin_data/gmall/db/base_category1_full/$do_date
    ;;
  "base_category2")
    import_data /opt/module/datax/job/import/gmall.base_category2.json /origin_data/gmall/db/base_category2_full/$do_date
    ;;
  "base_category3")
    import_data /opt/module/datax/job/import/gmall.base_category3.json /origin_data/gmall/db/base_category3_full/$do_date
    ;;
  "base_dic")
    import_data /opt/module/datax/job/import/gmall.base_dic.json /origin_data/gmall/db/base_dic_full/$do_date
    ;;
  "base_province")
    import_data /opt/module/datax/job/import/gmall.base_province.json /origin_data/gmall/db/base_province_full/$do_date
    ;;
  "base_region")
    import_data /opt/module/datax/job/import/gmall.base_region.json /origin_data/gmall/db/base_region_full/$do_date
    ;;
  "base_trademark")
    import_data /opt/module/datax/job/import/gmall.base_trademark.json /origin_data/gmall/db/base_trademark_full/$do_date
    ;;
  "cart_info")
    import_data /opt/module/datax/job/import/gmall.cart_info.json /origin_data/gmall/db/cart_info_full/$do_date
    ;;
  "coupon_info")
    import_data /opt/module/datax/job/import/gmall.coupon_info.json /origin_data/gmall/db/coupon_info_full/$do_date
    ;;
  "sku_attr_value")
    import_data /opt/module/datax/job/import/gmall.sku_attr_value.json /origin_data/gmall/db/sku_attr_value_full/$do_date
    ;;
  "sku_info")
    import_data /opt/module/datax/job/import/gmall.sku_info.json /origin_data/gmall/db/sku_info_full/$do_date
    ;;
  "sku_sale_attr_value")
    import_data /opt/module/datax/job/import/gmall.sku_sale_attr_value.json /origin_data/gmall/db/sku_sale_attr_value_full/$do_date
    ;;
  "spu_info")
    import_data /opt/module/datax/job/import/gmall.spu_info.json /origin_data/gmall/db/spu_info_full/$do_date
    ;;
  "all")
    import_data /opt/module/datax/job/import/gmall.activity_info.json /origin_data/gmall/db/activity_info_full/$do_date
    import_data /opt/module/datax/job/import/gmall.activity_rule.json /origin_data/gmall/db/activity_rule_full/$do_date
    import_data /opt/module/datax/job/import/gmall.base_category1.json /origin_data/gmall/db/base_category1_full/$do_date
    import_data /opt/module/datax/job/import/gmall.base_category2.json /origin_data/gmall/db/base_category2_full/$do_date
    import_data /opt/module/datax/job/import/gmall.base_category3.json /origin_data/gmall/db/base_category3_full/$do_date
    import_data /opt/module/datax/job/import/gmall.base_dic.json /origin_data/gmall/db/base_dic_full/$do_date
    import_data /opt/module/datax/job/import/gmall.base_province.json /origin_data/gmall/db/base_province_full/$do_date
    import_data /opt/module/datax/job/import/gmall.base_region.json /origin_data/gmall/db/base_region_full/$do_date
    import_data /opt/module/datax/job/import/gmall.base_trademark.json /origin_data/gmall/db/base_trademark_full/$do_date
    import_data /opt/module/datax/job/import/gmall.cart_info.json /origin_data/gmall/db/cart_info_full/$do_date
    import_data /opt/module/datax/job/import/gmall.coupon_info.json /origin_data/gmall/db/coupon_info_full/$do_date
    import_data /opt/module/datax/job/import/gmall.sku_attr_value.json /origin_data/gmall/db/sku_attr_value_full/$do_date
    import_data /opt/module/datax/job/import/gmall.sku_info.json /origin_data/gmall/db/sku_info_full/$do_date
    import_data /opt/module/datax/job/import/gmall.sku_sale_attr_value.json /origin_data/gmall/db/sku_sale_attr_value_full/$do_date
    import_data /opt/module/datax/job/import/gmall.spu_info.json /origin_data/gmall/db/spu_info_full/$do_date
    ;;
  esac
  ```

- **èµ‹æƒ**

- **æ‰§è¡Œ**

  ```sh
  $ mysql_to_hdfs_full.sh all 2020-06-14
  ```

- **åœ¨HDFSä¸ŠæŸ¥çœ‹**





#### å¢é‡åŒæ­¥

------





















