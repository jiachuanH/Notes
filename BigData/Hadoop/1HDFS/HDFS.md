# ä¸€ã€æ¦‚è¿°

### 	HDFSäº§å‡ºèƒŒæ™¯åŠå®šä¹‰

<img src="../image/image-20211207181850778.png" alt="image-20211207181850778" style="zoom: 200%;" />

 

### 	HDFSä¼˜ç¼ºç‚¹

<img src="../image/image-20211207181950941.png" alt="image-20211207181950941" style="zoom:200%;" />

![image-20211207182132925](../image/image-20211207182132925.png)

### 	HDFSç»„æˆæ¶æ„

<img src="../image/image-20211207182202141.png" alt="image-20211207182202141" style="zoom: 200%;" />

<img src="../image/image-20211207182229396.png" alt="image-20211207182229396" style="zoom:200%;" />

### 	ğŸš©HDFSæ–‡ä»¶å—å¤§å°ï¼ˆé¢è¯•é‡ç‚¹ï¼‰

![image-20211207182328830](../image/image-20211207182328830.png)	![image-20211207182341549](../image/image-20211207182341549.png)

# äºŒã€ğŸš©HDFSçš„Shellç›¸å…³æ“ä½œ

### 	åŸºæœ¬è¯­æ³•

hadoop fs å…·ä½“å‘½ä»¤        OR       hdfs dfs å…·ä½“å‘½ä»¤

ä¸¤ä¸ªæ˜¯å®Œå…¨ç›¸åŒçš„ã€‚

**1. æ˜¾ç¤ºå½“å‰ç›®å½•ç»“æ„**

```shell
# æ˜¾ç¤ºå½“å‰ç›®å½•ç»“æ„
hadoop fs -ls  <path>
# é€’å½’æ˜¾ç¤ºå½“å‰ç›®å½•ç»“æ„
hadoop fs -ls  -R  <path>
# æ˜¾ç¤ºæ ¹ç›®å½•ä¸‹å†…å®¹
hadoop fs -ls  /
```

**2. åˆ›å»ºç›®å½•**

```shell
# åˆ›å»ºç›®å½•
hadoop fs -mkdir  <path> 
# é€’å½’åˆ›å»ºç›®å½•
hadoop fs -mkdir -p  <path>  
```

**3. åˆ é™¤æ“ä½œ**

```shell
# åˆ é™¤æ–‡ä»¶
hadoop fs -rm  <path>
# é€’å½’åˆ é™¤ç›®å½•å’Œæ–‡ä»¶
hadoop fs -rm -R  <path> 
```

**4. ä»æœ¬åœ°åŠ è½½æ–‡ä»¶åˆ° HDFS**

```shell
# äºŒé€‰ä¸€æ‰§è¡Œå³å¯
hadoop fs -put  [localsrc] [dst] 
hadoop fs -copyFromLocal [localsrc] [dst] 
```

**5. ä» HDFS å¯¼å‡ºæ–‡ä»¶åˆ°æœ¬åœ°**

```shell
# äºŒé€‰ä¸€æ‰§è¡Œå³å¯
hadoop fs -get  [dst] [localsrc] 
hadoop fs -copyToLocal [dst] [localsrc] 
```

**6. æŸ¥çœ‹æ–‡ä»¶å†…å®¹**

```shell
# äºŒé€‰ä¸€æ‰§è¡Œå³å¯
hadoop fs -text  <path> 
hadoop fs -cat  <path>  
```

**7. æ˜¾ç¤ºæ–‡ä»¶çš„æœ€åä¸€åƒå­—èŠ‚**

```shell
hadoop fs -tail  <path> 
# å’ŒLinuxä¸‹ä¸€æ ·ï¼Œä¼šæŒç»­ç›‘å¬æ–‡ä»¶å†…å®¹å˜åŒ– å¹¶æ˜¾ç¤ºæ–‡ä»¶çš„æœ€åä¸€åƒå­—èŠ‚
hadoop fs -tail -f  <path> 
```

**8. æ‹·è´æ–‡ä»¶**

```shell
hadoop fs -cp [src] [dst]
```

**9. ç§»åŠ¨æ–‡ä»¶**

```shell
hadoop fs -mv [src] [dst] 
```


**10. ç»Ÿè®¡å½“å‰ç›®å½•ä¸‹å„æ–‡ä»¶å¤§å°**  
+ é»˜è®¤å•ä½å­—èŠ‚  
+ -s : æ˜¾ç¤ºæ‰€æœ‰æ–‡ä»¶å¤§å°æ€»å’Œï¼Œ
+ -h : å°†ä»¥æ›´å‹å¥½çš„æ–¹å¼æ˜¾ç¤ºæ–‡ä»¶å¤§å°ï¼ˆä¾‹å¦‚ 64.0m è€Œä¸æ˜¯ 67108864ï¼‰
```shell
hadoop fs -du  <path>  
```

**11. åˆå¹¶ä¸‹è½½å¤šä¸ªæ–‡ä»¶**
+ -nl  åœ¨æ¯ä¸ªæ–‡ä»¶çš„æœ«å°¾æ·»åŠ æ¢è¡Œç¬¦ï¼ˆLFï¼‰
+ -skip-empty-file è·³è¿‡ç©ºæ–‡ä»¶

```shell
hadoop fs -getmerge
# ç¤ºä¾‹ å°†HDFSä¸Šçš„hbase-policy.xmlå’Œhbase-site.xmlæ–‡ä»¶åˆå¹¶åä¸‹è½½åˆ°æœ¬åœ°çš„/usr/test.xml
hadoop fs -getmerge -nl  /test/hbase-policy.xml /test/hbase-site.xml /usr/test.xml
```

**12. ç»Ÿè®¡æ–‡ä»¶ç³»ç»Ÿçš„å¯ç”¨ç©ºé—´ä¿¡æ¯**

```shell
hadoop fs -df -h /
```

**13. æ›´æ”¹æ–‡ä»¶å¤åˆ¶å› å­**
```shell
hadoop fs -setrep [-R] [-w] <numReplicas> <path>
```
+ æ›´æ”¹æ–‡ä»¶çš„å¤åˆ¶å› å­ï¼ˆå‰¯æœ¬ï¼‰ã€‚å¦‚æœ path æ˜¯ç›®å½•ï¼Œåˆ™æ›´æ”¹å…¶ä¸‹æ‰€æœ‰æ–‡ä»¶çš„å¤åˆ¶å› å­
+ -w : è¯·æ±‚å‘½ä»¤æ˜¯å¦ç­‰å¾…å¤åˆ¶å®Œæˆ

```shell
# ç¤ºä¾‹
hadoop fs -setrep -w 3 /user/hadoop/dir1
```

**14. æƒé™æ§åˆ¶**  
```shell
# æƒé™æ§åˆ¶å’ŒLinuxä¸Šä½¿ç”¨æ–¹å¼ä¸€è‡´
# å˜æ›´æ–‡ä»¶æˆ–ç›®å½•çš„æ‰€å±ç¾¤ç»„ã€‚ ç”¨æˆ·å¿…é¡»æ˜¯æ–‡ä»¶çš„æ‰€æœ‰è€…æˆ–è¶…çº§ç”¨æˆ·ã€‚
hadoop fs -chgrp [-R] GROUP URI [URI ...]
# ä¿®æ”¹æ–‡ä»¶æˆ–ç›®å½•çš„è®¿é—®æƒé™  ç”¨æˆ·å¿…é¡»æ˜¯æ–‡ä»¶çš„æ‰€æœ‰è€…æˆ–è¶…çº§ç”¨æˆ·ã€‚
hadoop fs -chmod [-R] <MODE[,MODE]... | OCTALMODE> URI [URI ...]
# ä¿®æ”¹æ–‡ä»¶çš„æ‹¥æœ‰è€…  ç”¨æˆ·å¿…é¡»æ˜¯è¶…çº§ç”¨æˆ·ã€‚
hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ]
```

**15. æ–‡ä»¶æ£€æµ‹**

```shell
hadoop fs -test - [defsz]  URI
```
å¯é€‰é€‰é¡¹ï¼š
+ -dï¼šå¦‚æœè·¯å¾„æ˜¯ç›®å½•ï¼Œè¿”å› 0ã€‚
+ -eï¼šå¦‚æœè·¯å¾„å­˜åœ¨ï¼Œåˆ™è¿”å› 0ã€‚
+ -fï¼šå¦‚æœè·¯å¾„æ˜¯æ–‡ä»¶ï¼Œåˆ™è¿”å› 0ã€‚
+ -sï¼šå¦‚æœè·¯å¾„ä¸ä¸ºç©ºï¼Œåˆ™è¿”å› 0ã€‚
+ -rï¼šå¦‚æœè·¯å¾„å­˜åœ¨ä¸”æˆäºˆè¯»æƒé™ï¼Œåˆ™è¿”å› 0ã€‚
+ -wï¼šå¦‚æœè·¯å¾„å­˜åœ¨ä¸”æˆäºˆå†™å…¥æƒé™ï¼Œåˆ™è¿”å› 0ã€‚
+ -zï¼šå¦‚æœæ–‡ä»¶é•¿åº¦ä¸ºé›¶ï¼Œåˆ™è¿”å› 0ã€‚

```shell
# ç¤ºä¾‹
hadoop fs -test -e filename
```



> **è®¾ç½®çš„å‰¯æœ¬æ•°åªæ˜¯è®°å½•åœ¨NameNodeçš„å…ƒæ•°æ®ä¸­ï¼Œæ˜¯å¦çœŸçš„ä¼šæœ‰è¿™ä¹ˆå¤šå‰¯æœ¬ï¼Œè¿˜å¾—çœ‹DataNodeçš„æ•°é‡ã€‚å› ä¸ºç›®å‰åªæœ‰3å°è®¾å¤‡ï¼Œæœ€å¤šä¹Ÿå°±3ä¸ªå‰¯æœ¬ï¼Œåªæœ‰èŠ‚ç‚¹æ•°çš„å¢åŠ åˆ°10å°æ—¶ï¼Œå‰¯æœ¬æ•°æ‰èƒ½è¾¾åˆ°10ã€‚**



# ä¸‰ã€HDFSçš„å®¢æˆ·ç«¯API



### 	å®¢æˆ·ç«¯ç¯å¢ƒå‡†å¤‡



![image-20211207185029550](../image/image-20211207185029550.png)



![image-20211207185059741](../image/image-20211207185059741.png)

![image-20211207185114356](../image/image-20211207185114356.png)

**åœ¨IDEAä¸­åˆ›å»ºä¸€ä¸ªMavenå·¥ç¨‹HdfsClientDemoï¼Œå¹¶å¯¼å…¥ç›¸åº”çš„ä¾èµ–åæ ‡+æ—¥å¿—æ·»åŠ **

```xml
<dependencies>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-client</artifactId>
        <version>3.1.3</version>
    </dependency>
    <dependency>
        <groupId>junit</groupId>
        <artifactId>junit</artifactId>
        <version>4.12</version>
    </dependency>
    <dependency>
        <groupId>org.slf4j</groupId>
        <artifactId>slf4j-log4j12</artifactId>
        <version>1.7.30</version>
    </dependency>
</dependencies>

```

> åœ¨é¡¹ç›®çš„src/main/resourcesç›®å½•ä¸‹ï¼Œæ–°å»ºä¸€ä¸ªæ–‡ä»¶ï¼Œå‘½åä¸ºâ€œlog4j.propertiesâ€ï¼Œåœ¨æ–‡ä»¶ä¸­å¡«å…¥

```properties
log4j.rootLogger=INFO, stdout  
log4j.appender.stdout=org.apache.log4j.ConsoleAppender  
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout  
log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n  
log4j.appender.logfile=org.apache.log4j.FileAppender  
log4j.appender.logfile.File=target/spring.log  
log4j.appender.logfile.layout=org.apache.log4j.PatternLayout  
log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n
```

**åˆ›å»ºåŒ…åï¼šcom.atguigu.hdfs**

**åˆ›å»ºHdfsClientç±»**

```java
public class HdfsClient {

    @Test
    public void testMkdirs() throws IOException, URISyntaxException, InterruptedException {

        // 1 è·å–æ–‡ä»¶ç³»ç»Ÿ
        Configuration configuration = new Configuration();

        // FileSystem fs = FileSystem.get(new URI("hdfs://hadoop102:8020"), configuration);
        FileSystem fs = FileSystem.get(new URI("hdfs://hadoop102:8020"), configuration,"atguigu");

        // 2 åˆ›å»ºç›®å½•
        fs.mkdirs(new Path("/xiyou/huaguoshan/"));

        // 3 å…³é—­èµ„æº
        fs.close();
    }
}

```

**æ‰§è¡Œç¨‹åº**

`å®¢æˆ·ç«¯å»æ“ä½œHDFSæ—¶ï¼Œæ˜¯æœ‰ä¸€ä¸ªç”¨æˆ·èº«ä»½çš„ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼ŒHDFSå®¢æˆ·ç«¯APIä¼šä»é‡‡ç”¨Windowsé»˜è®¤ç”¨æˆ·è®¿é—®HDFSï¼Œä¼šæŠ¥æƒé™å¼‚å¸¸é”™è¯¯ã€‚æ‰€ä»¥åœ¨è®¿é—®HDFSæ—¶ï¼Œä¸€å®šè¦é…ç½®ç”¨æˆ·ã€‚`

### 	HDFSçš„APIä½¿ç”¨

### FileSystem

FileSystem æ˜¯æ‰€æœ‰ HDFS æ“ä½œçš„ä¸»å…¥å£ã€‚ç”±äºä¹‹åçš„æ¯ä¸ªå•å…ƒæµ‹è¯•éƒ½éœ€è¦ç”¨åˆ°å®ƒï¼Œè¿™é‡Œä½¿ç”¨ `@Before` æ³¨è§£è¿›è¡Œæ ‡æ³¨ã€‚

```java
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.*;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;

import java.io.IOException;
import java.net.URI;
import java.net.URISyntaxException;
import java.util.Arrays;

public class HdfsClient {

    private FileSystem fs;

    @Before
    public void initO() throws URISyntaxException, IOException, InterruptedException {
        // 1 è·å–æ–‡ä»¶ç³»ç»Ÿ

        Configuration configuration = new Configuration();
        // FileSystem fs = FileSystem.get(new URI("hdfs://hadoop102:8020"), configuration);
        fs = FileSystem.get(new URI("hdfs://hadoop102:8020"), configuration, "atguigu");
    }

    @After
    public void close() throws IOException {

        // 3 å…³é—­èµ„æº
        fs.close();
    }
```



### 		HDFSæ–‡ä»¶ä¸Šä¼ ï¼ˆæµ‹è¯•å‚æ•°ä¼˜å…ˆçº§ï¼‰

```java
    @Test
    public void testMkdirs() throws IOException, URISyntaxException, InterruptedException {

        // 2 åˆ›å»ºç›®å½•
        fs.mkdirs(new Path("/xiyou/huaguoshan1"));
    }
    @Test
    public void testput() throws IOException {
        /*
        * å‚æ•°è§£è¯»
        * å‚æ•°1ï¼š æ˜¯å¦åˆ é™¤æºæ–‡ä»¶
        * å‚æ•°2ï¼š å­˜åœ¨æ˜¯å¦è¦†å†™
        * å‚æ•°3ï¼š æºè·¯å¾„
        * å‚æ•°4ï¼š ç›®æ ‡è·¯å¾„ï¼ˆnew Patchï¼ˆ åè®®://ä¸»æœºå/è·¯å¾„ï¼‰ ï¼‰*/
        fs.copyFromLocalFile(true,true,new Path("D:\\sunwukong.txt"),
                new Path("hdfs://hadoop102/xiyou/huaguoshan"));
    }
```

`å‚æ•°ä¼˜å…ˆçº§æ’åº`ï¼šï¼ˆ1ï¼‰å®¢æˆ·ç«¯ä»£ç ä¸­è®¾ç½®çš„å€¼ >ï¼ˆ2ï¼‰ClassPathä¸‹çš„ç”¨æˆ·è‡ªå®šä¹‰é…ç½®æ–‡ä»¶ >ï¼ˆ3ï¼‰ç„¶åæ˜¯æœåŠ¡å™¨çš„è‡ªå®šä¹‰é…ç½®ï¼ˆxxx-site.xmlï¼‰ >ï¼ˆ4ï¼‰æœåŠ¡å™¨çš„é»˜è®¤é…ç½®ï¼ˆxxx-default.xmlï¼‰

### 		HDFSæ–‡ä»¶ä¸‹è½½

```java
 @Test
    public void testget() throws IOException {

        /*
        * å‚æ•°è§£è¯»
        * å‚æ•°1ï¼šæ˜¯å¦åˆ é™¤æºæ–‡ä»¶
        * å‚æ•°2ï¼š æºè·¯å¾„
        * å‚æ•°3ï¼š ç›®æ ‡è·¯å¾„
        * å‚æ•°4ï¼š æ˜¯å¦å¼€å¯æ–‡ä»¶æ ¡éªŒ*/
        fs.copyToLocalFile(false,
                new Path("hdfs://hadoop102/xiyou/huaguoshan"),
                new Path("D:\\sunwukong"),true);

    }
```



### 		HDFSæ–‡ä»¶æ›´åå’Œç§»åŠ¨

```java
@Test
public void testMv() throws IOException {
    //å‚æ•°è§£è¯» å‚æ•°1ï¼š æºæ–‡ä»¶è·¯å¾„  å‚æ•°2ï¼š ç›®æ ‡æ–‡ä»¶è·¯å¾„
    fs.rename(new Path("/input/word.txt")
            ,new Path("/input/ss.txt"));
     //æ–‡ä»¶çš„ç§»åŠ¨å’Œæ›´å
    fs.rename(new Path("/input/word.txt")
            ,new Path("/cls.txt"));
    //ç›®å½•æ”¹å
    fs.rename(new Path("/input")
            ,new Path("/output"));
}
```

### 		HDFSåˆ é™¤æ–‡ä»¶å’Œç›®å½•

```java
@Test
public void testRm() throws IOException {
    //  å‚æ•°è§£è¯» å‚æ•°1ï¼šè¦åˆ é™¤çš„è·¯å¾„ç‚¹ å‚æ•°2: æ˜¯å¦é€’å½’
    fs.delete(new Path("/jdk-8u212-linux-x64.tar.gz"),false);
    //åˆ é™¤ç©ºç›®å½•
    fs.delete(new Path("/xiyou"),false);
    //åˆ é™¤éç©ºç›®å½•  é€’å½’åˆ é™¤
    fs.delete(new Path("/emptyDir"));

}
```

### 		HDFSæ–‡ä»¶è¯¦æƒ…æŸ¥çœ‹

```java
  @Test
    public void fileDetail() throws IOException {
        // 2 è·å–æ–‡ä»¶è¯¦æƒ…
        RemoteIterator<LocatedFileStatus> listFiles = fs.listFiles(new Path("/"), true);

        while (listFiles.hasNext()) {
            LocatedFileStatus fileStatus = listFiles.next();

            System.out.println("========" + fileStatus.getPath() + "=========");
            System.out.println(fileStatus.getPermission());
            System.out.println(fileStatus.getOwner());
            System.out.println(fileStatus.getGroup());
            System.out.println(fileStatus.getLen());
            System.out.println(fileStatus.getModificationTime());
            System.out.println(fileStatus.getReplication());
            System.out.println(fileStatus.getBlockSize());
            System.out.println(fileStatus.getPath().getName());

            // è·å–å—ä¿¡æ¯
            BlockLocation[] blockLocations = fileStatus.getBlockLocations();
            System.out.println(Arrays.toString(blockLocations));

        }
    }
}

```



### 		HDFSæ–‡ä»¶å’Œæ–‡ä»¶å¤¹åˆ¤æ–­

```java
@Test
public void testListStatus() throws IOException, InterruptedException, URISyntaxException{

    // 1 è·å–æ–‡ä»¶é…ç½®ä¿¡æ¯
    Configuration configuration = new Configuration();
    FileSystem fs = FileSystem.get(new URI("hdfs://hadoop102:8020"), configuration, "atguigu");

    // 2 åˆ¤æ–­æ˜¯æ–‡ä»¶è¿˜æ˜¯æ–‡ä»¶å¤¹
    FileStatus[] listStatus = fs.listStatus(new Path("/"));

    for (FileStatus fileStatus : listStatus) {

        // å¦‚æœæ˜¯æ–‡ä»¶
        if (fileStatus.isFile()) {
            System.out.println("f:"+fileStatus.getPath().getName());
        }else {
            System.out.println("d:"+fileStatus.getPath().getName());
        }
    }

```

### åˆ¤æ–­æ–‡ä»¶æ˜¯å¦å­˜åœ¨

```java
@Test
public void exist() throws Exception {
    boolean exists = fileSystem.exists(new Path("/hdfs-api/test/a.txt"));
    System.out.println(exists);
}
```



# å››ã€ğŸ”¥HDFSçš„è¯»å†™æµç¨‹ï¼ˆé¢è¯•é‡ç‚¹ï¼‰

## 	HDFSå†™æ•°æ®æµç¨‹

<img src="../image/image-20211231195303166.png" alt="image-20211231195303166"  />

## 		å‰–ææ–‡ä»¶å†™å…¥

ï¼ˆ1ï¼‰å®¢æˆ·ç«¯é€šè¿‡Distributed FileSystemæ¨¡å—å‘NameNodeè¯·æ±‚ä¸Šä¼ æ–‡ä»¶ï¼ŒNameNodeæ£€æŸ¥ç›®æ ‡æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼Œçˆ¶ç›®å½•æ˜¯å¦å­˜åœ¨ã€‚

ï¼ˆ2ï¼‰NameNodeè¿”å›æ˜¯å¦å¯ä»¥ä¸Šä¼ ã€‚

ï¼ˆ3ï¼‰å®¢æˆ·ç«¯è¯·æ±‚ç¬¬ä¸€ä¸ª Blockä¸Šä¼ åˆ°å“ªå‡ ä¸ªDataNodeæœåŠ¡å™¨ä¸Šã€‚

ï¼ˆ4ï¼‰NameNodeè¿”å›3ä¸ªDataNodeèŠ‚ç‚¹ï¼Œåˆ†åˆ«ä¸ºdn1ã€dn2ã€dn3ã€‚

ï¼ˆ5ï¼‰å®¢æˆ·ç«¯é€šè¿‡FSDataOutputStreamæ¨¡å—è¯·æ±‚dn1ä¸Šä¼ æ•°æ®ï¼Œdn1æ”¶åˆ°è¯·æ±‚ä¼šç»§ç»­è°ƒç”¨dn2ï¼Œç„¶ådn2è°ƒç”¨dn3ï¼Œå°†è¿™ä¸ªé€šä¿¡ç®¡é“å»ºç«‹å®Œæˆã€‚

ï¼ˆ6ï¼‰dn1ã€dn2ã€dn3é€çº§åº”ç­”å®¢æˆ·ç«¯ã€‚

ï¼ˆ7ï¼‰å®¢æˆ·ç«¯å¼€å§‹å¾€dn1ä¸Šä¼ ç¬¬ä¸€ä¸ªBlockï¼ˆå…ˆä»ç£ç›˜è¯»å–æ•°æ®æ”¾åˆ°ä¸€ä¸ªæœ¬åœ°å†…å­˜ç¼“å­˜ï¼‰ï¼Œä»¥Packetä¸ºå•ä½ï¼Œdn1æ”¶åˆ°ä¸€ä¸ªPacketå°±ä¼šä¼ ç»™dn2ï¼Œdn2ä¼ ç»™dn3ï¼›dn1æ¯ä¼ ä¸€ä¸ª`packet`ä¼šæ”¾å…¥ä¸€ä¸ªåº”ç­”é˜Ÿåˆ—ç­‰å¾…åº”ç­”ã€‚

ï¼ˆ8ï¼‰å½“ä¸€ä¸ªBlockä¼ è¾“å®Œæˆä¹‹åï¼Œå®¢æˆ·ç«¯å†æ¬¡è¯·æ±‚NameNodeä¸Šä¼ ç¬¬äºŒä¸ªBlockçš„æœåŠ¡å™¨ã€‚ï¼ˆé‡å¤æ‰§è¡Œ3-7æ­¥ï¼‰ã€‚

## 		ç½‘ç»œæ‹“æ‰‘-èŠ‚ç‚¹è·ç¦»è®¡ç®—

 `ä¸¤ä¸ªæœºå™¨ï¼ˆèŠ‚ç‚¹ï¼‰çš„ç›´æ¥çš„è·ç¦»`

![image-20211231202004132](../image/image-20211231202004132.png)

`n-1   â€”â€”â€”â€”   n-2`    ä¸º`2`

## 		æœºæ¶æ„ŸçŸ¥ï¼ˆå‰¯æœ¬å­˜å‚¨èŠ‚ç‚¹é€‰æ‹©ï¼‰

==ç¬¬ä¸€ä¸ªå‰¯æœ¬åœ¨Clinentæ‰€å¤„çš„èŠ‚ç‚¹ä¸Šï¼Œå¦‚æœå®¢æˆ·ç«¯åœ¨é›†ç¾¤å¤–ï¼Œéšæœºé€‰ä¸€ä¸ª==

==ç¬¬äºŒä¸ªå‰¯æœ¬åœ¨Â·å¦ä¸€ä¸ªæœºæ¶çš„éšæœºä¸€ä¸ªèŠ‚ç‚¹==

==ç¬¬ä¸‰ä¸ªå‰¯æœ¬åœ¨ç¬¬äºŒä¸ªå‰¯æœ¬æ‰€åœ¨æœºæ¶çš„éšæœºèŠ‚ç‚¹==

![image-20211231205433372](../image/image-20211231205433372.png)

## 	HDFSè¯»æ•°æ®æµç¨‹

![image-20211231205511276](../image/image-20211231205511276.png)

> ï¼ˆ1ï¼‰å®¢æˆ·ç«¯é€šè¿‡DistributedFileSystemå‘NameNodeè¯·æ±‚ä¸‹è½½æ–‡ä»¶ï¼ŒNameNodeé€šè¿‡æŸ¥è¯¢å…ƒæ•°æ®ï¼Œæ‰¾åˆ°æ–‡ä»¶å—æ‰€åœ¨çš„DataNodeåœ°å€ã€‚
>
> ï¼ˆ2ï¼‰æŒ‘é€‰ä¸€å°DataNodeï¼ˆå°±è¿‘åŸåˆ™ï¼Œç„¶åéšæœºï¼‰æœåŠ¡å™¨ï¼Œè¯·æ±‚è¯»å–æ•°æ®ã€‚
>
> ï¼ˆ3ï¼‰DataNodeå¼€å§‹ä¼ è¾“æ•°æ®ç»™å®¢æˆ·ç«¯ï¼ˆä»ç£ç›˜é‡Œé¢è¯»å–æ•°æ®è¾“å…¥æµï¼Œä»¥Packetä¸ºå•ä½æ¥åšæ ¡éªŒï¼‰ã€‚
>
> ï¼ˆ4ï¼‰å®¢æˆ·ç«¯ä»¥Packetä¸ºå•ä½æ¥æ”¶ï¼Œå…ˆåœ¨æœ¬åœ°ç¼“å­˜ï¼Œç„¶åå†™å…¥ç›®æ ‡æ–‡ä»¶ã€‚

# äº”ã€NNå’Œ2NN

## 	NNå’Œ2NNå·¥ä½œæœºåˆ¶

`fsimage`å­˜å‚¨çš„æ˜¯æ–‡ä»¶ç³»ç»Ÿçš„æ ‘å½¢ç»“æ„ä¿¡æ¯

`edit`å­˜å‚¨çš„æ˜¯å¯¹æºæ–‡ä»¶çš„å¢åˆ æ”¹æ“ä½œ

![image-20211231210402772](../image/image-20211231210402772.png)

**1ï¼‰ç¬¬ä¸€é˜¶æ®µï¼šNameNodeå¯åŠ¨**

ï¼ˆ1ï¼‰ç¬¬ä¸€æ¬¡å¯åŠ¨NameNodeæ ¼å¼åŒ–åï¼Œåˆ›å»ºFsimageå’ŒEditsæ–‡ä»¶ã€‚å¦‚æœä¸æ˜¯ç¬¬ä¸€æ¬¡å¯åŠ¨ï¼Œç›´æ¥åŠ è½½ç¼–è¾‘æ—¥å¿—å’Œé•œåƒæ–‡ä»¶åˆ°å†…å­˜ã€‚

ï¼ˆ2ï¼‰å®¢æˆ·ç«¯å¯¹å…ƒæ•°æ®è¿›è¡Œå¢åˆ æ”¹çš„è¯·æ±‚ã€‚

ï¼ˆ3ï¼‰NameNodeè®°å½•æ“ä½œæ—¥å¿—ï¼Œæ›´æ–°æ»šåŠ¨æ—¥å¿—ã€‚

ï¼ˆ4ï¼‰NameNodeåœ¨å†…å­˜ä¸­å¯¹å…ƒæ•°æ®è¿›è¡Œå¢åˆ æ”¹ã€‚

**2ï¼‰ç¬¬äºŒé˜¶æ®µï¼šSecondary NameNodeå·¥ä½œ**

ï¼ˆ1ï¼‰Secondary NameNodeè¯¢é—®NameNodeæ˜¯å¦éœ€è¦CheckPointã€‚ç›´æ¥å¸¦å›NameNodeæ˜¯å¦æ£€æŸ¥ç»“æœã€‚

ï¼ˆ2ï¼‰Secondary NameNodeè¯·æ±‚æ‰§è¡ŒCheckPointã€‚

ï¼ˆ3ï¼‰NameNodeæ»šåŠ¨æ­£åœ¨å†™çš„Editsæ—¥å¿—ã€‚

ï¼ˆ4ï¼‰å°†æ»šåŠ¨å‰çš„ç¼–è¾‘æ—¥å¿—å’Œé•œåƒæ–‡ä»¶æ‹·è´åˆ°Secondary NameNodeã€‚

ï¼ˆ5ï¼‰Secondary NameNodeåŠ è½½ç¼–è¾‘æ—¥å¿—å’Œé•œåƒæ–‡ä»¶åˆ°å†…å­˜ï¼Œå¹¶åˆå¹¶ã€‚

ï¼ˆ6ï¼‰ç”Ÿæˆæ–°çš„é•œåƒæ–‡ä»¶fsimage.chkpointã€‚

ï¼ˆ7ï¼‰æ‹·è´fsimage.chkpointåˆ°NameNodeã€‚

ï¼ˆ8ï¼‰NameNodeå°†fsimage.chkpointé‡æ–°å‘½åæˆfsimageã€‚

## 	Fsimageå’ŒEditsè§£æ

`Fsimageæ–‡ä»¶`ï¼šï¼¨ï¼¤ï¼¦ï¼³æ–‡ä»¶ç³»ç»Ÿå…ƒæ•°æ®çš„ä¸€ä¸ªæ°¸ä¹…æ€§æ£€æŸ¥ç‚¹ï¼Œå…¶ä¸­åŒ…å«HDFSæ–‡ä»¶ç³»ç»Ÿçš„æ‰€æœ‰ç›®å½•å’Œæ–‡ä»¶çš„åºåˆ—åŒ–ä¿¡æ¯

`Editæ–‡ä»¶`ï¼šå­˜æ”¾HDFSæ–‡ä»¶ç³»ç»Ÿçš„æ‰€æœ‰æ›´æ–°æ“ä½œçš„è·¯å¾„,æ–‡ä»¶ç³»ç»Ÿå®¢æˆ·ç«¯æ‰§è¡Œçš„æ‰€æœ‰å†™æ“ä½œ

`seen_txidæ–‡ä»¶`ä¿å­˜çš„æ˜¯ä¸€ä¸ªæ•°å­—,å°±æ˜¯æœ€åä¸€ä¸ªedits_çš„æ•°å­—

æ¯æ¬¡NNå¯åŠ¨æ—¶éƒ½ä¼šå°†Fsimageæ–‡ä»¶è¯»å…¥å†…å­˜,åŠ è½½Editsé‡Œé¢çš„æ›´æ–°æ“ä½œ,ä¿è¯å†…å­˜ä¸­çš„å…ƒæ•°æ®æœ€æ–°çš„,åŒæ­¥çš„,å¯ä»¥çœ‹æˆNNå¯åŠ¨çš„æ—¶å€™å°±å°†Fsimageå’ŒEditsæ–‡ä»¶è¿›è¡Œäº†åˆå¹¶

##### 1ï¼‰oivæŸ¥çœ‹Fsimageæ–‡ä»¶

> åŸºæœ¬è¯­æ³•     **hdfs oiv -p æ–‡ä»¶ç±»å‹ -ié•œåƒæ–‡ä»¶ -o è½¬æ¢åæ–‡ä»¶è¾“å‡ºè·¯å¾„**

```sh
[atguigu@hadoop102 current]$ pwd
/opt/module/hadoop-3.1.3/data/dfs/name/current

[atguigu@hadoop102 current]$ hdfs oiv -p XML -i fsimage_0000000000000000025 -o /opt/module/hadoop-3.1.3/fsimage.xml

[atguigu@hadoop102 current]$ cat /opt/module/hadoop-3.1.3/fsimage.xml


```

##### **2**ï¼‰oevæŸ¥çœ‹Editsæ–‡ä»¶

> åŸºæœ¬è¯­æ³•        **hdfs oev -p æ–‡ä»¶ç±»å‹ -i  ç¼–è¾‘æ—¥å¿— -o è½¬æ¢åæ–‡ä»¶è¾“å‡ºè·¯å¾„**

```sh
[atguigu@hadoop102 current]$ hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o /opt/module/hadoop-3.1.3/edits.xml

[atguigu@hadoop102 current]$ cat /opt/module/hadoop-3.1.3/edits.xml

```









## 	CheckPointæ—¶é—´è®¾ç½®

==1ï¼‰é€šå¸¸æƒ…å†µä¸‹ï¼ŒSecondaryNameNodeæ¯éš”ä¸€å°æ—¶æ‰§è¡Œä¸€æ¬¡==

```xml
	[hdfs-default.xml]
<property>
  <name>dfs.namenode.checkpoint.period</name>
  <value>3600s</value>
</property>

```

==2ï¼‰ä¸€åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡æ“ä½œæ¬¡æ•°ï¼Œå½“æ“ä½œæ¬¡æ•°è¾¾åˆ°1ç™¾ä¸‡æ—¶ï¼ŒSecondaryNameNodeæ‰§è¡Œä¸€æ¬¡ã€‚==

```xml
<property>
  <name>dfs.namenode.checkpoint.txns</name>
  <value>1000000</value>
<description>æ“ä½œåŠ¨ä½œæ¬¡æ•°</description>
</property>

<property>
  <name>dfs.namenode.checkpoint.check.period</name>
  <value>60s</value>
<description> 1åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡æ“ä½œæ¬¡æ•°</description>
</property>

```







# å…­ã€DataNode

## DataNodeå·¥ä½œæœºåˆ¶

![image-20211231213740949](../image/image-20211231213740949.png)

ï¼ˆ1ï¼‰ä¸€ä¸ªæ•°æ®å—åœ¨DataNodeä¸Šä»¥æ–‡ä»¶å½¢å¼å­˜å‚¨åœ¨ç£ç›˜ä¸Šï¼ŒåŒ…æ‹¬ä¸¤ä¸ªæ–‡ä»¶ï¼Œä¸€**ä¸ªæ˜¯æ•°æ®æœ¬èº«ï¼Œä¸€ä¸ªæ˜¯å…ƒæ•°æ®åŒ…æ‹¬æ•°æ®å—çš„é•¿åº¦ï¼Œå—æ•°æ®çš„æ ¡éªŒå’Œä»¥åŠæ—¶é—´æˆ³ã€‚**

ï¼ˆ2ï¼‰DataNodeå¯åŠ¨åå‘NameNodeæ³¨å†Œï¼Œé€šè¿‡åï¼Œ**å‘¨æœŸæ€§ï¼ˆ6å°æ—¶ï¼‰**çš„å‘NameNodeä¸ŠæŠ¥æ‰€æœ‰çš„å—ä¿¡æ¯ã€‚

DNå‘NNæ±‡æŠ¥å½“å‰è§£è¯»ä¿¡æ¯çš„æ—¶é—´é—´éš”ï¼Œé»˜è®¤6å°æ—¶ï¼›

```xml
<property>
	<name>dfs.blockreport.intervalMsec</name>
	<value>21600000</value>
	<description>Determines block reporting interval in milliseconds.</description>
</property>

```

DNæ‰«æè‡ªå·±èŠ‚ç‚¹å—ä¿¡æ¯åˆ—è¡¨çš„æ—¶é—´ï¼Œé»˜è®¤6å°æ—¶

```xml
<property>
	<name>dfs.datanode.directoryscan.interval</name>
	<value>21600s</value>
	<description>Interval in seconds for Datanode to scan data directories and reconcile the difference between blocks in memory and on the disk.
	Support multiple time unit suffix(case insensitive), as described
	in dfs.heartbeat.interval.
	</description>
</property>

```

ï¼ˆ3ï¼‰å¿ƒè·³æ˜¯æ¯3ç§’ä¸€æ¬¡ï¼Œ**å¿ƒè·³è¿”å›ç»“æœå¸¦æœ‰NameNodeç»™è¯¥DataNodeçš„å‘½ä»¤å¦‚å¤åˆ¶å—æ•°æ®åˆ°å¦ä¸€å°æœºå™¨**ï¼Œæˆ–åˆ é™¤æŸä¸ªæ•°æ®å—ã€‚å¦‚æœè¶…è¿‡10åˆ†é’Ÿæ²¡æœ‰æ”¶åˆ°æŸä¸ªDataNodeçš„å¿ƒè·³ï¼Œåˆ™è®¤ä¸ºè¯¥èŠ‚ç‚¹ä¸å¯ç”¨ã€‚

ï¼ˆ4ï¼‰**é›†ç¾¤è¿è¡Œä¸­å¯ä»¥å®‰å…¨åŠ å…¥å’Œé€€å‡ºä¸€äº›æœºå™¨ã€‚**









## æ•°æ®å®Œæ•´æ€§

ğŸ§æ€è€ƒï¼š

å¦‚æœç”µè„‘ç£ç›˜é‡Œé¢å­˜å‚¨çš„æ•°æ®æ˜¯æ§åˆ¶é«˜é“ä¿¡å·ç¯çš„çº¢ç¯ä¿¡å·ï¼ˆ1ï¼‰å’Œç»¿ç¯ä¿¡å·ï¼ˆ0ï¼‰ï¼Œä½†æ˜¯å­˜å‚¨è¯¥æ•°æ®çš„ç£ç›˜åäº†ï¼Œä¸€ç›´æ˜¾ç¤ºæ˜¯ç»¿ç¯ï¼Œæ˜¯å¦å¾ˆå±é™©ï¼ŸåŒç†DataNodeèŠ‚ç‚¹ä¸Šçš„æ•°æ®æŸåäº†ï¼Œå´æ²¡æœ‰å‘ç°ï¼Œæ˜¯å¦ä¹Ÿå¾ˆå±é™©ï¼Œé‚£ä¹ˆå¦‚ä½•è§£å†³å‘¢ï¼Ÿ

å¦‚ä¸‹æ˜¯DataNodeèŠ‚ç‚¹ä¿è¯æ•°æ®å®Œæ•´æ€§çš„æ–¹æ³•ã€‚

ï¼ˆ1ï¼‰å½“DataNodeè¯»å–Blockçš„æ—¶å€™ï¼Œå®ƒä¼šè®¡ç®—`CheckSum`ã€‚

ï¼ˆ2ï¼‰å¦‚æœè®¡ç®—åçš„CheckSumï¼Œä¸Blockåˆ›å»ºæ—¶å€¼`ä¸ä¸€æ ·`ï¼Œè¯´æ˜Blockå·²ç»æŸåã€‚

ï¼ˆ3ï¼‰Clientè¯»å–`å…¶ä»–DataNodeä¸Šçš„Block`ã€‚

ï¼ˆ4ï¼‰å¸¸è§çš„æ ¡éªŒç®—æ³•`crcï¼ˆ32ï¼‰`ï¼Œ`md5ï¼ˆ128`ï¼‰ï¼Œ`sha1ï¼ˆ160ï¼‰`

ï¼ˆ5ï¼‰DataNodeåœ¨å…¶æ–‡ä»¶åˆ›å»ºåå‘¨æœŸéªŒè¯CheckSumã€‚





## æ‰çº¿æ—¶é™å‚æ•°è®¾ç½®

   ![image-20211231215030749](../image/image-20211231215030749.png)

==æ³¨æ„==:   hdfs-site.xml é…ç½®æ–‡ä»¶ä¸­çš„`heartbeat.recheck.interval`çš„å•ä½ä¸ºæ¯«ç§’ï¼Œ`dfs.heartbeat.interval`çš„å•ä½ä¸ºç§’ã€‚

```xml
<property>
    <name>dfs.namenode.heartbeat.recheck-interval</name>
    <value>300000</value>
</property>

<property>
    <name>dfs.heartbeat.interval</name>
    <value>3</value>
</property>

```





# ä¸ƒã€ğŸš©æ€»ç»“





### 1ã€å¸¸ç”¨ç«¯å£å·



==Hadoop3.x==

|     HDFS NameNode å†…éƒ¨é€šå¸¸ç«¯å£ï¼š     | 8020/9000/9820 |
| :----------------------------------: | -------------- |
| **HDFS NameNode å¯¹ç”¨æˆ·çš„æŸ¥è¯¢ç«¯å£ï¼š** | **9870**       |
|     **YarnæŸ¥çœ‹ä»»åŠ¡è¿è¡Œæƒ…å†µçš„ï¼š**     | **8088**       |
|           **å†å²æœåŠ¡å™¨ï¼š**           | **19888**      |


Hadoop2.x 

|   HDFS NameNode å†…éƒ¨é€šå¸¸ç«¯å£ï¼š   | 8020/9000 |
| :------------------------------: | --------- |
| HDFS NameNode å¯¹ç”¨æˆ·çš„æŸ¥è¯¢ç«¯å£ï¼š | 50070     |
|     YarnæŸ¥çœ‹ä»»åŠ¡è¿è¡Œæƒ…å†µçš„ï¼š     | 8088      |
|           å†å²æœåŠ¡å™¨ï¼š           | 19888     |



### 	2ã€å¸¸ç”¨çš„é…ç½®æ–‡ä»¶

|             3.x             | 2.x                        |
| :-------------------------: | -------------------------- |
|      **core-site.xml**      | **core-site.xml**          |
|      **hdfs-site.xml**      | **hdfs-site.xml**          |
|      **yarn-site.xml**      | **yarn-site.xml**          |
| **mapred-site.xml workers** | **mapred-site.xml slaves** |


â€‹	 


### 3ã€é¢è¯•é‡ç‚¹

1ã€HDFSæ–‡ä»¶å—å¤§å°ï¼ˆé¢è¯•é‡ç‚¹ï¼‰
	ç¡¬ç›˜è¯»å†™é€Ÿåº¦
	åœ¨ä¼ä¸šä¸­  ä¸€èˆ¬128mï¼ˆä¸­å°å…¬å¸ï¼‰   256m ï¼ˆå¤§å…¬å¸ï¼‰

2ã€HDFSçš„Shellæ“ä½œï¼ˆå¼€å‘é‡ç‚¹ï¼‰

3ã€HDFSçš„è¯»å†™æµç¨‹ï¼ˆé¢è¯•é‡ç‚¹ï¼‰
