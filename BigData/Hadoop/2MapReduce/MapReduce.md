# MapReduce



## ä¸€ã€MapReduceæ¦‚è¿°

### å®šä¹‰

MapReduceæ˜¯ä¸€ä¸ª==åˆ†å¸ƒå¼è¿ç®—ç¨‹åº==çš„ç¼–ç¨‹æ¡†æ¶ï¼Œæ˜¯ç”¨æˆ·å¼€å‘â€œåŸºäºHadoopçš„æ•°æ®åˆ†æåº”ç”¨â€çš„æ ¸å¿ƒæ¡†æ¶ã€‚

MapReduceæ ¸å¿ƒåŠŸèƒ½æ˜¯å°†==ç”¨æˆ·ç¼–å†™çš„ä¸šåŠ¡é€»è¾‘ä»£ç ==å’Œ==è‡ªå¸¦é»˜è®¤ç»„ä»¶==æ•´åˆæˆä¸€ä¸ªå®Œæ•´çš„==åˆ†å¸ƒå¼è¿ç®—ç¨‹åº==ï¼Œå¹¶å‘è¿è¡Œåœ¨ä¸€ä¸ªHadoopé›†ç¾¤ä¸Šã€‚

### ä¼˜ç¼ºç‚¹

#### ä¼˜ç‚¹

+ æ˜“äºç¼–ç¨‹ã€‚ç”¨æˆ·åªå…³å¿ƒä¸šåŠ¡é€»è¾‘   ï¼Œå®ç°æ¡†æ¶çš„æ¥å£
+ **è‰¯å¥½çš„æ‰©å±•æ€§**ï¼šåŠ¨æ€å¢åŠ Â·æœåŠ¡å™¨ï¼Œè§£å†³è®¡ç®—èµ„æºä¸å¤Ÿçš„é—®é¢˜
+ **é«˜å®¹é”™æ€§**  å…¶ä¸­ä¸€å°æœºå™¨æŒ‚äº†ï¼Œå®ƒå¯ä»¥æŠŠä¸Šé¢çš„è®¡ç®—ä»»åŠ¡è½¬ç§»åˆ°å¦å¤–ä¸€ä¸ªèŠ‚ç‚¹ä¸Šè¿è¡Œï¼Œä¸è‡³äºè¿™ä¸ªä»»åŠ¡è¿è¡Œå¤±è´¥
+ **é€‚åˆPBçº§ä»¥ä¸Šæµ·é‡æ•°æ®çš„ç¦»çº¿å¤„ç†**

#### ç¼ºç‚¹

- **ä¸æ“…é•¿å®æ—¶è®¡ç®—**ï¼šMapReduceæ— æ³•åƒMySQLä¸€æ ·ï¼Œåœ¨æ¯«ç§’æˆ–è€…ç§’çº§å†…è¿”å›ç»“æœ

- **ä¸æ“…é•¿æµå¼è®¡ç®—**ã€‚  ï¼ˆSparksteaming Flink æ“…é•¿ï¼‰

- ä¸æ“…é•¿**DAG**æœ‰å‘æ— ç¯å›¾è®¡ç®—ã€‚[^DAG]:æœºå™¨â‘ çš„ç»“æœä½œä¸ºæœºå™¨â‘¡çš„è¾“å…¥å€¼ï¼Œæœºå™¨â‘¡çš„ç»“æœä½œä¸ºæœºå™¨â‘¢çš„è¾“å…¥ã€å¾€ä¸‹å¾ªç¯ã€‘åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒMapReduceå¹¶ä¸æ˜¯ä¸èƒ½åšï¼Œè€Œæ˜¯ä½¿ç”¨åï¼Œ==æ¯ä¸ªMapReduceä½œä¸šçš„è¾“å‡ºç»“æœéƒ½ä¼šå†™å…¥åˆ°ç£ç›˜ï¼Œä¼šé€ æˆå¤§é‡çš„ç£ç›˜IOï¼Œå¯¼è‡´æ€§èƒ½éå¸¸çš„ä½ä¸‹ã€‚==

  

### æ ¸å¿ƒæ€æƒ³

  MapReduce ä½œä¸šé€šè¿‡å°†è¾“å…¥çš„æ•°æ®é›†æ‹†åˆ†ä¸ºç‹¬ç«‹çš„å—ï¼Œè¿™äº›å—ç”± `map` ä»¥å¹¶è¡Œçš„æ–¹å¼å¤„ç†ï¼Œæ¡†æ¶å¯¹ `map` çš„è¾“å‡ºè¿›è¡Œæ’åºï¼Œç„¶åè¾“å…¥åˆ° `reduce` ä¸­ã€‚MapReduce æ¡†æ¶ä¸“é—¨ç”¨äº `<keyï¼Œvalue>` é”®å€¼å¯¹å¤„ç†ï¼Œå®ƒå°†ä½œä¸šçš„è¾“å…¥è§†ä¸ºä¸€ç»„ `<keyï¼Œvalue>` å¯¹ï¼Œå¹¶ç”Ÿæˆä¸€ç»„ `<keyï¼Œvalue>` å¯¹ä½œä¸ºè¾“å‡ºã€‚è¾“å…¥å’Œè¾“å‡ºçš„ `key` å’Œ `value` éƒ½å¿…é¡»å®ç°[Writable](http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/io/Writable.html) æ¥å£ã€‚![image-20220102174546935](../image/image-20220102174546935.png)

è¿™é‡Œä»¥è¯é¢‘ç»Ÿè®¡ä¸ºä¾‹è¿›è¡Œè¯´æ˜ï¼ŒMapReduce å¤„ç†çš„æµç¨‹å¦‚ä¸‹ï¼š

1. **input** : è¯»å–æ–‡æœ¬æ–‡ä»¶ï¼›
2. **splitting** : å°†æ–‡ä»¶æŒ‰ç…§è¡Œè¿›è¡Œæ‹†åˆ†ï¼Œæ­¤æ—¶å¾—åˆ°çš„ `K1` è¡Œæ•°ï¼Œ`V1` è¡¨ç¤ºå¯¹åº”è¡Œçš„æ–‡æœ¬å†…å®¹ï¼›
3. **mapping** : å¹¶è¡Œå°†æ¯ä¸€è¡ŒæŒ‰ç…§ç©ºæ ¼è¿›è¡Œæ‹†åˆ†ï¼Œæ‹†åˆ†å¾—åˆ°çš„ `List(K2,V2)`ï¼Œå…¶ä¸­ `K2` ä»£è¡¨æ¯ä¸€ä¸ªå•è¯ï¼Œç”±äºæ˜¯åšè¯é¢‘ç»Ÿè®¡ï¼Œæ‰€ä»¥ `V2` çš„å€¼ä¸º 1ï¼Œä»£è¡¨å‡ºç° 1 æ¬¡ï¼›
4. **shuffling**ï¼šç”±äº `Mapping` æ“ä½œå¯èƒ½æ˜¯åœ¨ä¸åŒçš„æœºå™¨ä¸Šå¹¶è¡Œå¤„ç†çš„ï¼Œæ‰€ä»¥éœ€è¦é€šè¿‡ `shuffling` å°†ç›¸åŒ `key` å€¼çš„æ•°æ®åˆ†å‘åˆ°åŒä¸€ä¸ªèŠ‚ç‚¹ä¸Šå»åˆå¹¶ï¼Œè¿™æ ·æ‰èƒ½ç»Ÿè®¡å‡ºæœ€ç»ˆçš„ç»“æœï¼Œæ­¤æ—¶å¾—åˆ° `K2` ä¸ºæ¯ä¸€ä¸ªå•è¯ï¼Œ`List(V2)` ä¸ºå¯è¿­ä»£é›†åˆï¼Œ`V2` å°±æ˜¯ Mapping ä¸­çš„ V2ï¼›
5. **Reducing** : è¿™é‡Œçš„æ¡ˆä¾‹æ˜¯ç»Ÿè®¡å•è¯å‡ºç°çš„æ€»æ¬¡æ•°ï¼Œæ‰€ä»¥ `Reducing` å¯¹ `List(V2)` è¿›è¡Œå½’çº¦æ±‚å’Œæ“ä½œï¼Œæœ€ç»ˆè¾“å‡ºã€‚

MapReduce ç¼–ç¨‹æ¨¡å‹ä¸­ `splitting` å’Œ `shuffing` æ“ä½œéƒ½æ˜¯ç”±æ¡†æ¶å®ç°çš„ï¼Œéœ€è¦æˆ‘ä»¬è‡ªå·±ç¼–ç¨‹å®ç°çš„åªæœ‰ `mapping` å’Œ `reducing`ï¼Œ==è¿™ä¹Ÿå°±æ˜¯ MapReduce è¿™ä¸ªç§°å‘¼çš„æ¥æºã€‚==

ä¸€ä¸ªå®Œæ•´çš„MapReduceç¨‹åºåœ¨åˆ†å¸ƒå¼è¿è¡Œæ—¶æœ‰==ä¸‰ç±»å®ä¾‹è¿›ç¨‹==ï¼š

ï¼ˆ1ï¼‰**MrAppMaster**ï¼šè´Ÿè´£æ•´ä¸ªç¨‹åºçš„è¿‡ç¨‹è°ƒåº¦åŠçŠ¶æ€åè°ƒã€‚

ï¼ˆ2ï¼‰**MapTask**ï¼šè´Ÿè´£Mapé˜¶æ®µçš„æ•´ä¸ªæ•°æ®å¤„ç†æµç¨‹ã€‚

ï¼ˆ3ï¼‰**ReduceTask**ï¼šè´Ÿè´£Reduceé˜¶æ®µçš„æ•´ä¸ªæ•°æ®å¤„ç†æµç¨‹ã€‚



> **å¸¸ç”¨æ•°æ®åºåˆ—åŒ–ç±»å‹**

| **Java**ç±»å‹ | Hadoop Writableç±»å‹ |
| ------------ | ------------------- |
| Boolean      | BooleanWritable     |
| Byte         | ByteWritable        |
| Int          | IntWritable         |
| Float        | FloatWritable       |
| Long         | LongWritable        |
| Double       | DoubleWritable      |
| ==String==   | ==Text==            |
| Map          | MapWritable         |
| Array        | ArrayWritable       |
| Null         | NullWritable        |



### MapReduceç¼–ç¨‹è§„èŒƒ

**Mapperé˜¶æ®µ**

+ ç”¨æˆ·è‡ªå®šä¹‰çš„Mapperè¦ç»§æ‰¿è‡ªå·±çš„çˆ¶ç±»
+ Mapperçš„å‡ºå…¥æ•°æ®æ˜¯KVå¯¹å½¢å¼ï¼ˆKVçš„ç±»å‹å¯è‡ªå®šä¹‰ï¼‰
+ Mapperä¸­çš„ä¸šåŠ¡é€»è¾‘å†™åœ¨mapï¼ˆï¼‰æ–¹æ³•ä¸­
+ MApperçš„å¤„å¤„æ•°æ®æ˜¯KVå¯¹å½¢å¼ï¼ˆKVçš„ç±»å‹å¯è‡ªå®šä¹‰ï¼‰
+ ==mapï¼ˆï¼‰æ–¹æ³•ï¼ˆMapTaskè¿›ç¨‹ï¼‰å¯¹æ¯ä¸€ä¸ª<K,V>è°ƒç”¨ä¸€æ¬¡==



**Reduceé˜¶æ®µ**

+ ç”¨æˆ·è‡ªå®šä¹‰çš„Reduceè¦ç»§æ‰¿è‡ªå·±çš„çˆ¶ç±»
+ Reduceçš„è¾“å…¥æ•°æ®ç±»å‹å¯¹åº”Mapperçš„è¾“å…¥æ•°æ®ç±»å‹ä¹Ÿæ˜¯KV
+ Reduceçš„ä¸šåŠ¡é€»è¾‘å†™åœ¨reduceï¼ˆï¼‰æ–¹æ³•ä¸­
+ ==ReduceTaskè¿›ç¨‹å¯¹æ¯ä¸€ç»„ç›¸åŒkçš„<k,v> ç»„è°ƒç”¨ä¸€æ¬¡reduceï¼ˆï¼‰æ–¹æ³•==



**Driveré˜¶æ®µ**

ç›¸å½“äºYARNé›†ç¾¤çš„å®¢æˆ·ç«¯ï¼Œç”¨äºæäº¤æˆ‘ä»¬æ•´ä¸ªç¨‹åºåˆ°YARNé›†ç¾¤ï¼Œæäº¤çš„æ˜¯å°è£…äº†MapReduceç¨‹åºç›¸å…³è£•å…´å‚æ•°çš„jobå¯¹è±¡

~~~java
				Driverçš„7ä¸ªæ­¥éª¤

		// 1 è·å–é…ç½®ä¿¡æ¯ä»¥åŠè·å–jobå¯¹è±¡
		Configuration conf = new Configuration();
		Job job = Job.getInstance(conf);

		// 2 å…³è”æœ¬Driverç¨‹åºçš„jar
		job.setJarByClass(æœ¬Driverç¨‹åºä¸»ç±»å.class);

		// 3 å…³è”Mapperå’ŒReducerçš„jar
		job.setMapperClass(Mapperç¨‹åºä¸»ç±»å.class);
		job.setReducerClass(Reducerç¨‹åºä¸»ç±»å.class);

		// 4 è®¾ç½®Mapperè¾“å‡ºçš„kvç±»å‹
		job.setMapOutputKeyClass(Mapperè¾“å‡ºçš„Kç±»å‹.class);
		job.setMapOutputValueClass(Mapperè¾“å‡ºçš„Vç±»å‹.class);

		// 5 è®¾ç½®æœ€ç»ˆè¾“å‡ºkvç±»å‹
		job.setOutputKeyClass(æœ€ç»ˆè¾“å‡ºkç±»å‹.class);
		job.setOutputValueClass(æœ€ç»ˆè¾“å‡ºkvç±»å‹.class);
		
		// 6 è®¾ç½®è¾“å…¥å’Œè¾“å‡ºè·¯å¾„
		ï¼ˆLinuxé›†ç¾¤ä¸‹ï¼‰
		FileInputFormat.setInputPaths(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
		ï¼ˆWindowsä¸‹ï¼‰
		FileInputFormat.setInputPaths(job, new Path("æºæ–‡ä»¶åœ°å€"));
		FileOutputFormat.setOutputPath(job, new Path("è¾“å‡ºè·¯å¾„"));
		// 7 æäº¤job
		boolean result = job.waitForCompletion(true);
		System.exit(result ? 0 : 1);

~~~



### WordCountæ¡ˆä¾‹

> éœ€æ±‚ï¼šç»Ÿè®¡ä¸€æ–‡ä»¶ä¸­å•è¯å‡ºç°çš„ä¸ªæ•°

#### **ç¯å¢ƒå‡†å¤‡**

+ åˆ›å»ºmavenå·¥ç¨‹

+ æ·»åŠ pomä¾èµ–

  ~~~xml
  <dependencies>
      <dependency>
          <groupId>org.apache.hadoop</groupId>
          <artifactId>hadoop-client</artifactId>
          <version>3.1.3</version>
      </dependency>
      <dependency>
          <groupId>junit</groupId>
          <artifactId>junit</artifactId>
          <version>4.12</version>
      </dependency>
      <dependency>
          <groupId>org.slf4j</groupId>
          <artifactId>slf4j-log4j12</artifactId>
          <version>1.7.30</version>
      </dependency>
  </dependencies>
  
  ~~~

  

+ åœ¨é¡¹ç›®çš„src/main/resourcesç›®å½•ä¸‹ï¼Œæ–°å»ºä¸€ä¸ªæ–‡ä»¶ï¼Œå‘½åä¸ºâ€œlog4j.propertiesâ€æ·»åŠ å¦‚ä¸‹

  ~~~properties
  log4j.rootLogger=INFO, stdout  
  log4j.appender.stdout=org.apache.log4j.ConsoleAppender  
  log4j.appender.stdout.layout=org.apache.log4j.PatternLayout  
  log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n  
  log4j.appender.logfile=org.apache.log4j.FileAppender  
  log4j.appender.logfile.File=target/spring.log  
  log4j.appender.logfile.layout=org.apache.log4j.PatternLayout  
  log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n
  
  ~~~

+ åˆ›å»ºåŒ…å

  

#### ç¼–å†™ç¨‹åº

> æ³¨æ„ï¼šå¯¼åŒ…åˆ«å¯¼é”™

+ ç¼–å†™Mapperç±»

  ~~~java
  package com.atguigu.mapreduce.wordcount;
  import java.io.IOException;
  import org.apache.hadoop.io.IntWritable;
  import org.apache.hadoop.io.LongWritable;
  import org.apache.hadoop.io.Text;
  import org.apache.hadoop.mapreduce.Mapper;
  
  public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable>{
  	
  	Text k = new Text();
  	IntWritable v = new IntWritable(1);
  	
  	@Override
  	protected void map(LongWritable key, Text value, Context context)	throws IOException, InterruptedException {
  		
  		// 1 è·å–ä¸€è¡Œ
  		String line = value.toString();
  		
  		// 2 åˆ‡å‰²
  		String[] words = line.split(" ");
  		
  		// 3 è¾“å‡º
  		for (String word : words) {
  			
  			k.set(word);
  			context.write(k, v);
  		}
  	}
  }
  
  ~~~

+  ç¼–å†™Reducerç±»

  ~~~java
  package com.atguigu.mapreduce.wordcount;
  import java.io.IOException;
  import org.apache.hadoop.io.IntWritable;
  import org.apache.hadoop.io.Text;
  import org.apache.hadoop.mapreduce.Reducer;
  
  public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable>{
  
  int sum;
  IntWritable v = new IntWritable();
  
  	@Override
  	protected void reduce(Text key, Iterable<IntWritable> values,Context context) throws IOException, InterruptedException {
  		
  		// 1 ç´¯åŠ æ±‚å’Œ
  		sum = 0;
  		for (IntWritable count : values) {
  			sum += count.get();
  		}
  		
  		// 2 è¾“å‡º
           v.set(sum);
  		context.write(key,v);
  	}
  }
  
  ~~~

+ ç¼–å†™Driveré©±åŠ¨ç±»

  â€‹							**ç‰¢è®°ä¸ƒä¸ªæ­¥éª¤**

#### æœ¬åœ°æµ‹è¯•

+ éœ€è¦é¦–å…ˆé…ç½®å¥½HADOOP_HOMEå˜é‡ä»¥åŠWindowsè¿è¡Œä¾èµ–

+ åœ¨IDEA/Eclipseä¸Šè¿è¡Œç¨‹åº

### é›†ç¾¤æµ‹è¯•

+ ç”¨mavenæ‰“jaråŒ…ï¼Œéœ€è¦æ·»åŠ çš„æ‰“åŒ…æ’ä»¶ä¾èµ–

  ~~~xml
  <build>
      <plugins>
          <plugin>
              <artifactId>maven-compiler-plugin</artifactId>
              <version>3.6.1</version>
              <configuration>
                  <source>1.8</source>
                  <target>1.8</target>
              </configuration>
          </plugin>
          <plugin>
              <artifactId>maven-assembly-plugin</artifactId>
              <configuration>
                  <descriptorRefs>
                      <descriptorRef>jar-with-dependencies</descriptorRef>
                  </descriptorRefs>
              </configuration>
              <executions>
                  <execution>
                      <id>make-assembly</id>
                      <phase>package</phase>
                      <goals>
                          <goal>single</goal>
                      </goals>
                  </execution>
              </executions>
          </plugin>
      </plugins>
  </build>
  
  ~~~

+ å°†ç¨‹åºæ‰“æˆjaråŒ…

+ ä¿®æ”¹ä¸å¸¦ä¾èµ–çš„jaråŒ…åç§°==ç®€åŒ–åçš„åç§°==.jarï¼Œå¹¶æ‹·è´è¯¥jaråŒ…åˆ°Hadoopé›†ç¾¤çš„/opt/module/hadoop-3.1.3è·¯å¾„ã€‚

+ å¯åŠ¨Hadoopé›†ç¾¤

  ~~~sh
  [atguigu@hadoop102 hadoop-3.1.3]$ sbin/start-dfs.sh
  [atguigu@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh
  ~~~

+ æ‰§è¡ŒWordCountç¨‹åº

  ~~~shell
  [atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar  ç®€åŒ–åçš„åç§°.jar
   com.atguigu.mapreduce.wordcount.WordCountDriver /user/atguigu/input     /user/atguigu/output
  ~~~
  


## äºŒã€Hadoopåºåˆ—åŒ–



### åºåˆ—åŒ–æ¦‚è¿°

1ï¼‰**ä»€ä¹ˆæ˜¯åºåˆ—åŒ–**

**åºåˆ—åŒ–**å°±æ˜¯==æŠŠå†…å­˜ä¸­çš„å¯¹è±¡ï¼Œè½¬æ¢æˆå­—èŠ‚åºåˆ—==ï¼ˆæˆ–å…¶ä»–æ•°æ®ä¼ è¾“åè®®ï¼‰ä»¥ä¾¿äºå­˜å‚¨åˆ°ç£ç›˜ï¼ˆæŒä¹…åŒ–ï¼‰å’Œç½‘ç»œä¼ è¾“ã€‚ 

**ååºåˆ—åŒ–**å°±æ˜¯å°†æ”¶åˆ°å­—èŠ‚åºåˆ—ï¼ˆæˆ–å…¶ä»–æ•°æ®ä¼ è¾“åè®®ï¼‰æˆ–è€…æ˜¯==ç£ç›˜çš„æŒä¹…åŒ–æ•°æ®ï¼Œè½¬æ¢æˆå†…å­˜ä¸­çš„å¯¹è±¡ã€‚==

2ï¼‰**ä¸ºä»€ä¹ˆè¦åºåˆ—åŒ–**

ä¸€èˆ¬æ¥è¯´ï¼Œâ€œæ´»çš„â€å¯¹è±¡åªç”Ÿå­˜åœ¨å†…å­˜é‡Œï¼Œå…³æœºæ–­ç”µå°±æ²¡æœ‰äº†ã€‚è€Œä¸”â€œæ´»çš„â€å¯¹è±¡åªèƒ½ç”±æœ¬åœ°çš„è¿›ç¨‹ä½¿ç”¨ï¼Œä¸èƒ½è¢«å‘é€åˆ°ç½‘ç»œä¸Šçš„å¦å¤–ä¸€å°è®¡ç®—æœºã€‚ ç„¶è€Œ==åºåˆ—åŒ–å¯ä»¥å­˜å‚¨â€œæ´»çš„â€å¯¹è±¡ï¼Œå¯ä»¥å°†â€œæ´»çš„â€å¯¹è±¡å‘é€åˆ°è¿œç¨‹è®¡ç®—æœºã€‚==

3ï¼‰ä¸ºä»€ä¹ˆä¸ç”¨Javaçš„åºåˆ—åŒ–

Javaçš„åºåˆ—åŒ–æ˜¯ä¸€ä¸ªé‡é‡çº§åºåˆ—åŒ–æ¡†æ¶ï¼ˆSerializableï¼‰ï¼Œä¸€ä¸ªå¯¹è±¡è¢«åºåˆ—åŒ–åï¼Œä¼šé™„å¸¦å¾ˆå¤šé¢å¤–çš„ä¿¡æ¯ï¼ˆå„ç§æ ¡éªŒä¿¡æ¯ï¼ŒHeaderï¼Œç»§æ‰¿ä½“ç³»ç­‰ï¼‰ï¼Œ==ä¸ä¾¿äºåœ¨ç½‘ç»œä¸­é«˜æ•ˆä¼ è¾“==ã€‚æ‰€ä»¥ï¼ŒHadoopè‡ªå·±å¼€å‘äº†ä¸€å¥—åºåˆ—åŒ–æœºåˆ¶==ï¼ˆWritableï¼‰==ã€‚

4ï¼‰Hadoopåºåˆ—åŒ–ç‰¹ç‚¹ï¼š

**ï¼ˆ1**ï¼‰ç´§å‡‘ **ï¼š**é«˜æ•ˆä½¿ç”¨å­˜å‚¨ç©ºé—´ã€‚

**ï¼ˆ2**ï¼‰å¿«é€Ÿï¼šè¯»å†™æ•°æ®çš„é¢å¤–å¼€é”€å°ã€‚

**ï¼ˆ3**ï¼‰äº’æ“ä½œï¼šæ”¯æŒå¤šè¯­è¨€çš„äº¤äº’



### è‡ªå®šä¹‰beanå¯¹è±¡å®ç°åºåˆ—åŒ–æ¥å£

> **å…·ä½“å®ç°beanå¯¹è±¡åºåˆ—åŒ–æ­¥éª¤å¦‚ä¸‹7æ­¥ã€‚**

+ å¿…é¡»å®ç°Writableæ¥å£

+ ååºåˆ—åŒ–æ—¶ï¼Œéœ€è¦åå°„è°ƒç”¨ç©ºå‚æ„é€ å‡½æ•°ï¼Œæ‰€ä»¥==å¿…é¡»æœ‰ç©ºå‚æ„é€ ==

  ~~~java
  public FlowBean() {
  	super();
  }
  ~~~

+ é‡å†™åºåˆ—åŒ–æ–¹æ³•

  ~~~java
  @Override
  public void write(DataOutput out) throws IOException {
  	out.writeLong(upFlow);
  	out.writeLong(downFlow);
  	out.writeLong(sumFlow);
  }
  ~~~

+ é‡å†™ååºåˆ—åŒ–æ–¹æ³• 

  ~~~java
  @Override
  public void readFields(DataInput in) throws IOException {
  	upFlow = in.readLong();
  	downFlow = in.readLong();
  	sumFlow = in.readLong();
  }
  ~~~

+ ==æ³¨æ„==: ååºåˆ—åŒ–çš„é¡ºåºå’Œåºåˆ—åŒ–çš„é¡ºåºå®Œå…¨ä¸€è‡´

+ è¦æƒ³æŠŠç»“æœæ˜¾ç¤ºåœ¨æ–‡ä»¶ä¸­ï¼Œéœ€è¦**é‡å†™toString()**ï¼Œå¯ç”¨"\t"åˆ†å¼€ï¼Œæ–¹ä¾¿åç»­ç”¨

+ å¦‚æœéœ€è¦å°†è‡ªå®šä¹‰çš„beanæ”¾åœ¨keyä¸­ä¼ è¾“ï¼Œåˆ™è¿˜éœ€è¦==å®ç°Comparableæ¥å£==ï¼Œå› ä¸ºMapReduceæ¡†ä¸­çš„Shuffleè¿‡ç¨‹è¦æ±‚å¯¹keyå¿…é¡»èƒ½æ’åºã€‚

  ~~~java
  @Override
  public int compareTo(FlowBean o) {
  	// å€’åºæ’åˆ—ï¼Œä»å¤§åˆ°å°
  	return this.sumFlow > o.getSumFlow() ? -1 : 1;
  }
  ~~~

  

  

  

  

### åºåˆ—åŒ–æ¡ˆä¾‹å®æ“

> **éœ€æ±‚**
>
> ç»Ÿè®¡æ¯ä¸€ä¸ªæ‰‹æœºå·è€—è´¹çš„æ€»ä¸Šè¡Œæµé‡ã€æ€»ä¸‹è¡Œæµé‡ã€æ€»æµé‡



![image-20220105215709016](../image/image-20220105215709016.png)



**ç¼–å†™MapReduceç¨‹åº**

+ ç¼–å†™æµé‡ç»Ÿè®¡çš„Beanå¯¹è±¡

  ~~~java
  package com.atguigu.mapreduce.writable;
  
  import org.apache.hadoop.io.Writable;
  import java.io.DataInput;
  import java.io.DataOutput;
  import java.io.IOException;
  
  //1 ç»§æ‰¿Writableæ¥å£
  public class FlowBean implements Writable {
  
      private long upFlow; //ä¸Šè¡Œæµé‡
      private long downFlow; //ä¸‹è¡Œæµé‡
      private long sumFlow; //æ€»æµé‡
  
      //2 æä¾›æ— å‚æ„é€ 
      
  		/*æ²¡å•¥çœ‹çš„åˆ äº†*/
      
      //3 æä¾›ä¸‰ä¸ªå‚æ•°çš„getterå’Œsetteræ–¹æ³•
              
     		/*å åœ°æ–¹åˆ äº†*/
              
      //4 å®ç°åºåˆ—åŒ–å’Œååºåˆ—åŒ–æ–¹æ³•,æ³¨æ„é¡ºåºä¸€å®šè¦ä¿æŒä¸€è‡´
      @Override
      public void write(DataOutput dataOutput) throws IOException {
          dataOutput.writeLong(upFlow);
          dataOutput.writeLong(downFlow);
          dataOutput.writeLong(sumFlow);
      }
  
      @Override
      public void readFields(DataInput dataInput) throws IOException {
          this.upFlow = dataInput.readLong();
          this.downFlow = dataInput.readLong();
          this.sumFlow = dataInput.readLong();
      }
  
      //5 é‡å†™ToString
      @Override
      public String toString() {
          return upFlow + "\t" + downFlow + "\t" + sumFlow;
      }
  }
  
  ~~~

+ ç¼–å†™Mapperç±»

  ~~~java
  package com.atguigu.mapreduce.writable;
  
  import org.apache.hadoop.io.LongWritable;
  import org.apache.hadoop.io.Text;
  import org.apache.hadoop.mapreduce.Mapper;
  import java.io.IOException;
  
  public class FlowMapper extends Mapper<LongWritable, Text, Text, FlowBean> {
      private Text outK = new Text();
      private FlowBean outV = new FlowBean();
  
      @Override
      protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
  
          //1 è·å–ä¸€è¡Œæ•°æ®,è½¬æˆå­—ç¬¦ä¸²
          String line = value.toString();
  
          //2 åˆ‡å‰²æ•°æ®
          String[] split = line.split("\t");
  
          //3 æŠ“å–æˆ‘ä»¬éœ€è¦çš„æ•°æ®:æ‰‹æœºå·,ä¸Šè¡Œæµé‡,ä¸‹è¡Œæµé‡
          String phone = split[1];
          String up = split[split.length - 3];
          String down = split[split.length - 2];
  
          //4 å°è£…outK outV
          outK.set(phone);
          outV.setUpFlow(Long.parseLong(up));
          outV.setDownFlow(Long.parseLong(down));
          outV.setSumFlow();
  
          //5 å†™å‡ºoutK outV
          context.write(outK, outV);
      }
  }
  
  
  ~~~

+ ç¼–å†™Reducerç±»

  ~~~java
  package com.atguigu.mapreduce.writable;
  
  import org.apache.hadoop.io.Text;
  import org.apache.hadoop.mapreduce.Reducer;
  import java.io.IOException;
  
  public class FlowReducer extends Reducer<Text, FlowBean, Text, FlowBean> {
      private FlowBean outV = new FlowBean();
      @Override
      protected void reduce(Text key, Iterable<FlowBean> values, Context context) throws IOException, InterruptedException {
  
          long totalUp = 0;
          long totalDown = 0;
  
          //1 éå†values,å°†å…¶ä¸­çš„ä¸Šè¡Œæµé‡,ä¸‹è¡Œæµé‡åˆ†åˆ«ç´¯åŠ 
          for (FlowBean flowBean : values) {
              totalUp += flowBean.getUpFlow();
              totalDown += flowBean.getDownFlow();
          }
  
          //2 å°è£…outKV
          outV.setUpFlow(totalUp);
          outV.setDownFlow(totalDown);
          outV.setSumFlow();
  
          //3 å†™å‡ºoutK outV
          context.write(key,outV);
      }
  }
  
  ~~~

+ ç¼–å†™Driveré©±åŠ¨ç±»

  #ç•¥





## ä¸‰ã€MapReduceæ¡†æ¶åŸç†

![image-20220106221210872](../image/image-20220106221210872.png)



### 1ï¼‰InputFormatæ•°æ®è¾“å…¥

------

==MapTaskçš„å¹¶è¡Œåº¦å†³å®šMapé˜¶æ®µçš„ä»»åŠ¡å¤„ç†å¹¶å‘åº¦ï¼Œè¿›è€Œå½±å“åˆ°æ•´ä¸ªJobçš„å¤„ç†é€Ÿåº¦ã€‚==

+ **MapTask**å¹¶è¡Œåº¦å†³å®šæœºåˆ¶ï¼š

**æ•°æ®å—ï¼š**Blockæ˜¯HDFSç‰©ç†ä¸ŠæŠŠæ•°æ®åˆ†æˆä¸€å—ä¸€å—ã€‚æ•°æ®å—æ˜¯HDFSå­˜å‚¨æ•°æ®å•ä½ã€‚

**æ•°æ®åˆ‡ç‰‡ï¼š**æ•°æ®åˆ‡ç‰‡åªæ˜¯åœ¨é€»è¾‘ä¸Šå¯¹è¾“å…¥è¿›è¡Œåˆ†ç‰‡ï¼Œå¹¶ä¸ä¼šåœ¨ç£ç›˜ä¸Šå°†å…¶åˆ‡åˆ†æˆç‰‡è¿›è¡Œå­˜å‚¨ã€‚æ•°æ®åˆ‡ç‰‡æ˜¯MapReduceç¨‹åºè®¡ç®—è¾“å…¥æ•°æ®çš„å•ä½ï¼Œä¸€ä¸ªåˆ‡ç‰‡ä¼šå¯¹åº”å¯åŠ¨ä¸€ä¸ªMapTaskã€‚



![image-20220106222045764](../image/image-20220106222045764.png)



------

ğŸ˜ª*FileInputFormatå¸¸è§çš„æ¥å£å®ç°ç±»åŒ…æ‹¬ï¼šTextInputFormat(é»˜è®¤)ã€KeyValueTextInputFormatã€NLineInputFormatã€CombineTextInputFormatå’Œè‡ªå®šä¹‰InputFormatç­‰ã€‚*

#### **Jobæäº¤æµç¨‹æºç **

------

> é‡ç‚¹ï¼šJOBä¼šä¸Šä¼ åˆ°é›†ç¾¤ä¸‰ä¸ªæ–‡ä»¶ **jaråŒ…  åˆ‡ç‰‡è§„åˆ’æ–‡ä»¶  XMLé…ç½®æ–‡ä»¶**

~~~java
waitForCompletion()

submit();

// 1å»ºç«‹è¿æ¥
	connect();	
		// 1ï¼‰åˆ›å»ºæäº¤Jobçš„ä»£ç†
		new Cluster(getConfiguration());
			// ï¼ˆ1ï¼‰åˆ¤æ–­æ˜¯æœ¬åœ°è¿è¡Œç¯å¢ƒè¿˜æ˜¯yarné›†ç¾¤è¿è¡Œç¯å¢ƒ
			initialize(jobTrackAddr, conf); 

// 2 æäº¤job
submitter.submitJobInternal(Job.this, cluster)

	// 1ï¼‰åˆ›å»ºç»™é›†ç¾¤æäº¤æ•°æ®çš„Stagè·¯å¾„
	Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);

	// 2ï¼‰è·å–jobid ï¼Œå¹¶åˆ›å»ºJobè·¯å¾„
	JobID jobId = submitClient.getNewJobID();

	// 3ï¼‰æ‹·è´jaråŒ…åˆ°é›†ç¾¤
copyAndConfigureFiles(job, submitJobDir);	
	rUploader.uploadFiles(job, jobSubmitDir);

	// 4ï¼‰è®¡ç®—åˆ‡ç‰‡ï¼Œç”Ÿæˆåˆ‡ç‰‡è§„åˆ’æ–‡ä»¶
writeSplits(job, submitJobDir);
		maps = writeNewSplits(job, jobSubmitDir);
		input.getSplits(job);

	// 5ï¼‰å‘Stagè·¯å¾„å†™XMLé…ç½®æ–‡ä»¶
writeConf(conf, submitJobFile);
	conf.writeXml(out);

	// 6ï¼‰æäº¤Job,è¿”å›æäº¤çŠ¶æ€
status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());

~~~

+ jobæµç¨‹è§£æ

![image-20220106222811409](../image/image-20220106222811409.png)



#### **FileInputFormatåˆ‡ç‰‡æºç **

------

`é‡ç‚¹`ï¼š

   + åˆ‡ç‰‡æ—¶ä¸è€ƒè™‘æ•°æ®é›†æ•´ä½“ã€‚è€Œæ˜¯==é’ˆå¯¹æ¯ä¸€ä¸ªæ–‡ä»¶å•ç‹¬åˆ‡ç‰‡==

   + æºç ä¸­è®¡ç®—åˆ‡ç‰‡å¤§å°çš„å…¬å¼

     ~~~java
     Math.max(minSize, Math.min(maxSize, blockSize))
         /*æ”¹å˜åˆ‡ç‰‡å¤§å°ï¼Œè®©åˆ‡ç‰‡å˜å¤§å¢å¤§minSize
         			  è®©åˆ‡ç‰‡å˜å°å‡å°maxSize
                       blockSizeä¸ºç‰©ç†åˆ‡ç‰‡å¤§å°æ— æ³•æ”¹å˜*/
     mapreduce.input.fileinputformat.split.minsize=1
     mapreduce.input.fileinputformat.split.maxsize=Long.MAXValue  é»˜è®¤å€¼
     ~~~

     [^PS]:==é€šè¿‡æ”¹å˜minSize  maxSizeçš„å¤§å°é—´æ¥çš„æ”¹å˜åˆ‡ç‰‡å—å¤§å°==
     
   + æ¯æ¬¡åˆ‡ç‰‡æ—¶éƒ½è¦åˆ¤æ–­åˆ‡å®Œå‰©ä¸‹çš„éƒ¨åˆ†æ˜¯å¦å¤§äºå—çš„1.1å€ï¼Œä¸å¤§äº1.1å€çš„å°±åˆ’åˆ†ä¸€å—åˆ‡ç‰‡



------



![image-20220106230258950](../image/image-20220106230258950.png)

  

   ![image-20220106230427785](../image/image-20220106230427785.png)

â€‹    

#### TextInputFormat

------

> TextInputFormatæ˜¯é»˜è®¤çš„FileInputFormatå®ç°ç±»ã€‚æŒ‰è¡Œè¯»å–æ¯æ¡è®°å½•ã€‚**é”®æ˜¯å­˜å‚¨è¯¥è¡Œåœ¨æ•´ä¸ªæ–‡ä»¶ä¸­çš„èµ·å§‹å­—èŠ‚åç§»é‡ï¼Œ LongWritableç±»å‹ã€‚å€¼æ˜¯è¿™è¡Œçš„å†…å®¹ï¼Œä¸åŒ…æ‹¬ä»»ä½•è¡Œç»ˆæ­¢ç¬¦ï¼ˆæ¢è¡Œç¬¦å’Œå›è½¦ç¬¦ï¼‰ï¼ŒTextç±»å‹ã€‚**
>



`ç¤ºä¾‹`

+ è¾“å…¥

  ~~~txt
  Rich learning form
  Intelligent learning engine
  Learning more convenient
  From the real demand for more close to the enterprise
  
  ~~~

+ è¾“å‡º

  ~~~txt
   Kï¼Œ  V
  (0,  Rich learning form)
  (20,  Intelligent learning engine)
  (49,  Learning more convenient)
  (74,  From the real demand for more close to the enterprise)
  
  ~~~

  



#### CombineTextInputFormatåˆ‡ç‰‡æœºåˆ¶

------

â€‹	**èƒŒæ™¯ï¼š**

æ¡†æ¶é»˜è®¤çš„TextInputFormatåˆ‡ç‰‡æœºåˆ¶æ˜¯å¯¹ä»»åŠ¡æŒ‰æ–‡ä»¶è§„åˆ’åˆ‡ç‰‡ï¼Œ**ä¸ç®¡æ–‡ä»¶å¤šå°ï¼Œéƒ½ä¼šæ˜¯ä¸€ä¸ªå•ç‹¬çš„åˆ‡ç‰‡**ï¼Œéƒ½ä¼šäº¤ç»™ä¸€ä¸ªMapTaskï¼Œè¿™æ ·å¦‚æœæœ‰å¤§é‡å°æ–‡ä»¶ï¼Œå°±ä¼šäº§ç”Ÿå¤§é‡çš„MapTaskï¼Œå¤„ç†æ•ˆç‡æå…¶ä½ä¸‹ã€‚

â€‹	

â€‹	**è§£å†³æ–¹æ¡ˆ**

CombineTextInputFormatç”¨äºå°æ–‡ä»¶è¿‡å¤šçš„åœºæ™¯ï¼Œå®ƒå¯ä»¥å°†å¤šä¸ªå°æ–‡ä»¶ä»é€»è¾‘ä¸Šè§„åˆ’åˆ°ä¸€ä¸ªåˆ‡ç‰‡ä¸­ï¼Œè¿™æ ·ï¼Œå¤šä¸ªå°æ–‡ä»¶å°±å¯ä»¥äº¤ç»™ä¸€ä¸ªMapTaskå¤„ç†



â€‹	**ä¸šåŠ¡ä»£ç **

~~~java
//JOB  5~6ä¹‹é—´æ·»åŠ 
		
		//é©±åŠ¨ç±»ä¸­æ·»åŠ ä»£ç å¦‚ä¸‹ï¼š
		// å¦‚æœä¸è®¾ç½®InputFormatï¼Œå®ƒé»˜è®¤ç”¨çš„æ˜¯TextInputFormat.class
job.setInputFormatClass(CombineTextInputFormat.class);

			//è™šæ‹Ÿå­˜å‚¨åˆ‡ç‰‡æœ€å¤§å€¼è®¾ç½®4m
CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);

~~~

â€‹	æ³¨æ„ï¼šè™šæ‹Ÿå­˜å‚¨åˆ‡ç‰‡æœ€å¤§å€¼è®¾ç½®æœ€å¥½æ ¹æ®å®é™…çš„å°æ–‡ä»¶å¤§å°æƒ…å†µæ¥è®¾ç½®å…·ä½“çš„å€¼ã€‚



![image-20220106231810228](../image/image-20220106231810228.png)



### 2ï¼‰ğŸš©MapReduceè¯¦ç»†å·¥ä½œæµç¨‹

------



![image-20220109144603521](../image/image-20220109144603521.png)



![image-20220109144619323](../image/image-20220109144619323.png)



> â€‹				Shuffleè¿‡ç¨‹ä»ç¬¬7æ­¥å¼€å§‹åˆ°ç¬¬16æ­¥ç»“æŸ     å…·ä½“æ­¥éª¤å¦‚ä¸‹

**ï¼ˆ1ï¼‰MapTaskæ”¶é›†æˆ‘ä»¬çš„map()æ–¹æ³•è¾“å‡ºçš„kvå¯¹ï¼Œæ”¾åˆ°å†…å­˜ç¼“å†²åŒºä¸­**

**ï¼ˆ2ï¼‰ä»å†…å­˜ç¼“å†²åŒºä¸æ–­æº¢å‡ºæœ¬åœ°ç£ç›˜æ–‡ä»¶ï¼Œå¯èƒ½ä¼šæº¢å‡ºå¤šä¸ªæ–‡ä»¶**

**ï¼ˆ3ï¼‰å¤šä¸ªæº¢å‡ºæ–‡ä»¶ä¼šè¢«åˆå¹¶æˆå¤§çš„æº¢å‡ºæ–‡ä»¶**

**ï¼ˆ4ï¼‰åœ¨æº¢å‡ºè¿‡ç¨‹åŠåˆå¹¶çš„è¿‡ç¨‹ä¸­ï¼Œéƒ½è¦è°ƒç”¨Partitionerè¿›è¡Œåˆ†åŒºå’Œé’ˆå¯¹keyè¿›è¡Œæ’åº**

**ï¼ˆ5ï¼‰ReduceTaskæ ¹æ®è‡ªå·±çš„åˆ†åŒºå·ï¼Œå»å„ä¸ªMapTaskæœºå™¨ä¸Šå–ç›¸åº”çš„ç»“æœåˆ†åŒºæ•°æ®**

**ï¼ˆ6ï¼‰ReduceTaskä¼šæŠ“å–åˆ°åŒä¸€ä¸ªåˆ†åŒºçš„æ¥è‡ªä¸åŒMapTaskçš„ç»“æœæ–‡ä»¶ï¼ŒReduceTaskä¼šå°†è¿™äº›æ–‡ä»¶å†è¿›è¡Œåˆå¹¶ï¼ˆå½’å¹¶æ’åºï¼‰**

**ï¼ˆ7ï¼‰åˆå¹¶æˆå¤§æ–‡ä»¶åï¼ŒShuffleçš„è¿‡ç¨‹ä¹Ÿå°±ç»“æŸäº†ï¼Œåé¢è¿›å…¥ReduceTaskçš„é€»è¾‘è¿ç®—è¿‡ç¨‹ï¼ˆä»æ–‡ä»¶ä¸­å–å‡ºä¸€ä¸ªä¸€ä¸ªçš„é”®å€¼å¯¹Groupï¼Œè°ƒç”¨ç”¨æˆ·è‡ªå®šä¹‰çš„reduce()æ–¹æ³•ï¼‰**

**==æ³¨æ„==ï¼š**

ï¼ˆ1ï¼‰Shuffleä¸­çš„ç¼“å†²åŒºå¤§å°ä¼šå½±å“åˆ°MapReduceç¨‹åºçš„æ‰§è¡Œæ•ˆç‡ï¼ŒåŸåˆ™ä¸Šè¯´ï¼Œç¼“å†²åŒºè¶Šå¤§ï¼Œç£ç›˜ioçš„æ¬¡æ•°è¶Šå°‘ï¼Œæ‰§è¡Œé€Ÿåº¦å°±è¶Šå¿«ã€‚

ï¼ˆ2ï¼‰ç¼“å†²åŒºçš„å¤§å°å¯ä»¥é€šè¿‡å‚æ•°è°ƒæ•´ï¼Œå‚æ•°ï¼š**mapreduce.task.io.sort.mb**é»˜è®¤**100M**ã€‚



### 3) Shuffleæœºåˆ¶

------

#### æ¦‚å¿µ

Mapæ–¹æ³•ä¹‹åï¼ŒReduceæ–¹æ³•ä¹‹å‰çš„æ•°æ®å¤„ç†è¿‡ç¨‹ç§°ä¹‹ä¸ºShuffleã€‚

+ è¯¦ç»†æµç¨‹

  ![image-20220109145658111](../image/image-20220109145658111.png)

+ 

#### Partitionåˆ†åŒº

------

+ ä¸»è¦åŠŸèƒ½

  **å°†ç»“æœæŒ‰ç…§ä¸åŒçš„éœ€æ±‚è¾“å‡ºåˆ°ä¸åŒçš„æ–‡ä»¶ä¸­**



+ é»˜è®¤åˆ†åŒº

  ~~~java
  public class HashPartitioner<K2, V2> implements Partitioner<K2, V2> {
  
  
      public int getPartition(K2 key, V2 value, int numReduceTasks) {
          return (key.hashCode() &Integer.MAX_VALUE ) % numReduceTasks;
      }								2147483647
  }
  ~~~

  é»˜è®¤åˆ†åŒºæ˜¯æ ¹æ®**keyçš„hashCodeå¯¹ReduceTaskä¸ªæ•°å–æ¨¡**å¾—åˆ°çš„ã€‚ç”¨æˆ·æ²¡æ³•æ§åˆ¶å“ªä¸ªkeyå­˜å‚¨åˆ°é‚£ä¸ªåˆ†åŒº



+ **è‡ªå®šä¹‰Partitioneræ­¥éª¤**

  + è‡ªå®šä¹‰ç±»ç»§æ‰¿Partitionerï¼Œé‡å†™getPartitionï¼ˆï¼‰æ–¹æ³•

  ~~~java
  public class MyPartition extends Partitioner<Text, FlowBean> {
      
      @Override
      public int getPartition(Text text, FlowBean flowBean, int i) {
          //å®ç°åˆ†åŒºçš„ä¸šåŠ¡ä»£ç 
          return 0;
      }
  }
  ~~~

  + åœ¨Jobé©±åŠ¨ä¸­ï¼Œè®¾ç½®è‡ªå®šä¹‰Partitioner

    ~~~java
            job.setPartitionerClass(MyPartition.class);
    ~~~

  + è‡ªå®šä¹‰Partitionä¹‹åæ ¹æ®è‡ªå®šä¹‰Partitionerçš„é€»è¾‘è®¾ç½®ç›¸åº”æ•°é‡çš„ReduceTask

    ~~~java
         job.setNumReduceTasks(5);   //Partition=RedeceTask
    ~~~





`æ€»ç»“`

+ ReduceTasksæ•°é‡>getPartitionçš„ç»“æœ     åˆ™ä¼šäº§ç”Ÿå‡ ä¸ªç©ºçš„è¾“å‡ºæ–‡ä»¶part-r-000xx
+ 1<ReduceTasksæ•°é‡>getPartitionçš„ç»“æœ     æŠ¥é”™æŠ›å¼‚å¸¸
+ ReduceTasksæ•°é‡=1            åªäº§ç”Ÿä¸€ä¸ªç»“æœæ–‡ä»¶
+ ==åˆ†åŒºå·ä»0å¼€å§‹é€ä¸€ç´¯åŠ ==









> â€‹																Partitionåˆ†åŒºæ¡ˆä¾‹å®æ“



**éœ€æ±‚**

â€‹		æ‰‹æœºå·136ã€137ã€138ã€139å¼€å¤´éƒ½åˆ†åˆ«æ”¾åˆ°ä¸€ä¸ªç‹¬ç«‹çš„4ä¸ªæ–‡ä»¶ä¸­ï¼Œå…¶ä»–å¼€å¤´çš„æ”¾åˆ°ä¸€ä¸ªæ–‡ä»¶ä¸­ã€‚



**åˆ†æ**

â€‹		![image-20220109155103857](../image/image-20220109155103857.png)





**ä¸šåŠ¡ä»£ç **

+ **å¢åŠ åˆ†åŒºç±»**

  ~~~java
  import org.apache.hadoop.io.Text;
  import org.apache.hadoop.mapreduce.Partitioner;
  
  public class ProvincePartitioner extends Partitioner<Text, FlowBean> {
  
      @Override
      public int getPartition(Text text, FlowBean flowBean, int numPartitions) {
          //è·å–æ‰‹æœºå·å‰ä¸‰ä½prePhone
          String phone = text.toString();
          String prePhone = phone.substring(0, 3);
  
          //å®šä¹‰ä¸€ä¸ªåˆ†åŒºå·å˜é‡partition,æ ¹æ®prePhoneè®¾ç½®åˆ†åŒºå·
          int partition;
  
          if("136".equals(prePhone)){
              partition = 0;
          }else if("137".equals(prePhone)){
              partition = 1;
          }else if("138".equals(prePhone)){
              partition = 2;
          }else if("139".equals(prePhone)){
              partition = 3;
          }else {
              partition = 4;
          }
  
          //æœ€åè¿”å›åˆ†åŒºå·partition
          return partition;
      }
  }
  
  ~~~

+ **åœ¨é©±åŠ¨å‡½æ•°ä¸­å¢åŠ è‡ªå®šä¹‰æ•°æ®åˆ†åŒºè®¾ç½®å’ŒReduceTaskè®¾ç½®**

  ~~~java
  import org.apache.hadoop.conf.Configuration;
  import org.apache.hadoop.fs.Path;
  import org.apache.hadoop.io.Text;
  import org.apache.hadoop.mapreduce.Job;
  import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
  import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
  import java.io.IOException;
  
  public class FlowDriver {
  
      public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
  
          //1 è·å–jobå¯¹è±¡
          Configuration conf = new Configuration();
          Job job = Job.getInstance(conf);
  
          //2 å…³è”æœ¬Driverç±»
          job.setJarByClass(FlowDriver.class);
  
          //3 å…³è”Mapperå’ŒReducer
          job.setMapperClass(FlowMapper.class);
          job.setReducerClass(FlowReducer.class);
  
          //4 è®¾ç½®Mapç«¯è¾“å‡ºæ•°æ®çš„KVç±»å‹
          job.setMapOutputKeyClass(Text.class);
          job.setMapOutputValueClass(FlowBean.class);
  
          //5 è®¾ç½®ç¨‹åºæœ€ç»ˆè¾“å‡ºçš„KVç±»å‹
          job.setOutputKeyClass(Text.class);
          job.setOutputValueClass(FlowBean.class);
  
          ==//8 æŒ‡å®šè‡ªå®šä¹‰åˆ†åŒºå™¨
          job.setPartitionerClass(ProvincePartitioner.class);
  
          //9 åŒæ—¶æŒ‡å®šç›¸åº”æ•°é‡çš„ReduceTask
          job.setNumReduceTasks(5);
  
          //6 è®¾ç½®è¾“å…¥è¾“å‡ºè·¯å¾„
          FileInputFormat.setInputPaths(job, new Path("D:\\inputflow"));
          FileOutputFormat.setOutputPath(job, new Path("D\\partitionout"));
  
          //7 æäº¤Job
          boolean b = job.waitForCompletion(true);
          System.exit(b ? 0 : 1);
      }
  }
  
  ~~~

  

#### WritableComparableæ’åº

------

â€‹		shufferé˜¶æ®µçš„ä¸‰æ¬¡æ’åº

						+ ç¯å½¢ç¼“å†²åŒº              å†…å­˜ä¸­å¿«æ’ 
						+ ç£ç›˜ä¸­                     å½’å¹¶æ’åº
						+ ReduceTask            å½’å¹¶æ’åº

> æ¦‚è¿°

![image-20220109160647034](../image/image-20220109160647034.png)

![image-20220109160656666](../image/image-20220109160656666.png)



> æ’åºåˆ†ç±»

![image-20220109160731925](../image/image-20220109160731925.png)



**è‡ªå®šä¹‰æ’åºWritableComparable**

+ å®ç°WritableComparableæ¥å£é‡å†™compareToæ–¹æ³•

  ~~~java
  @Override
  public int compareTo(FlowBean bean) {
  
  	int result;
  		
  	// æŒ‰ç…§æ€»æµé‡å¤§å°ï¼Œå€’åºæ’åˆ—
  	if (this.sumFlow > bean.getSumFlow()) {
  		result = -1;
  	}else if (this.sumFlow < bean.getSumFlow()) {
  		result = 1;
  	}else {
  		result = 0;
  	}
  
  	return result;
  }
  ~~~

  

> â€‹								WritableComparableæ’åºæ¡ˆä¾‹å®æ“ï¼ˆå…¨æ’åºï¼‰



â€‹	**éœ€æ±‚**

â€‹		åºåˆ—åŒ–æ¡ˆä¾‹äº§ç”Ÿçš„ç»“æœå†æ¬¡å¯¹æ€»æµé‡è¿›è¡Œå€’åºæ’åº

â€‹	**éœ€æ±‚åˆ†æ**

![image-20220109161900518](../image/image-20220109161900518.png)



**ä¸šåŠ¡ä»£ç **

~~~java
import org.apache.hadoop.io.WritableComparable;
import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;

public class FlowBean implements WritableComparable<FlowBean> {

    private long upFlow; //ä¸Šè¡Œæµé‡
    private long downFlow; //ä¸‹è¡Œæµé‡
    private long sumFlow; //æ€»æµé‡

    //æä¾›æ— å‚æ„é€ 

    //ç”Ÿæˆä¸‰ä¸ªå±æ€§çš„getterå’Œsetteræ–¹æ³•
  		
    //å®ç°åºåˆ—åŒ–å’Œååºåˆ—åŒ–æ–¹æ³•,æ³¨æ„é¡ºåºä¸€å®šè¦ä¸€è‡´

    //é‡å†™ToString,æœ€åè¦è¾“å‡ºFlowBean

    @Override
    public int compareTo(FlowBean o) {

       //æŒ‰ç…§æ€»æµé‡æ¯”è¾ƒ,å€’åºæ’åˆ—
        if(this.sumFlow > o.sumFlow){
            return -1;
        }else if(this.sumFlow < o.sumFlow){
            return 1;
        }else {
            return 0;
        }
    }
}


	//æ€»æµé‡ç›¸åŒæŒ‰ç…§ä¸Šè¡Œæµé‡ç”±å°åˆ°å¤§
        if (this.sumFlow>o.sumFlow){
            return -1;
        }else if (this.sumFlow<o.sumFlow){
            return 1;
        }else {
            if (this.upFlow>o.upFlow){
                return 1;

            }else if (this.upFlow<o.upFlow){
                return -1;
            }
            else {
                return 0;}

        }
~~~

â€‹	

æ³¨æ„è¾“å‡ºKï¼ŒVä½ç½®

+ ç¼–å†™Mapperç±»

  ~~~java
  import org.apache.hadoop.io.LongWritable;
  import org.apache.hadoop.io.Text;
  import org.apache.hadoop.mapreduce.Mapper;
  import java.io.IOException;
  
  public class FlowMapper extends Mapper<LongWritable, Text, FlowBean, Text> {
      private FlowBean outK = new FlowBean();
      private Text outV = new Text();
  
      @Override
      protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
  
          //1 è·å–ä¸€è¡Œæ•°æ®
          String line = value.toString();
  
          //2 æŒ‰ç…§"\t",åˆ‡å‰²æ•°æ®
          String[] split = line.split("\t");
  
          //3 å°è£…outK outV
          outK.setUpFlow(Long.parseLong(split[1]));
          outK.setDownFlow(Long.parseLong(split[2]));
          outK.setSumFlow();
          outV.set(split[0]);
  
          //4 å†™å‡ºoutK outV
          context.write(outK,outV);
      }
  }
  
  ~~~





**è°ƒæ¢KVä½ç½®,åå‘å†™å‡º**



+ ç¼–å†™Reducerç±»

~~~java
public class FlowReducer extends Reducer<FlowBean, Text, Text, FlowBean> {
    @Override
    protected void reduce(FlowBean key, Iterable<Text> values, Context context) throws IOException, InterruptedException {

        //éå†valuesé›†åˆ,å¾ªç¯å†™å‡º,é¿å…æ€»æµé‡ç›¸åŒçš„æƒ…å†µ
        for (Text value : values) {
            //è°ƒæ¢KVä½ç½®,åå‘å†™å‡º
            context.write(value,key);
        }
    }
}
~~~



+ ç¼–å†™Driverç±»  ==æ³¨æ„ç¬¬å››æ­¥Mapçš„è¾“å‡ºKV==

  ~~~java
   //1 è·å–jobå¯¹è±¡
          Configuration conf = new Configuration();
          Job job = Job.getInstance(conf);
  
          //2 å…³è”æœ¬Driverç±»
          job.setJarByClass(FlowDriver.class);
  
          //3 å…³è”Mapperå’ŒReducer
          job.setMapperClass(FlowMapper.class);
          job.setReducerClass(FlowReducer.class);
  
          //4 è®¾ç½®Mapç«¯è¾“å‡ºæ•°æ®çš„KVç±»å‹  
          job.setMapOutputKeyClass(FlowBean.class);
          job.setMapOutputValueClass(Text.class);
  
          //5 è®¾ç½®ç¨‹åºæœ€ç»ˆè¾“å‡ºçš„KVç±»å‹
          job.setOutputKeyClass(Text.class);
          job.setOutputValueClass(FlowBean.class);
  
          //6 è®¾ç½®è¾“å…¥è¾“å‡ºè·¯å¾„
          FileInputFormat.setInputPaths(job, new Path("D:\\inputflow2"));
          FileOutputFormat.setOutputPath(job, new Path("D:\\comparout"));
  
          //7 æäº¤Job
          boolean b = job.waitForCompletion(true);
          System.exit(b ? 0 : 1);
      }
  }
  ~~~





#### æ’åºï¼‹åˆ†åŒº

------

> â€‹		ğŸ˜¶			WritableComparableæ’åº  + Partitionåˆ†åŒº                            æ¡ˆä¾‹å®æ“
>



**1**ï¼‰éœ€æ±‚

â€‹		è¦æ±‚æ¯ä¸ªçœä»½æ‰‹æœºå·è¾“å‡ºçš„æ–‡ä»¶ä¸­æŒ‰ç…§æ€»æµé‡å†…éƒ¨æ’åºã€‚



**2**ï¼‰éœ€æ±‚åˆ†æ

â€‹    	åŸºäºå‰ä¸€ä¸ªéœ€æ±‚ï¼Œå¢åŠ è‡ªå®šä¹‰åˆ†åŒºç±»ï¼Œåˆ†åŒºæŒ‰ç…§çœä»½æ‰‹æœºå·è®¾ç½®ã€‚



![image-20220109163406508](../image/image-20220109163406508.png)



**3**)ä¸šåŠ¡ä»£ç 

  + å¢åŠ è‡ªå®šä¹‰åˆ†åŒºç±»

    + ~~~java
      import org.apache.hadoop.io.Text;
      import org.apache.hadoop.mapreduce.Partitioner;
      
      public class ProvincePartitioner2 extends Partitioner<FlowBean, Text> {
      
          @Override
          public int getPartition(FlowBean flowBean, Text text, int numPartitions) {
              //è·å–æ‰‹æœºå·å‰ä¸‰ä½
              String phone = text.toString();
              String prePhone = phone.substring(0, 3);
      
              //å®šä¹‰ä¸€ä¸ªåˆ†åŒºå·å˜é‡partition,æ ¹æ®prePhoneè®¾ç½®åˆ†åŒºå·
              int partition;
              if("136".equals(prePhone)){
                  partition = 0;
              }else if("137".equals(prePhone)){
                  partition = 1;
              }else if("138".equals(prePhone)){
                  partition = 2;
              }else if("139".equals(prePhone)){
                  partition = 3;
              }else {
                  partition = 4;
              }
      
              //æœ€åè¿”å›åˆ†åŒºå·partition
              return partition;
          }
      }
      ~~~



**ä¿®æ”¹4  æ·»åŠ 8   9**

  + åœ¨é©±åŠ¨ç±»ä¸­æ·»åŠ åˆ†åŒºç±»

    + ~~~java
              //1 è·å–jobå¯¹è±¡
              Configuration conf = new Configuration();
              Job job = Job.getInstance(conf);
            
              //2 å…³è”æœ¬Driverç±»
              job.setJarByClass(FlowDriver.class);
            
              //3 å…³è”Mapperå’ŒReducer
              job.setMapperClass(FlowMapper.class);
              job.setReducerClass(FlowReducer.class);
            
              //4 è®¾ç½®Mapç«¯è¾“å‡ºæ•°æ®çš„KVç±»å‹
              job.setMapOutputKeyClass(FlowBean.class);
              job.setMapOutputValueClass(Text.class);
            
              //5 è®¾ç½®ç¨‹åºæœ€ç»ˆè¾“å‡ºçš„KVç±»å‹
              job.setOutputKeyClass(Text.class);
              job.setOutputValueClass(FlowBean.class);
            
              //8 æŒ‡å®šè‡ªå®šä¹‰åˆ†åŒºå™¨
              job.setPartitionerClass(ProvincePartitioner.class);
            
              //9 åŒæ—¶æŒ‡å®šç›¸åº”æ•°é‡çš„ReduceTask
              job.setNumReduceTasks(5);
            
              //6 è®¾ç½®è¾“å…¥è¾“å‡ºè·¯å¾„
              FileInputFormat.setInputPaths(job, new Path("D:\\inputflow"));
              FileOutputFormat.setOutputPath(job, new Path("D\\partitionout"));
            
              //7 æäº¤Job
              boolean b = job.waitForCompletion(true);
              System.exit(b ? 0 : 1);
          }
      ~~~





#### Combineråˆå¹¶

â€‹				**Combineræ˜¯ç”¨æ¥åˆ†æ‹…ReduceTaskçš„ä»»åŠ¡é‡çš„,å…¶åœ¨MapTaskç«¯æ‰§è¡Œéƒ¨åˆ†ä¸šåŠ¡**

![image-20220109164338012](../image/image-20220109164338012.png)





è‡ªå®šä¹‰Combinerå®ç°æ­¥éª¤



 + è‡ªå®šä¹‰ä¸€ä¸ªCombinerç»§æ‰¿Reducerï¼Œé‡å†™Reduceæ–¹æ³•

   + ~~~java
     public class WordCountCombiner extends Reducer<Text, IntWritable, Text, IntWritable> {
     
         private IntWritable outV = new IntWritable();
     
         @Override
         protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
     
             int sum = 0;
             for (IntWritable value : values) {
                 sum += value.get();
             }
          
             outV.set(sum);
          
             context.write(key,outV);
         }
     }
     
     ~~~





 + åœ¨Jobé©±åŠ¨ç±»ä¸­è®¾ç½®

   + ~~~java
     job.setCombinerClass(WordCountCombiner.class);
     ~~~

   

> â€‹																Combineråˆå¹¶æ¡ˆä¾‹å®æ“



**1**ï¼‰éœ€æ±‚

ç»Ÿè®¡è¿‡ç¨‹ä¸­å¯¹æ¯ä¸€ä¸ªMapTaskçš„è¾“å‡ºè¿›è¡Œå±€éƒ¨æ±‡æ€»ï¼Œä»¥å‡å°ç½‘ç»œä¼ è¾“é‡å³é‡‡ç”¨CombineråŠŸèƒ½ã€‚  



2)éœ€æ±‚åˆ†æ

![image-20220109164745975](../image/image-20220109164745975.png)



**æ–¹æ¡ˆä¸€**

~~~java
                 // 1ï¼‰å¢åŠ ä¸€ä¸ªWordCountCombinerç±»ç»§æ‰¿Reducer
package com.atguigu.mapreduce.combiner;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;
import java.io.IOException;

public class WordCountCombiner extends Reducer<Text, IntWritable, Text, IntWritable> {

private IntWritable outV = new IntWritable();

    @Override
    protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {

        int sum = 0;
        for (IntWritable value : values) {
            sum += value.get();
        }

        //å°è£…outKV
        outV.set(sum);

        //å†™å‡ºoutKV
        context.write(key,outV);
    }
}

		//ï¼ˆ2ï¼‰åœ¨WordcountDriveré©±åŠ¨ç±»ä¸­æŒ‡å®šCombiner

			// æŒ‡å®šéœ€è¦ä½¿ç”¨combinerï¼Œä»¥åŠç”¨å“ªä¸ªç±»ä½œä¸ºcombinerçš„é€»è¾‘
	job.setCombinerClass(WordCountCombiner.class);

~~~





**æ–¹æ¡ˆäºŒ**

â€‹			å°†WordcountReducerä½œä¸ºCombineråœ¨WordcountDriveré©±åŠ¨ç±»ä¸­æŒ‡å®š

```java
// æŒ‡å®šéœ€è¦ä½¿ç”¨Combinerï¼Œä»¥åŠç”¨å“ªä¸ªç±»ä½œä¸ºCombinerçš„é€»è¾‘
job.setCombinerClass(WordCountReducer.class);
```





### 4)OutputFormatæ•°æ®è¾“å‡º

------



+ ä»‹ç»

![image-20220110144457594](../image/image-20220110144457594.png)





> è‡ªå®šä¹‰OutputFormatæ¡ˆä¾‹å®æ“



+ éœ€æ±‚

  å«atguiguçš„ç½‘ç«™è¾“å‡ºåˆ°atguigu.logï¼Œä¸åŒ…å«atguiguçš„ç½‘ç«™è¾“å‡ºåˆ°other.logã€‚

  

 + åˆ†æ

   ![image-20220110144721856](../image/image-20220110144721856.png)

   



ä¸šåŠ¡ä»£ç 

+ ç¼–å†™LogMapperç±»

  + ~~~java
    public class LogMapper extends Mapper<LongWritable, Text,Text, NullWritable> {
        @Override
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            //ä¸åšä»»ä½•å¤„ç†,ç›´æ¥å†™å‡ºä¸€è¡Œlogæ•°æ®
            context.write(value,NullWritable.get());
        }
    }
    
    ~~~

+ ç¼–å†™LogReducerç±»

  + ~~~java
    public class LogReducer extends Reducer<Text, NullWritable,Text, NullWritable> {
        @Override
        protected void reduce(Text key, Iterable<NullWritable> values, Context context) throws IOException, InterruptedException {
            // é˜²æ­¢æœ‰ç›¸åŒçš„æ•°æ®,è¿­ä»£å†™å‡º
            for (NullWritable value : values) {
                context.write(key,NullWritable.get());
            }
        }
    }
    
    ~~~

+ è‡ªå®šä¹‰ä¸€ä¸ªLogOutputFormatç±»

  + ~~~java
    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.RecordWriter;
    import org.apache.hadoop.mapreduce.TaskAttemptContext;
    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
    
    import java.io.IOException;
    
    public class LogOutputFormat extends FileOutputFormat<Text, NullWritable> {
        @Override
        public RecordWriter<Text, NullWritable> getRecordWriter(TaskAttemptContext job) throws IOException, InterruptedException {
            //åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰çš„RecordWriterè¿”å›
            LogRecordWriter logRecordWriter = new LogRecordWriter(job);
            return logRecordWriter;
        }
    }
    ~~~

+ ç¼–å†™LogRecordWriterç±»

  + ~~~java
    public class LogRecordWriter extends RecordWriter<Text, NullWritable> {
    
        private FSDataOutputStream atguiguOut;
        private FSDataOutputStream otherOut;
    
        public LogRecordWriter(TaskAttemptContext job) {
            try {
                //è·å–æ–‡ä»¶ç³»ç»Ÿå¯¹è±¡
                FileSystem fs = FileSystem.get(job.getConfiguration());
                //ç”¨æ–‡ä»¶ç³»ç»Ÿå¯¹è±¡åˆ›å»ºä¸¤ä¸ªè¾“å‡ºæµå¯¹åº”ä¸åŒçš„ç›®å½•
                atguiguOut = fs.create(new Path("d:/hadoop/atguigu.log"));
                otherOut = fs.create(new Path("d:/hadoop/other.log"));
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
    
        @Override
        public void write(Text key, NullWritable value) throws IOException, InterruptedException {
            String log = key.toString();
            //æ ¹æ®ä¸€è¡Œçš„logæ•°æ®æ˜¯å¦åŒ…å«atguigu,åˆ¤æ–­ä¸¤æ¡è¾“å‡ºæµè¾“å‡ºçš„å†…å®¹
            if (log.contains("atguigu")) {
                atguiguOut.writeBytes(log + "\n");
            } else {
                otherOut.writeBytes(log + "\n");
            }
        }
    
        @Override
        public void close(TaskAttemptContext context) throws IOException, InterruptedException {
            //å…³æµ
            IOUtils.closeStream(atguiguOut);
            IOUtils.closeStream(otherOut);
        }
    }
    
    ~~~

+ ç¼–å†™LogDriverç±»

  + ~~~java
    
    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Job;
    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
    
    import java.io.IOException;
    
    public class LogDriver {
        public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
    
            Configuration conf = new Configuration();
            Job job = Job.getInstance(conf);
    
            job.setJarByClass(LogDriver.class);
            job.setMapperClass(LogMapper.class);
            job.setReducerClass(LogReducer.class);
    
            job.setMapOutputKeyClass(Text.class);
            job.setMapOutputValueClass(NullWritable.class);
    
            job.setOutputKeyClass(Text.class);
            job.setOutputValueClass(NullWritable.class);
    
            //è®¾ç½®è‡ªå®šä¹‰çš„outputformat
            job.setOutputFormatClass(LogOutputFormat.class);
    
            FileInputFormat.setInputPaths(job, new Path("D:\\input"));
            //è™½ç„¶æˆ‘ä»¬è‡ªå®šä¹‰äº†outputformatï¼Œä½†æ˜¯å› ä¸ºæˆ‘ä»¬çš„outputformatç»§æ‰¿è‡ª						fileoutputformat
            //è€Œfileoutputformatè¦è¾“å‡ºä¸€ä¸ª_SUCCESSæ–‡ä»¶ï¼Œæ‰€ä»¥åœ¨è¿™è¿˜å¾—æŒ‡å®šä¸€ä¸ªè¾“å‡ºç›®å½•
            FileOutputFormat.setOutputPath(job, new Path("D:\\logoutput"));
    
            boolean b = job.waitForCompletion(true);
            System.exit(b ? 0 : 1);
        }
    }
    ~~~

  + 



### 5ï¼‰MapReduceå†…æ ¸æºç è§£æ

------



#### MapTask

------

![image-20220110151921593](../image/image-20220110151921593.png)





â€‹	ï¼ˆ1ï¼‰`Readé˜¶æ®µ`ï¼šMapTaské€šè¿‡InputFormatè·å¾—çš„RecordReaderï¼Œä»è¾“å…¥InputSplitä¸­**è§£æå‡ºä¸€ä¸ªä¸ªkey/valueã€‚**

â€‹    ï¼ˆ2ï¼‰`Mapé˜¶æ®µ`ï¼šè¯¥èŠ‚ç‚¹ä¸»è¦æ˜¯å°†è§£æå‡ºçš„key/value**äº¤ç»™ç”¨æˆ·ç¼–å†™map()å‡½æ•°å¤„ç†**ï¼Œå¹¶äº§ç”Ÿä¸€ç³»åˆ—æ–°çš„key/valueã€‚

â€‹    ï¼ˆ3ï¼‰`Collectæ”¶é›†é˜¶æ®µ`ï¼šåœ¨ç”¨æˆ·ç¼–å†™map()å‡½æ•°ä¸­ï¼Œå½“æ•°æ®å¤„ç†å®Œæˆåï¼Œä¸€èˆ¬ä¼šè°ƒç”¨OutputCollector.collect()è¾“å‡ºç»“æœã€‚åœ¨è¯¥å‡½æ•°å†…éƒ¨ï¼Œå®ƒä¼šå°†ç”Ÿæˆçš„key/valueåˆ†åŒºï¼ˆè°ƒç”¨Partitionerï¼‰ï¼Œå¹¶**å†™å…¥ä¸€ä¸ªç¯å½¢å†…å­˜ç¼“å†²åŒº**ä¸­ã€‚

â€‹    ï¼ˆ4ï¼‰`Spillé˜¶æ®µ`ï¼šå³â€œæº¢å†™â€ï¼Œå½“ç¯å½¢ç¼“å†²åŒºæ»¡åï¼Œ**MapReduceä¼šå°†æ•°æ®å†™åˆ°æœ¬åœ°ç£ç›˜ä¸Š**ï¼Œç”Ÿæˆä¸€ä¸ªä¸´æ—¶æ–‡ä»¶ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå°†æ•°æ®å†™å…¥æœ¬åœ°ç£ç›˜ä¹‹å‰ï¼Œ**å…ˆè¦å¯¹æ•°æ®è¿›è¡Œä¸€æ¬¡æœ¬åœ°æ’åº**ï¼Œå¹¶åœ¨å¿…è¦æ—¶å¯¹æ•°æ®è¿›è¡Œåˆå¹¶ã€å‹ç¼©ç­‰æ“ä½œã€‚

â€‹    æº¢å†™é˜¶æ®µè¯¦æƒ…ï¼š

â€‹    æ­¥éª¤1ï¼šåˆ©ç”¨å¿«é€Ÿæ’åºç®—æ³•å¯¹ç¼“å­˜åŒºå†…çš„æ•°æ®è¿›è¡Œæ’åºï¼Œæ’åºæ–¹å¼æ˜¯ï¼Œå…ˆæŒ‰ç…§åˆ†åŒºç¼–å·Partitionè¿›è¡Œæ’åºï¼Œç„¶åæŒ‰ç…§keyè¿›è¡Œæ’åºã€‚è¿™æ ·ï¼Œç»è¿‡æ’åºåï¼Œæ•°æ®ä»¥åˆ†åŒºä¸ºå•ä½èšé›†åœ¨ä¸€èµ·ï¼Œä¸”åŒä¸€åˆ†åŒºå†…æ‰€æœ‰æ•°æ®æŒ‰ç…§keyæœ‰åºã€‚

â€‹    æ­¥éª¤2ï¼šæŒ‰ç…§åˆ†åŒºç¼–å·ç”±å°åˆ°å¤§ä¾æ¬¡å°†æ¯ä¸ªåˆ†åŒºä¸­çš„æ•°æ®å†™å…¥ä»»åŠ¡å·¥ä½œç›®å½•ä¸‹çš„ä¸´æ—¶æ–‡ä»¶output/spillN.outï¼ˆNè¡¨ç¤ºå½“å‰æº¢å†™æ¬¡æ•°ï¼‰ä¸­ã€‚å¦‚æœç”¨æˆ·è®¾ç½®äº†Combinerï¼Œåˆ™å†™å…¥æ–‡ä»¶ä¹‹å‰ï¼Œå¯¹æ¯ä¸ªåˆ†åŒºä¸­çš„æ•°æ®è¿›è¡Œä¸€æ¬¡èšé›†æ“ä½œã€‚

â€‹    æ­¥éª¤3ï¼šå°†åˆ†åŒºæ•°æ®çš„å…ƒä¿¡æ¯å†™åˆ°å†…å­˜ç´¢å¼•æ•°æ®ç»“æ„SpillRecordä¸­ï¼Œå…¶ä¸­æ¯ä¸ªåˆ†åŒºçš„å…ƒä¿¡æ¯åŒ…æ‹¬åœ¨ä¸´æ—¶æ–‡ä»¶ä¸­çš„åç§»é‡ã€å‹ç¼©å‰æ•°æ®å¤§å°å’Œå‹ç¼©åæ•°æ®å¤§å°ã€‚å¦‚æœå½“å‰å†…å­˜ç´¢å¼•å¤§å°è¶…è¿‡1MBï¼Œåˆ™å°†å†…å­˜ç´¢å¼•å†™åˆ°æ–‡ä»¶output/spillN.out.indexä¸­ã€‚

â€‹    ï¼ˆ5ï¼‰`Mergeé˜¶æ®µ`ï¼šå½“æ‰€æœ‰æ•°æ®å¤„ç†å®Œæˆåï¼ŒMapTaskå¯¹**æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶è¿›è¡Œä¸€æ¬¡åˆå¹¶**ï¼Œä»¥ç¡®ä¿æœ€ç»ˆåªä¼šç”Ÿæˆä¸€ä¸ªæ•°æ®æ–‡ä»¶ã€‚

â€‹    å½“æ‰€æœ‰æ•°æ®å¤„ç†å®Œåï¼ŒMapTaskä¼šå°†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶åˆå¹¶æˆä¸€ä¸ªå¤§æ–‡ä»¶ï¼Œå¹¶ä¿å­˜åˆ°æ–‡ä»¶output/file.outä¸­ï¼Œ**åŒæ—¶ç”Ÿæˆç›¸åº”çš„ç´¢å¼•æ–‡ä»¶**output/file.out.indexã€‚

â€‹    åœ¨è¿›è¡Œæ–‡ä»¶åˆå¹¶è¿‡ç¨‹ä¸­ï¼ŒMapTaskä»¥åˆ†åŒºä¸ºå•ä½è¿›è¡Œåˆå¹¶ã€‚å¯¹äºæŸä¸ªåˆ†åŒºï¼Œå®ƒå°†é‡‡ç”¨å¤šè½®é€’å½’åˆå¹¶çš„æ–¹å¼ã€‚æ¯è½®åˆå¹¶mapreduce.task.io.sort.factorï¼ˆé»˜è®¤10ï¼‰ä¸ªæ–‡ä»¶ï¼Œå¹¶å°†äº§ç”Ÿçš„æ–‡ä»¶é‡æ–°åŠ å…¥å¾…åˆå¹¶åˆ—è¡¨ä¸­ï¼Œå¯¹æ–‡ä»¶æ’åºåï¼Œé‡å¤ä»¥ä¸Šè¿‡ç¨‹ï¼Œç›´åˆ°æœ€ç»ˆå¾—åˆ°ä¸€ä¸ªå¤§æ–‡ä»¶ã€‚

â€‹    **è®©æ¯ä¸ªMapTaskæœ€ç»ˆåªç”Ÿæˆä¸€ä¸ªæ•°æ®æ–‡ä»¶ï¼Œå¯é¿å…åŒæ—¶æ‰“å¼€å¤§é‡æ–‡ä»¶å’ŒåŒæ—¶è¯»å–å¤§é‡å°æ–‡ä»¶äº§ç”Ÿçš„éšæœºè¯»å–å¸¦æ¥çš„å¼€é”€ã€‚**



> æºç è§£æ
>

~~~java
=================== MapTask ===================
context.write(k, NullWritable.get());   //è‡ªå®šä¹‰çš„mapæ–¹æ³•çš„å†™å‡ºï¼Œè¿›å…¥
output.write(key, value);  

	//MapTask727è¡Œï¼Œæ”¶é›†æ–¹æ³•ï¼Œè¿›å…¥ä¸¤æ¬¡ 
collector.collect(key, value,partitioner.getPartition(key, value, partitions));
	HashPartitioner(); //é»˜è®¤åˆ†åŒºå™¨
collect()  //MapTask1082è¡Œ mapç«¯æ‰€æœ‰çš„kvå…¨éƒ¨å†™å‡ºåä¼šèµ°ä¸‹é¢çš„closeæ–¹æ³•
	close() //MapTask732è¡Œ
	collector.flush() // æº¢å‡ºåˆ·å†™æ–¹æ³•ï¼ŒMapTask735è¡Œï¼Œæå‰æ‰“ä¸ªæ–­ç‚¹ï¼Œè¿›å…¥
sortAndSpill() //æº¢å†™æ’åºï¼ŒMapTask1505è¡Œï¼Œè¿›å…¥
	sorter.sort()   QuickSort //æº¢å†™æ’åºæ–¹æ³•ï¼ŒMapTask1625è¡Œï¼Œè¿›å…¥
mergeParts(); //åˆå¹¶æ–‡ä»¶ï¼ŒMapTask1527è¡Œï¼Œè¿›å…¥
	 
collector.close(); //MapTask739è¡Œ,æ”¶é›†å™¨å…³é—­,å³å°†è¿›å…¥ReduceTask
~~~





#### ReduceTask

------



![image-20220110152942852](../image/image-20220110152942852.png)



â€‹	ï¼ˆ1ï¼‰`Copyé˜¶æ®µ`ï¼šReduceTask**ä»å„ä¸ªMapTaskä¸Šè¿œç¨‹æ‹·è´ä¸€ç‰‡æ•°æ®**ï¼Œå¹¶é’ˆå¯¹æŸä¸€ç‰‡æ•°æ®ï¼Œ**å¦‚æœå…¶å¤§å°è¶…è¿‡ä¸€å®šé˜ˆå€¼ï¼Œåˆ™å†™åˆ°ç£ç›˜ä¸Šï¼Œå¦åˆ™ç›´æ¥æ”¾åˆ°å†…å­˜ä¸­ã€‚**

â€‹    ï¼ˆ2ï¼‰`Sorté˜¶æ®µ`ï¼šåœ¨è¿œç¨‹æ‹·è´æ•°æ®çš„åŒæ—¶ï¼ŒReduceTaskå¯åŠ¨äº†ä¸¤ä¸ªåå°çº¿ç¨‹**å¯¹å†…å­˜å’Œç£ç›˜ä¸Šçš„æ–‡ä»¶è¿›è¡Œåˆå¹¶**ï¼Œä»¥é˜²æ­¢å†…å­˜ä½¿ç”¨è¿‡å¤šæˆ–ç£ç›˜ä¸Šæ–‡ä»¶è¿‡å¤šã€‚æŒ‰ç…§MapReduceè¯­ä¹‰ï¼Œç”¨æˆ·ç¼–å†™reduce()å‡½æ•°è¾“å…¥æ•°æ®æ˜¯æŒ‰keyè¿›è¡Œèšé›†çš„ä¸€ç»„æ•°æ®ã€‚ä¸ºäº†å°†keyç›¸åŒçš„æ•°æ®èšåœ¨ä¸€èµ·ï¼ŒHadoopé‡‡ç”¨äº†åŸºäºæ’åºçš„ç­–ç•¥ã€‚ç”±äºå„ä¸ªMapTaskå·²ç»å®ç°å¯¹è‡ªå·±çš„å¤„ç†ç»“æœè¿›è¡Œäº†å±€éƒ¨æ’åºï¼Œå› æ­¤ï¼Œ**ReduceTaskåªéœ€å¯¹æ‰€æœ‰æ•°æ®è¿›è¡Œä¸€æ¬¡å½’å¹¶æ’åºå³å¯**ã€‚

â€‹    ï¼ˆ3ï¼‰`Reduceé˜¶æ®µ`ï¼šreduce()å‡½æ•°å°†è®¡ç®—ç»“æœå†™åˆ°HDFSä¸Šã€‚



> æºç è§£æ
>



~~~java
=================== ReduceTask ===================
if (isMapOrReduce())  //reduceTask324è¡Œï¼Œæå‰æ‰“æ–­ç‚¹
initialize()   // reduceTask333è¡Œ,è¿›å…¥
init(shuffleContext);  // reduceTask375è¡Œ,èµ°åˆ°è¿™éœ€è¦å…ˆç»™ä¸‹é¢çš„æ‰“æ–­ç‚¹
        totalMaps = job.getNumMapTasks(); // ShuffleSchedulerImplç¬¬120è¡Œï¼Œæå‰æ‰“æ–­ç‚¹
         merger = createMergeManager(context); //åˆå¹¶æ–¹æ³•ï¼ŒShuffleç¬¬80è¡Œ
			// MergeManagerImplç¬¬232 235è¡Œï¼Œæå‰æ‰“æ–­ç‚¹
			this.inMemoryMerger = createInMemoryMerger(); //å†…å­˜åˆå¹¶
			this.onDiskMerger = new OnDiskMerger(this); //ç£ç›˜åˆå¹¶
rIter = shuffleConsumerPlugin.run();
		eventFetcher.start();  //å¼€å§‹æŠ“å–æ•°æ®ï¼ŒShuffleç¬¬107è¡Œï¼Œæå‰æ‰“æ–­ç‚¹
		eventFetcher.shutDown();  //æŠ“å–ç»“æŸï¼ŒShuffleç¬¬141è¡Œï¼Œæå‰æ‰“æ–­ç‚¹
		copyPhase.complete();   //copyé˜¶æ®µå®Œæˆï¼ŒShuffleç¬¬151è¡Œ
		taskStatus.setPhase(TaskStatus.Phase.SORT);  //å¼€å§‹æ’åºé˜¶æ®µï¼ŒShuffleç¬¬152è¡Œ
	sortPhase.complete();   //æ’åºé˜¶æ®µå®Œæˆï¼Œå³å°†è¿›å…¥reduceé˜¶æ®µ reduceTask382è¡Œ
reduce();  //reduceé˜¶æ®µè°ƒç”¨çš„å°±æ˜¯æˆ‘ä»¬è‡ªå®šä¹‰çš„reduceæ–¹æ³•ï¼Œä¼šè¢«è°ƒç”¨å¤šæ¬¡
	cleanup(context); //reduceå®Œæˆä¹‹å‰ï¼Œä¼šæœ€åè°ƒç”¨ä¸€æ¬¡Reduceré‡Œé¢çš„cleanupæ–¹æ³•

~~~



#### **ReduceTaskå¹¶è¡Œåº¦ï¼ˆä¸ªæ•°ï¼‰**

------

ğŸ³â€ğŸŒˆ MapTaskå¹¶è¡Œåº¦ç”±åˆ‡ç‰‡ä¸ªæ•°å†³å®šï¼Œåˆ‡ç‰‡ä¸ªæ•°ç”±è¾“å…¥æ–‡ä»¶å’Œåˆ‡ç‰‡è§„åˆ™å†³å®šã€‚





ğŸ¤”	ReduceTaskå¹¶è¡Œåº¦ç”±è°å†³å®šï¼Ÿ

+ **è®¾ç½®ReduceTaskå¹¶è¡Œåº¦ï¼ˆä¸ªæ•°ï¼‰**

  + ~~~java
    // é»˜è®¤å€¼æ˜¯1ï¼Œæ‰‹åŠ¨è®¾ç½®ä¸º4
    job.setNumReduceTasks(4);
    ~~~





ğŸ‘‰**æµ‹è¯•ReduceTaskå¤šå°‘åˆé€‚**

â€‹																								ğŸ‘‡

![image-20220110154746123](../image/image-20220110154746123.png)

â€‹																								

`ReduceTaskçš„å¼€å¯æ•°é‡`ç”±         æœåŠ¡å™¨æ•°é‡ï¼Œç½‘ç»œç¯å¢ƒï¼Œä»»åŠ¡æ•°æ®é‡å¤§å°   è€Œå®šã€‚





==æ³¨æ„äº‹é¡¹==



![image-20220110155203566](../image/image-20220110155203566.png)





Hadoopè¿­ä»£å™¨ä¸­ä½¿ç”¨äº†å¯¹è±¡é‡ç”¨å³è¿­ä»£æ—¶valueå§‹ç»ˆæŒ‡å‘ä¸€ä¸ªå†…å­˜åœ°å€ï¼ˆå¼•ç”¨å€¼å§‹ç»ˆä¸å˜ï¼‰æ”¹å˜çš„æ˜¯å¼•ç”¨æŒ‡å‘çš„å†…å­˜åœ°å€ä¸­çš„æ•°æ®



### 6ï¼‰Joinåº”ç”¨

------



â€‹		`Mapç«¯çš„ä¸»è¦å·¥ä½œ`ï¼šä¸ºæ¥è‡ªä¸åŒè¡¨æˆ–æ–‡ä»¶çš„key/valueå¯¹ï¼Œ**æ‰“æ ‡ç­¾ä»¥åŒºåˆ«ä¸åŒæ¥æºçš„è®°å½•ã€‚ç„¶åç”¨è¿æ¥å­—æ®µä½œä¸ºkey**ï¼Œå…¶ä½™éƒ¨åˆ†å’Œæ–°åŠ çš„æ ‡å¿—ä½œä¸ºvalueï¼Œæœ€åè¿›è¡Œè¾“å‡ºã€‚



  	`Reduceç«¯çš„ä¸»è¦å·¥ä½œ`ï¼šåœ¨Reduceç«¯ä»¥è¿æ¥å­—æ®µä½œä¸ºkeyçš„åˆ†ç»„å·²ç»å®Œæˆï¼Œæˆ‘ä»¬åªéœ€è¦åœ¨æ¯ä¸€ä¸ªåˆ†ç»„å½“ä¸­å°†é‚£äº›æ¥æºäºä¸åŒæ–‡ä»¶çš„è®°å½•ï¼ˆåœ¨Mapé˜¶æ®µå·²ç»æ‰“æ ‡å¿—ï¼‰**åˆ†å¼€**ï¼Œæœ€åè¿›è¡Œ**åˆå¹¶**å°±okäº†ã€‚



#### Reduce Join



> Reduce Joinæ¡ˆä¾‹å®æ“



éœ€æ±‚ï¼š

â€‹			å°†å•†å“ä¿¡æ¯è¡¨ä¸­æ•°æ®æ ¹æ®**å•†å“pid**åˆå¹¶åˆ°è®¢å•æ•°æ®è¡¨ä¸­ã€‚

æœ€ç»ˆæ•°æ®å½¢å¼ğŸ‘‡

| id   | pname | amount |
| ---- | ----- | ------ |
| 1001 | å°ç±³  | 1      |
| 1004 | å°ç±³  | 4      |
| 1002 | åä¸º  | 2      |
| 1005 | åä¸º  | 5      |
| 1003 | æ ¼åŠ›  | 3      |
| 1006 | æ ¼åŠ›  | 6      |

è®¢å•æ•°æ®è¡¨

ğŸ‘‡

| id   | pid  | amount |
| ---- | ---- | ------ |
| 1001 | 01   | 1      |
| 1002 | 02   | 2      |
| 1003 | 03   | 3      |
| 1004 | 01   | 4      |
| 1005 | 02   | 5      |
| 1006 | 03   | 6      |



å•†å“ä¿¡æ¯è¡¨

ğŸ‘‡

| pid  | pname |
| ---- | ----- |
| 01   | å°ç±³  |
| 02   | åä¸º  |
| 03   | æ ¼åŠ›  |



**éœ€æ±‚åˆ†æ**

![image-20220110201218944](../image/image-20220110201218944.png)



ä»£ç å®ç°

  + åˆ›å»ºå•†å“å’Œè®¢å•åˆå¹¶åçš„TableBeanç±»

    + ~~~java
      public class TableBean implements Writable {
      
          private String id; //è®¢å•id
          private String pid; //äº§å“id
          private int amount; //äº§å“æ•°é‡
          private String pname; //äº§å“åç§°
          private String flag; //åˆ¤æ–­æ˜¯orderè¡¨è¿˜æ˜¯pdè¡¨çš„æ ‡å¿—å­—æ®µ
      
          public TableBean() {
          }
      		//getï¼ˆï¼‰  setï¼ˆï¼‰
      
          @Override
          public String toString() {
              return id + "\t" + pname + "\t" + amount;
          }
      
          @Override
          public void write(DataOutput out) throws IOException {
              out.writeUTF(id);
              out.writeUTF(pid);
              out.writeInt(amount);
              out.writeUTF(pname);
              out.writeUTF(flag);
          }
      
          @Override
          public void readFields(DataInput in) throws IOException {
              this.id = in.readUTF();
              this.pid = in.readUTF();
              this.amount = in.readInt();
              this.pname = in.readUTF();
              this.flag = in.readUTF();
          }
      }
      
      ~~~

  + ç¼–å†™TableMapperç±»

    + ~~~java
      public class TableMapper extends Mapper<LongWritable,Text,Text,TableBean> {
      
          private String filename;
          private Text outK = new Text();
          private TableBean outV = new TableBean();
      
          @Override
          protected void setup(Context context) throws IOException, InterruptedException {
              //è·å–å¯¹åº”æ–‡ä»¶åç§°
              InputSplit split = context.getInputSplit();
              FileSplit fileSplit = (FileSplit) split;
              filename = fileSplit.getPath().getName();
          }
      
          @Override
          protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
      
              //è·å–ä¸€è¡Œ
              String line = value.toString();
      
              //åˆ¤æ–­æ˜¯å“ªä¸ªæ–‡ä»¶,ç„¶åé’ˆå¯¹æ–‡ä»¶è¿›è¡Œä¸åŒçš„æ“ä½œ
              if(filename.contains("order")){  //è®¢å•è¡¨çš„å¤„ç†
                  String[] split = line.split("\t");
                  //å°è£…outK
                  outK.set(split[1]);
                  //å°è£…outV
                  outV.setId(split[0]);
                  outV.setPid(split[1]);
                  outV.setAmount(Integer.parseInt(split[2]));
                  outV.setPname("");
                  outV.setFlag("order");
              }else {                             //å•†å“è¡¨çš„å¤„ç†
                  String[] split = line.split("\t");
                  //å°è£…outK
                  outK.set(split[0]);
                  //å°è£…outV
                  outV.setId("");
                  outV.setPid(split[0]);
                  outV.setAmount(0);
                  outV.setPname(split[1]);
                  outV.setFlag("pd");
              }
      
              //å†™å‡ºKV
              context.write(outK,outV);
          }
      }
      ~~~

  + ç¼–å†™TableReducerç±»

  + > ğŸš©Hadoopè¿­ä»£å™¨ä¸­ä½¿ç”¨äº†å¯¹è±¡é‡ç”¨å³**è¿­ä»£æ—¶valueå§‹ç»ˆæŒ‡å‘ä¸€ä¸ªå†…å­˜åœ°å€ï¼ˆå¼•ç”¨å€¼å§‹ç»ˆä¸å˜ï¼‰**æ”¹å˜çš„æ˜¯å¼•ç”¨æŒ‡å‘çš„å†…å­˜åœ°å€ä¸­çš„æ•°æ®
    >

    + ~~~java
      public class TableReducer extends Reducer<Text,TableBean,TableBean, NullWritable> {
      
          @Override
          protected void reduce(Text key, Iterable<TableBean> values, Context context) throws IOException, InterruptedException {
      
              ArrayList<TableBean> orderBeans = new ArrayList<>();
              TableBean pdBean = new TableBean();
      
              for (TableBean value : values) {
      
                  //åˆ¤æ–­æ•°æ®æ¥è‡ªå“ªä¸ªè¡¨
                  if("order".equals(value.getFlag())){   //è®¢å•è¡¨
      
      			  //ğŸš©åˆ›å»ºä¸€ä¸ªä¸´æ—¶TableBeanå¯¹è±¡æ¥æ”¶value
                      TableBean tmpOrderBean = new TableBean();
      
                      try {
                          BeanUtils.copyProperties(tmpOrderBean,value);
                      } catch (IllegalAccessException e) {
                          e.printStackTrace();
                      } catch (InvocationTargetException e) {
                          e.printStackTrace();
                      }
      
      			  //å°†ä¸´æ—¶TableBeanå¯¹è±¡æ·»åŠ åˆ°é›†åˆorderBeans
                      orderBeans.add(tmpOrderBean);
                  }else {                                    //å•†å“è¡¨
                      try {
                          BeanUtils.copyProperties(pdBean,value);
                      } catch (IllegalAccessException e) {
                          e.printStackTrace();
                      } catch (InvocationTargetException e) {
                          e.printStackTrace();
                      }
                  }
              }
      
              //éå†é›†åˆorderBeans,æ›¿æ¢æ‰æ¯ä¸ªorderBeançš„pidä¸ºpname,ç„¶åå†™å‡º
              for (TableBean orderBean : orderBeans) {
      
                  orderBean.setPname(pdBean.getPname());
      
      		   //å†™å‡ºä¿®æ”¹åçš„orderBeanå¯¹è±¡
                  context.write(orderBean,NullWritable.get());
              }
          }
      }
      
      ~~~

  + ç¼–å†™TableDriverç±»

    + ~~~java
      public class TableDriver {
          public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
              Job job = Job.getInstance(new Configuration());
      
              job.setJarByClass(TableDriver.class);
              job.setMapperClass(TableMapper.class);
              job.setReducerClass(TableReducer.class);
      
              job.setMapOutputKeyClass(Text.class);
              job.setMapOutputValueClass(TableBean.class);
      
              job.setOutputKeyClass(TableBean.class);
              job.setOutputValueClass(NullWritable.class);
      
              FileInputFormat.setInputPaths(job, new Path("D:\\input"));
              FileOutputFormat.setOutputPath(job, new Path("D:\\output"));
      
              boolean b = job.waitForCompletion(true);
              System.exit(b ? 0 : 1);
          }
      }
      
      ~~~

    + 





#### Reduce Joinæ€»ç»“



â€‹				ç¼ºç‚¹ï¼šè¿™ç§æ–¹å¼ä¸­ï¼Œåˆå¹¶çš„æ“ä½œæ˜¯åœ¨Reduceé˜¶æ®µå®Œæˆï¼ŒReduceç«¯çš„å¤„ç†å‹åŠ›å¤ªå¤§ï¼ŒMapèŠ‚ç‚¹çš„è¿ç®—è´Ÿè½½åˆ™å¾ˆä½ï¼Œèµ„æºåˆ©ç”¨ç‡ä¸é«˜ï¼Œä¸”åœ¨Reduceé˜¶æ®µ**ææ˜“äº§ç”Ÿæ•°æ®å€¾æ–œ**ã€‚

â€‹				==è§£å†³æ–¹æ¡ˆï¼šMapç«¯å®ç°æ•°æ®åˆå¹¶ã€‚==





#### Map Join

------



`å®ç°é€»è¾‘`ï¼šåœ¨Mapç«¯ç¼“å­˜å¤šå¼ è¡¨ï¼Œæå‰å¤„ç†ä¸šåŠ¡é€»è¾‘



1. **ä½¿ç”¨åœºæ™¯**

   Map Joiné€‚ç”¨äº**ä¸€å¼ è¡¨ååˆ†å°ã€ä¸€å¼ è¡¨å¾ˆå¤§çš„åœºæ™¯ã€‚**

2. ä¼˜ç‚¹

   å¢åŠ Mapç«¯ä¸šåŠ¡ï¼Œå‡å°‘Reduceç«¯æ•°æ®çš„å‹åŠ›
   
3. å®ç°æ­¥éª¤

   + åœ¨Mapperçš„setupé˜¶æ®µï¼Œå°†æ–‡ä»¶è¯»å–åˆ°ç¼“å­˜é›†åˆä¸­ã€‚

   + åœ¨Driveré©±åŠ¨ç±»ä¸­åŠ è½½ç¼“å­˜

     + ~~~java
       //ç¼“å­˜æ™®é€šæ–‡ä»¶åˆ°Taskè¿è¡ŒèŠ‚ç‚¹ã€‚
       job.addCacheFile(new URI("file:///e:/cache/pd.txt"));
       //å¦‚æœæ˜¯é›†ç¾¤è¿è¡Œ,éœ€è¦è®¾ç½®HDFSè·¯å¾„
       job.addCacheFile(new URI("hdfs://hadoop102:8020/cache/pd.txt"));
       
       ~~~







>  Map Joinæ¡ˆä¾‹å®æ“



![image-20220110202913825](../image/image-20220110202913825.png)



**éœ€æ±‚åˆ†æ**

![image-20220110203025708](../image/image-20220110203025708.png)





**å®ç°ä»£ç **

+ å…ˆåœ¨é©±åŠ¨ç±»ä¸­æ·»åŠ ç¼“å­˜æ–‡ä»¶

  + ~~~java
         // åŠ è½½ç¼“å­˜æ•°æ®
            job.addCacheFile(new URI("file:///D:/input/tablecache/pd.txt"));
            // Mapç«¯Joinçš„é€»è¾‘ä¸éœ€è¦Reduceé˜¶æ®µï¼Œè®¾ç½®reduceTaskæ•°é‡ä¸º0
            job.setNumReduceTasks(0);
    ~~~

+ åœ¨MapJoinMapperç±»ä¸­çš„setupæ–¹æ³•ä¸­è¯»å–ç¼“å­˜æ–‡ä»¶

  + ~~~java
    public class MapJoinMapper extends Mapper<LongWritable, Text, Text, NullWritable> {
    
        private Map<String, String> pdMap = new HashMap<>();
        private Text text = new Text();
    
        //ä»»åŠ¡å¼€å§‹å‰å°†pdæ•°æ®ç¼“å­˜è¿›pdMap
        @Override
        protected void setup(Context context) throws IOException, InterruptedException {
    		ğŸš©
            //é€šè¿‡ç¼“å­˜æ–‡ä»¶å¾—åˆ°å°è¡¨æ•°æ®pd.txt
            URI[] cacheFiles = context.getCacheFiles();
            Path path = new Path(cacheFiles[0]);
    
            //è·å–æ–‡ä»¶ç³»ç»Ÿå¯¹è±¡,å¹¶å¼€æµ
            FileSystem fs = FileSystem.get(context.getConfiguration());
            FSDataInputStream fis = fs.open(path);
    
            //é€šè¿‡åŒ…è£…æµè½¬æ¢ä¸ºreader,æ–¹ä¾¿æŒ‰è¡Œè¯»å–
            BufferedReader reader = new BufferedReader(new InputStreamReader(fis, "UTF-8"));
    
            //é€è¡Œè¯»å–ï¼ŒæŒ‰è¡Œå¤„ç†
            String line;
            while (StringUtils.isNotEmpty(line = reader.readLine())) {
                //åˆ‡å‰²ä¸€è¡Œ    
    //01	å°ç±³
                String[] split = line.split("\t");
                pdMap.put(split[0], split[1]);
            }
    
            //å…³æµ
            IOUtils.closeStream(reader);
        }
    
        @Override
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
    
            //è¯»å–å¤§è¡¨æ•°æ®    
    //1001	01	1
            String[] fields = value.toString().split("\t");
    
           ğŸš© //é€šè¿‡å¤§è¡¨æ¯è¡Œæ•°æ®çš„pid,å»pdMapé‡Œé¢å–å‡ºpname
            String pname = pdMap.get(fields[1]);
    
            //å°†å¤§è¡¨æ¯è¡Œæ•°æ®çš„pidæ›¿æ¢ä¸ºpname
            text.set(fields[0] + "\t" + pname + "\t" + fields[2]);
    
            //å†™å‡º
            context.write(text,NullWritable.get());
        }
    }
    
    ~~~

  + 



### 7)æ•°æ®æ¸…æ´—ï¼ˆETLï¼‰

â€‹			â€œETLï¼Œæ˜¯è‹±æ–‡Extract-Transform-Loadçš„ç¼©å†™ï¼Œç”¨æ¥æè¿°å°†æ•°æ®ä»æ¥æºç«¯ç»è¿‡æŠ½å–ï¼ˆExtractï¼‰ã€è½¬æ¢ï¼ˆTransformï¼‰ã€åŠ è½½ï¼ˆLoadï¼‰è‡³ç›®çš„ç«¯çš„è¿‡ç¨‹ã€‚ETLä¸€è¯è¾ƒå¸¸ç”¨åœ¨æ•°æ®ä»“åº“ï¼Œä½†å…¶å¯¹è±¡å¹¶ä¸é™äºæ•°æ®ä»“åº“

â€‹			åœ¨è¿è¡Œæ ¸å¿ƒä¸šåŠ¡MapReduceç¨‹åºä¹‹å‰ï¼Œå¾€å¾€è¦å…ˆå¯¹æ•°æ®è¿›è¡Œæ¸…æ´—ï¼Œæ¸…ç†æ‰ä¸ç¬¦åˆç”¨æˆ·è¦æ±‚çš„æ•°æ®ã€‚æ¸…ç†çš„è¿‡ç¨‹å¾€å¾€åªéœ€è¦è¿è¡ŒMapperç¨‹åºï¼Œä¸éœ€è¦è¿è¡ŒReduceç¨‹åºã€‚



> æ¡ˆä¾‹å®æ“

**1**ï¼‰éœ€æ±‚

å»é™¤æ—¥å¿—ä¸­å­—æ®µä¸ªæ•°å°äºç­‰äº11çš„æ—¥å¿—ã€‚

![image-20220110203914233](../image/image-20220110203914233.png)



**2**ï¼‰éœ€æ±‚åˆ†æ

éœ€è¦åœ¨Mapé˜¶æ®µå¯¹è¾“å…¥çš„æ•°æ®æ ¹æ®è§„åˆ™è¿›è¡Œè¿‡æ»¤æ¸…æ´—ã€‚

**3**ï¼‰å®ç°ä»£ç 

ï¼ˆ1ï¼‰ç¼–å†™WebLogMapperç±»

~~~java
public class WebLogMapper extends Mapper<LongWritable, Text, Text, NullWritable>{
	
	@Override
	protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
		
		// 1 è·å–1è¡Œæ•°æ®
		String line = value.toString();
		
		// 2 è§£ææ—¥å¿—
		boolean result = parseLog(line,context);
		
		// 3 æ—¥å¿—ä¸åˆæ³•é€€å‡º
		if (!result) {
			return;
		}
		
		// 4 æ—¥å¿—åˆæ³•å°±ç›´æ¥å†™å‡º
		context.write(value, NullWritable.get());
	}

	// ğŸ”¥2 å°è£…è§£ææ—¥å¿—çš„æ–¹æ³•
	private boolean parseLog(String line, Context context) {

		// 1 æˆªå–
		String[] fields = line.split(" ");
		
		// 2 æ—¥å¿—é•¿åº¦å¤§äº11çš„ä¸ºåˆæ³•
		if (fields.length > 11) {
			return true;
		}else {
			return false;
		}
	}
}
~~~



ï¼ˆ2ï¼‰ç¼–å†™WebLogDriverç±»

~~~java
public class WebLogDriver {
	public static void main(String[] args) throws Exception {

// è¾“å…¥è¾“å‡ºè·¯å¾„éœ€è¦æ ¹æ®è‡ªå·±ç”µè„‘ä¸Šå®é™…çš„è¾“å…¥è¾“å‡ºè·¯å¾„è®¾ç½®
        args = new String[] { "D:/input/inputlog", "D:/output1" };

		// 1 è·å–jobä¿¡æ¯
		Configuration conf = new Configuration();
		Job job = Job.getInstance(conf);

		// 2 åŠ è½½jaråŒ…
		job.setJarByClass(LogDriver.class);

		// 3 å…³è”map
		job.setMapperClass(WebLogMapper.class);

		//ğŸ”¥4 è®¾ç½®æœ€ç»ˆè¾“å‡ºç±»å‹
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(NullWritable.class);

		//ğŸ”¥è®¾ç½®reducetaskä¸ªæ•°ä¸º0
		job.setNumReduceTasks(0);

		// 5 è®¾ç½®è¾“å…¥å’Œè¾“å‡ºè·¯å¾„
		FileInputFormat.setInputPaths(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));

		// 6 æäº¤
         boolean b = job.waitForCompletion(true);
         System.exit(b ? 0 : 1);
	}
}

~~~



### MapReduceå¼€å‘æ€»ç»“

**1**ï¼‰`è¾“å…¥æ•°æ®æ¥å£ï¼šInputFormat`

â€‹	ï¼ˆ1ï¼‰é»˜è®¤ä½¿ç”¨çš„å®ç°ç±»æ˜¯ï¼šTextInputFormat

â€‹	ï¼ˆ2ï¼‰TextInputFormatçš„åŠŸèƒ½é€»è¾‘æ˜¯ï¼šä¸€æ¬¡è¯»ä¸€è¡Œæ–‡æœ¬ï¼Œç„¶åå°†è¯¥è¡Œçš„èµ·å§‹åç§»é‡ä½œä¸ºkeyï¼Œè¡Œå†…å®¹ä½œä¸ºvalueè¿”å›ã€‚

â€‹	ï¼ˆ3ï¼‰CombineTextInputFormatå¯ä»¥æŠŠå¤šä¸ªå°æ–‡ä»¶åˆå¹¶æˆä¸€ä¸ªåˆ‡ç‰‡å¤„ç†ï¼Œæé«˜å¤„ç†æ•ˆç‡ã€‚

**2**ï¼‰`é€»è¾‘å¤„ç†æ¥å£ï¼šMapper`

â€‹		ç”¨æˆ·æ ¹æ®ä¸šåŠ¡éœ€æ±‚å®ç°å…¶ä¸­ä¸‰ä¸ªæ–¹æ³•ï¼šmap()  setup()  cleanup () 

**3**ï¼‰`Partitioneråˆ†åŒº`

â€‹	ï¼ˆ1ï¼‰æœ‰é»˜è®¤å®ç° HashPartitionerï¼Œé€»è¾‘æ˜¯æ ¹æ®keyçš„å“ˆå¸Œå€¼å’ŒnumReducesæ¥è¿”å›ä¸€ä¸ªåˆ†åŒºå·ï¼›key.hashCode()&Integer.MAXVALUE % numReduces

â€‹	ï¼ˆ2ï¼‰å¦‚æœä¸šåŠ¡ä¸Šæœ‰ç‰¹åˆ«çš„éœ€æ±‚ï¼Œå¯ä»¥è‡ªå®šä¹‰åˆ†åŒºã€‚

**4**ï¼‰`Comparableæ’åº`

â€‹	ï¼ˆ1ï¼‰å½“æˆ‘ä»¬ç”¨è‡ªå®šä¹‰çš„å¯¹è±¡ä½œä¸ºkeyæ¥è¾“å‡ºæ—¶ï¼Œå°±å¿…é¡»è¦å®ç°WritableComparableæ¥å£ï¼Œé‡å†™å…¶ä¸­çš„compareTo()æ–¹æ³•ã€‚

â€‹	ï¼ˆ2ï¼‰éƒ¨åˆ†æ’åºï¼šå¯¹æœ€ç»ˆè¾“å‡ºçš„æ¯ä¸€ä¸ªæ–‡ä»¶è¿›è¡Œå†…éƒ¨æ’åºã€‚

â€‹	ï¼ˆ3ï¼‰å…¨æ’åºï¼šå¯¹æ‰€æœ‰æ•°æ®è¿›è¡Œæ’åºï¼Œé€šå¸¸åªæœ‰ä¸€ä¸ªReduceã€‚

â€‹	ï¼ˆ4ï¼‰äºŒæ¬¡æ’åºï¼šæ’åºçš„æ¡ä»¶æœ‰ä¸¤ä¸ªã€‚

**5**ï¼‰`Combineråˆå¹¶`

â€‹		Combineråˆå¹¶å¯ä»¥æé«˜ç¨‹åºæ‰§è¡Œæ•ˆç‡ï¼Œå‡å°‘IOä¼ è¾“

â€‹		ä½¿ç”¨æ—¶å¿…é¡»ä¸èƒ½å½±å“åŸæœ‰çš„ä¸šåŠ¡å¤„ç†ç»“æœã€‚ï¼ˆæ±‚å’Œâœ…     æ±‚å¹³å‡å€¼âï¼‰

â€‹		æå‰èšåˆmapğŸ‘‰è§£å†³æ•°æ®å€¾æ–œçš„ä¸€ä¸ªæ–¹æ³•

**6**ï¼‰`é€»è¾‘å¤„ç†æ¥å£ï¼šReducer`

â€‹		ç”¨æˆ·æ ¹æ®ä¸šåŠ¡éœ€æ±‚å®ç°å…¶ä¸­ä¸‰ä¸ªæ–¹æ³•ï¼šreduce()  setup()  cleanup () 

**7**ï¼‰`è¾“å‡ºæ•°æ®æ¥å£ï¼šOutputFormat`

â€‹	ï¼ˆ1ï¼‰é»˜è®¤å®ç°ç±»æ˜¯TextOutputFormatï¼ŒåŠŸèƒ½é€»è¾‘æ˜¯ï¼šå°†æ¯ä¸€ä¸ªKVå¯¹ï¼Œå‘ç›®æ ‡æ–‡æœ¬æ–‡ä»¶è¾“å‡ºä¸€è¡Œã€‚

â€‹	ï¼ˆ2ï¼‰ç”¨æˆ·è¿˜å¯ä»¥è‡ªå®šä¹‰OutputFormatã€‚





## å››ã€Hadoopæ•°æ®å‹ç¼©



| å‹ç¼©çš„ä¼˜ç‚¹ | ä»¥å‡å°‘ç£ç›˜IOã€å‡å°‘ç£ç›˜å­˜å‚¨ç©ºé—´ã€‚ |
| ---------- | -------------------------------- |
| å‹ç¼©çš„ç¼ºç‚¹ | å¢åŠ CPUå¼€é”€ã€‚                    |



| **å‹ç¼©åŸåˆ™**                   |
| :----------------------------- |
| ï¼ˆ1ï¼‰è¿ç®—å¯†é›†å‹çš„Jobï¼Œå°‘ç”¨å‹ç¼© |
| ï¼ˆ2ï¼‰IOå¯†é›†å‹çš„Jobï¼Œå¤šç”¨å‹ç¼©   |



### MRæ”¯æŒçš„å‹ç¼©ç¼–ç 

------

| å‹ç¼©æ ¼å¼ | Hadoopè‡ªå¸¦ï¼Ÿ    | ç®—æ³•    | æ–‡ä»¶æ‰©å±•å | æ˜¯å¦å¯åˆ‡ç‰‡ | æ¢æˆå‹ç¼©æ ¼å¼åï¼ŒåŸæ¥çš„ç¨‹åºæ˜¯å¦éœ€è¦ä¿®æ”¹ |
| -------- | --------------- | ------- | ---------- | ---------- | -------------------------------------- |
| DEFLATE  | æ˜¯ï¼Œç›´æ¥ä½¿ç”¨    | DEFLATE | .deflate   | å¦         | å’Œæ–‡æœ¬å¤„ç†ä¸€æ ·ï¼Œä¸éœ€è¦ä¿®æ”¹             |
| Gzip     | æ˜¯ï¼Œç›´æ¥ä½¿ç”¨    | DEFLATE | .gz        | å¦         | å’Œæ–‡æœ¬å¤„ç†ä¸€æ ·ï¼Œä¸éœ€è¦ä¿®æ”¹             |
| bzip2    | æ˜¯ï¼Œç›´æ¥ä½¿ç”¨    | bzip2   | .bz2       | ==æ˜¯==     | å’Œæ–‡æœ¬å¤„ç†ä¸€æ ·ï¼Œä¸éœ€è¦ä¿®æ”¹             |
| LZO      | ==å¦,éœ€è¦å®‰è£…== | LZO     | .lzo       | ==æ˜¯==     | ==éœ€è¦å»ºç´¢å¼•ï¼Œè¿˜éœ€è¦æŒ‡å®šè¾“å…¥æ ¼å¼==     |
| Snappy   | æ˜¯ï¼Œç›´æ¥ä½¿ç”¨    | Snappy  | .snappy    | å¦         | å’Œæ–‡æœ¬å¤„ç†ä¸€æ ·ï¼Œä¸éœ€è¦ä¿®æ”¹             |



[Snappç½‘ç«™](http://google.github.io/snappy/)

> Snappyæ˜¯ä¸€ä¸ªå‹ç¼©/è§£å‹ç¼©åº“ã€‚å®ƒä¸ç„å‡†æœ€å¤§å‹ç¼©ï¼Œæˆ–ä¸ä»»ä½•å…¶ä»–å‹ç¼©åº“çš„å…¼å®¹æ€§;ç›¸åï¼Œå®ƒæ—¨åœ¨éå¸¸é«˜é€Ÿå’Œåˆç†çš„å‹ç¼©ã€‚ä¾‹å¦‚ï¼Œä¸Zlibçš„æœ€å¿«æ¨¡å¼ç›¸æ¯”ï¼ŒSNAPPYæ˜¯å¤§å¤šæ•°è¾“å…¥çš„é€Ÿåº¦æ›´å¿«çš„å³°å€¼ï¼Œä½†æ˜¯ç”±æ­¤äº§ç”Ÿçš„å‹ç¼©æ–‡ä»¶ä½äº20ï¼…è‡³100ï¼…æ›´å¤§çš„ä½ç½®ã€‚åœ¨64ä½æ¨¡å¼ä¸‹çš„æ ¸å¿ƒI7å¤„ç†å™¨çš„å•ä¸ªæ ¸å¿ƒï¼Œ==SNAPPYå‹ç¼©çº¦250 MB /ç§’æˆ–æ›´é«˜ï¼Œå¹¶åœ¨çº¦500 MB /ç§’æˆ–æ›´é•¿æ—¶é—´å‡å‹ã€‚==



**å‹ç¼©æ€§èƒ½çš„æ¯”è¾ƒ**

| å‹ç¼©ç®—æ³• | åŸå§‹æ–‡ä»¶å¤§å° | å‹ç¼©æ–‡ä»¶å¤§å° | å‹ç¼©é€Ÿåº¦ | è§£å‹é€Ÿåº¦ |
| -------- | ------------ | ------------ | -------- | -------- |
| gzip     | 8.3GB        | 1.8GB        | 17.5MB/s | 58MB/s   |
| bzip2    | 8.3GB        | 1.1GB        | 2.4MB/s  | 9.5MB/s  |
| LZO      | 8.3GB        | 2.9GB        | 49.3MB/s | 74.6MB/s |





### å¸¸ç”¨å‹ç¼©æ ¼å¼

------

|            | ä¼˜ç‚¹                             | ç¼ºç‚¹                                     |
| ---------- | -------------------------------- | ---------------------------------------- |
| Gzipå‹ç¼©   | å‹ç¼©ç‡æ¯”è¾ƒé«˜                     | ä¸æ”¯æŒSplitï¼›å‹ç¼©/è§£å‹é€Ÿåº¦ä¸€èˆ¬ï¼›         |
| Bzip2å‹ç¼©  | å‹ç¼©ç‡é«˜ï¼›æ”¯æŒSplitï¼›            | å‹ç¼©/è§£å‹é€Ÿåº¦æ…¢ã€‚                        |
| Lzoå‹ç¼©    | å‹ç¼©/è§£å‹é€Ÿåº¦æ¯”è¾ƒå¿«ï¼›æ”¯æŒSplitï¼› | å‹ç¼©ç‡ä¸€èˆ¬ï¼›æƒ³æ”¯æŒåˆ‡ç‰‡éœ€è¦é¢å¤–åˆ›å»ºç´¢å¼•ã€‚ |
| Snappyå‹ç¼© | å‹ç¼©å’Œè§£å‹ç¼©é€Ÿåº¦å¿«               | ä¸æ”¯æŒSplitï¼›å‹ç¼©ç‡ä¸€èˆ¬ï¼›                |



### å‹ç¼©ä½ç½®é€‰æ‹©

------



![image-20220110225633721](../image/image-20220110225633721.png)



### å‹ç¼©å‚æ•°é…ç½®

------

ä¸ºäº†æ”¯æŒå¤šç§å‹ç¼©/è§£å‹ç¼©ç®—æ³•ï¼ŒHadoopå¼•å…¥äº†ç¼–ç /è§£ç å™¨

| å‹ç¼©æ ¼å¼ | å¯¹åº”çš„ç¼–ç /è§£ç å™¨                          |
| :------- | ------------------------------------------ |
| DEFLATE  | org.apache.hadoop.io.compress.DefaultCodec |
| gzip     | org.apache.hadoop.io.compress.GzipCodec    |
| bzip2    | org.apache.hadoop.io.compress.BZip2Codec   |
| LZO      | com.hadoop.compression.lzo.LzopCodec       |
| Snappy   | org.apache.hadoop.io.compress.SnappyCodec  |

è¦åœ¨Hadoopä¸­å¯ç”¨å‹ç¼©ï¼Œå¯ä»¥é…ç½®å¦‚ä¸‹å‚æ•°

|                             å‚æ•°                             | é»˜è®¤å€¼                                         | é˜¶æ®µ        | å»ºè®®                                          |
| :----------------------------------------------------------: | ---------------------------------------------- | ----------- | --------------------------------------------- |
|      io.compression.codecs    ï¼ˆåœ¨core-site.xmlä¸­é…ç½®ï¼‰      | æ— ï¼Œè¿™ä¸ªéœ€è¦åœ¨å‘½ä»¤è¡Œè¾“å…¥hadoop checknativeæŸ¥çœ‹ | è¾“å…¥å‹ç¼©    | Hadoopä½¿ç”¨æ–‡ä»¶æ‰©å±•ååˆ¤æ–­æ˜¯å¦æ”¯æŒæŸç§ç¼–è§£ç å™¨  |
|   mapreduce.map.output.compressï¼ˆåœ¨mapred-site.xmlä¸­é…ç½®ï¼‰   | false                                          | mapperè¾“å‡º  | è¿™ä¸ªå‚æ•°è®¾ä¸ºtrueå¯ç”¨å‹ç¼©                      |
| mapreduce.map.output.compress.codecï¼ˆåœ¨mapred-site.xmlä¸­é…ç½®ï¼‰ | org.apache.hadoop.io.compress.DefaultCodec     | mapperè¾“å‡º  | ä¼ä¸šå¤šä½¿ç”¨LZOæˆ–Snappyç¼–è§£ç å™¨åœ¨æ­¤é˜¶æ®µå‹ç¼©æ•°æ® |
| mapreduce.output.fileoutputformat.compressï¼ˆåœ¨mapred-site.xmlä¸­é…ç½®ï¼‰ | false                                          | reducerè¾“å‡º | è¿™ä¸ªå‚æ•°è®¾ä¸ºtrueå¯ç”¨å‹ç¼©                      |
| mapreduce.output.fileoutputformat.compress.codecï¼ˆåœ¨mapred-site.xmlä¸­é…ç½®ï¼‰ | org.apache.hadoop.io.compress.DefaultCodec     | reducerè¾“å‡º | ä½¿ç”¨æ ‡å‡†å·¥å…·æˆ–è€…ç¼–è§£ç å™¨ï¼Œå¦‚gzipå’Œbzip2       |







### å‹ç¼©å®æ“æ¡ˆä¾‹

------



**æŸ¥çœ‹é›†ç¾¤æ”¯æŒçš„å‹ç¼©æ ¼å¼**

~~~shell
$hadoop checknative
~~~



â€‹															**åŸºäºWordCountæ¡ˆä¾‹å¤„ç†**

> Mapè¾“å‡ºç«¯é‡‡ç”¨å‹ç¼©



+ Hadoopæºç æ”¯æŒçš„å‹ç¼©æ ¼å¼æœ‰ï¼š==BZip2Codecã€DefaultCodec==

~~~java
public class WordCountDriver {

	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {

		Configuration conf = new Configuration();

		ğŸ™ˆ// å¼€å¯mapç«¯è¾“å‡ºå‹ç¼©
		conf.setBoolean("mapreduce.map.output.compress", true);

		// è®¾ç½®mapç«¯è¾“å‡ºå‹ç¼©æ–¹å¼
		conf.setClass("mapreduce.map.output.compress.codec", BZip2Codec.class,CompressionCodec.class);

		Job job = Job.getInstance(conf);

		job.setJarByClass(WordCountDriver.class);

		job.setMapperClass(WordCountMapper.class);
		job.setReducerClass(WordCountReducer.class);

		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(IntWritable.class);

		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);

		FileInputFormat.setInputPaths(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));

		boolean result = job.waitForCompletion(true);

		System.exit(result ? 0 : 1);
	}
}

~~~

+ Mapperä¿æŒä¸å˜
+ Reducerä¿æŒä¸å˜





>  Reduceè¾“å‡ºç«¯é‡‡ç”¨å‹ç¼©



~~~java
public class WordCountDriver {

	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
		
		Configuration conf = new Configuration();
		
		Job job = Job.getInstance(conf);
		
		job.setJarByClass(WordCountDriver.class);
		
		job.setMapperClass(WordCountMapper.class);
		job.setReducerClass(WordCountReducer.class);
		
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(IntWritable.class);
		
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
		
		FileInputFormat.setInputPaths(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
		
		ğŸŒ¹// è®¾ç½®reduceç«¯è¾“å‡ºå‹ç¼©å¼€å¯
		FileOutputFormat.setCompressOutput(job, true);

		ğŸ‘€// è®¾ç½®å‹ç¼©çš„æ–¹å¼
	    FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class); 
//	    FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class); 
//	    FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class); 
	    
		boolean result = job.waitForCompletion(true);
		
		System.exit(result?0:1);
	}
}

~~~







## äº”ã€æ€»ç»“



==1ã€InputFormat==

+ é»˜è®¤çš„æ˜¯TextInputformat  kv  keyåç§»é‡ï¼Œv :ä¸€è¡Œå†…å®¹
+ å¤„ç†å°æ–‡ä»¶CombineTextInputFormat æŠŠå¤šä¸ªæ–‡ä»¶åˆå¹¶åˆ°ä¸€èµ·ç»Ÿä¸€åˆ‡ç‰‡



==2ã€Mapper==



  		setup()åˆå§‹åŒ–ï¼›  map()ç”¨æˆ·çš„ä¸šåŠ¡é€»è¾‘ï¼› clearup() å…³é—­èµ„æºï¼›



==3ã€åˆ†åŒº==
  		é»˜è®¤åˆ†åŒºHashPartitioner ï¼Œé»˜è®¤æŒ‰ç…§keyçš„hashå€¼%numreducetaskä¸ªæ•°
  		è‡ªå®šä¹‰åˆ†åŒº



==4ã€æ’åº==
  		1ï¼‰éƒ¨åˆ†æ’åº  æ¯ä¸ªè¾“å‡ºçš„æ–‡ä»¶å†…éƒ¨æœ‰åºã€‚
  		2ï¼‰å…¨æ’åºï¼š  ä¸€ä¸ªreduce ,å¯¹æ‰€æœ‰æ•°æ®å¤§æ’åºã€‚
  		3ï¼‰äºŒæ¬¡æ’åºï¼š  è‡ªå®šä¹‰æ’åºèŒƒç•´ï¼Œ å®ç° writableCompareæ¥å£ï¼Œ é‡å†™compareToæ–¹æ³•
  			æ€»æµé‡å€’åº  æŒ‰ç…§ä¸Šè¡Œæµé‡ æ­£åº



==5ã€Combiner==
  		å‰æï¼šä¸å½±å“æœ€ç»ˆçš„ä¸šåŠ¡é€»è¾‘ï¼ˆæ±‚å’Œ æ²¡é—®é¢˜   æ±‚å¹³å‡å€¼ï¼‰
  		æå‰èšåˆmap  => è§£å†³æ•°æ®å€¾æ–œçš„ä¸€ä¸ªæ–¹æ³•



==6ã€Reducer==

  		ç”¨æˆ·çš„ä¸šåŠ¡é€»è¾‘ï¼›
  		setup()åˆå§‹åŒ–ï¼›reduce()ç”¨æˆ·çš„ä¸šåŠ¡é€»è¾‘ï¼› clearup() å…³é—­èµ„æºï¼›



==7ã€OutputFormat==



+ é»˜è®¤TextOutputFormat  æŒ‰è¡Œè¾“å‡ºåˆ°æ–‡ä»¶
+ è‡ªå®šä¹‰













