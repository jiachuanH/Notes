# Yarn



## ä¸€ã€Yarnèµ„æºè°ƒåº¦å™¨

â€‹       

### å®šä¹‰

  Yarnæ˜¯ä¸€ä¸ª**èµ„æºè°ƒåº¦å¹³å°ï¼Œè´Ÿè´£ä¸ºè¿ç®—ç¨‹åºæä¾›æœåŠ¡å™¨è¿ç®—èµ„æº**ï¼Œç›¸å½“äºä¸€ä¸ªåˆ†å¸ƒå¼çš„æ“ä½œç³»ç»Ÿå¹³å°ï¼Œè€Œ**MapReduceç­‰è¿ç®—ç¨‹åºåˆ™ç›¸å½“äºè¿è¡Œäºæ“ä½œç³»ç»Ÿä¹‹ä¸Šçš„åº”ç”¨ç¨‹åºã€‚**



### åŸºç¡€æ¶æ„

YARNä¸»è¦ç”±==ResourceManagerã€NodeManagerã€ApplicationMasterå’ŒContainer==ç­‰ç»„ä»¶æ„æˆã€‚



![image-20220112222034583](../image/image-20220112222034583.png)



ğŸš©**ResourceManagerï¼ˆRMï¼‰ä¸»è¦ä½œç”¨å¦‚ä¸‹**									

+ å¤„ç†å®¢æˆ·ç«¯è¯·æ±‚
+ ç›‘æ§NodeManager
+ å¯åŠ¨æˆ–ç›‘æ§ApplicationMaster
+ èµ„æºçš„åˆ†é…ä¸è°ƒåº¦



**NodeManagerï¼ˆNMï¼‰ä¸»è¦ä½œç”¨å¦‚ä¸‹**

+ ç®¡ç†å•ä¸ªèŠ‚ç‚¹ä¸Šçš„èµ„æº

+ å¤„ç†æ¥è‡ªResourceManagerçš„å‘½ä»¤

+ å¤„ç†æ¥è‡ªApplicationMasterçš„å‘½ä»¤



**ApplicationMasterï¼ˆAMï¼‰ä½œç”¨å¦‚ä¸‹**

+ ä¸ºåº”ç”¨ç¨‹åºç”³è¯·èµ„æºå¹¶åˆ†é…ç»™å†…éƒ¨çš„ä»»åŠ¡

+ ä»»åŠ¡çš„ç›‘æ§ä¸å®¹é”™



**Container**

+ Containeræ˜¯YARNä¸­çš„èµ„æºæŠ½è±¡ï¼Œ**å®ƒå°è£…äº†æŸä¸ªèŠ‚ç‚¹ä¸Šçš„å¤šç»´åº¦èµ„æºï¼Œå¦‚å†…å­˜ã€CPUã€ç£ç›˜ã€ç½‘ç»œç­‰ã€‚**





### ğŸš©Yarnå·¥ä½œæœºåˆ¶

------





![image-20220112222904261](../image/image-20220112222904261.png)

â€‹	ï¼ˆ1ï¼‰MRç¨‹åº**æäº¤**åˆ°å®¢æˆ·ç«¯æ‰€åœ¨çš„èŠ‚ç‚¹ã€‚

â€‹    ï¼ˆ2ï¼‰YarnRunnerå‘ResourceManager**ç”³è¯·ä¸€ä¸ªApplication**ã€‚

â€‹    ï¼ˆ3ï¼‰RMå°†è¯¥åº”ç”¨ç¨‹åºçš„èµ„æº**è·¯å¾„è¿”å›**ç»™YarnRunnerã€‚

â€‹    ï¼ˆ4ï¼‰è¯¥ç¨‹åºå°†è¿è¡Œæ‰€éœ€èµ„æº**æäº¤åˆ°HDFSä¸Šã€‚**

â€‹    ï¼ˆ5ï¼‰ç¨‹åºèµ„æºæäº¤å®Œæ¯•åï¼Œ**ç”³è¯·è¿è¡ŒmrAppMaster**ã€‚

â€‹    ï¼ˆ6ï¼‰RMå°†ç”¨æˆ·çš„è¯·æ±‚**åˆå§‹åŒ–æˆä¸€ä¸ªTask**ã€‚

â€‹    ï¼ˆ7ï¼‰å…¶ä¸­ä¸€ä¸ªNodeManager**é¢†å–**åˆ°Taskä»»åŠ¡ã€‚

â€‹    ï¼ˆ8ï¼‰è¯¥NodeManager**åˆ›å»ºå®¹å™¨**Containerï¼Œå¹¶äº§ç”ŸMRAppmasterã€‚

â€‹    ï¼ˆ9ï¼‰Containerä»HDFSä¸Š**æ‹·è´èµ„æº**åˆ°æœ¬åœ°ã€‚

â€‹    ï¼ˆ10ï¼‰MRAppmasterå‘RM **ç”³è¯·è¿è¡ŒMapTaskèµ„æº**ã€‚

â€‹    ï¼ˆ11ï¼‰RMå°†è¿è¡ŒMapTask**ä»»åŠ¡åˆ†é…**ç»™å¦å¤–ä¸¤ä¸ªNodeManagerï¼Œå¦ä¸¤ä¸ªNodeManageråˆ†åˆ«**é¢†å–ä»»åŠ¡å¹¶åˆ›å»ºå®¹å™¨**ã€‚

â€‹    ï¼ˆ12ï¼‰**MR**å‘ä¸¤ä¸ªæ¥æ”¶åˆ°ä»»åŠ¡çš„NodeManager**å‘é€ç¨‹åºå¯åŠ¨è„šæœ¬**ï¼Œè¿™ä¸¤ä¸ªNodeManageråˆ†åˆ«å¯åŠ¨MapTaskï¼ŒMapTaskå¯¹æ•°æ®åˆ†åŒºæ’åºã€‚

â€‹	ï¼ˆ13ï¼‰MrAppMasterç­‰å¾…æ‰€æœ‰**MapTaskè¿è¡Œå®Œæ¯•å**ï¼Œå‘RM**ç”³è¯·å®¹å™¨ï¼Œè¿è¡ŒReduceTaskã€‚**

â€‹    ï¼ˆ14ï¼‰ReduceTaskå‘MapTask**è·å–**ç›¸åº”åˆ†åŒºçš„**æ•°æ®**ã€‚

â€‹    ï¼ˆ15ï¼‰ç¨‹åºè¿è¡Œå®Œæ¯•åï¼ŒMRä¼šå‘RM**ç”³è¯·æ³¨é”€è‡ªå·±ã€‚**



### ä½œä¸šæäº¤å…¨è¿‡ç¨‹

------















### ğŸš©Yarnè°ƒåº¦å™¨å’Œè°ƒåº¦ç®—æ³•

------

â€‹				ç›®å‰ï¼ŒHadoopä½œä¸šè°ƒåº¦å™¨ä¸»è¦æœ‰ä¸‰ç§ï¼š==FIFOã€å®¹é‡ï¼ˆCapacity Schedulerï¼‰å’Œå…¬å¹³ï¼ˆFair Schedulerï¼‰==ã€‚**Apache Hadoop3.1.3é»˜è®¤çš„èµ„æºè°ƒåº¦å™¨æ˜¯Capacity Schedulerï¼ˆå®¹é‡ï¼‰ã€‚**

â€‹									**CDHæ¡†æ¶é»˜è®¤è°ƒåº¦å™¨æ˜¯Fair Schedulerï¼ˆå…¬å¹³ï¼‰ã€‚**





**è°ƒåº¦ç®—æ³•é…ç½®æ–‡ä»¶**ï¼šğŸ§ `yarn-default.xmlæ–‡ä»¶`



~~~java
<property>
    <description>The class to use as the resource scheduler.</description>
            <name>yarn.resourcemanager.scheduler.class</name>					ğŸ‘‡
<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
</property>

~~~



#### å…ˆè¿›å…ˆå‡ºè°ƒåº¦å™¨ï¼ˆFIFOï¼‰



![image-20220112233453236](../image/image-20220112233453236.png)



ğŸ‡	ç®€å•æ˜“æ‡‚

ğŸ¢	ä¸æ”¯æŒå¤šé˜Ÿåˆ—ï¼Œç”Ÿäº§ç¯å¢ƒå¾ˆå°‘ä½¿ç”¨ï¼›





#### å®¹é‡è°ƒåº¦å™¨ï¼ˆCapacity Schedulerï¼‰

------

> Capacity Scheduleræ˜¯**Yahoo**å¼€å‘çš„å¤šç”¨æˆ·è°ƒåº¦å™¨ã€‚
>

![image-20220113153704843](../image/image-20220113153704843.png)



â€‹	1ã€å¤šé˜Ÿåˆ—ï¼šæ¯ä¸ªé˜Ÿåˆ—å¯é…ç½®ä¸€å®šçš„èµ„æºé‡ï¼Œæ¯ä¸ªé˜Ÿåˆ—é‡‡ç”¨FIFOè°ƒåº¦ç­–ç•¥ã€‚

â€‹	2ã€å®¹é‡ä¿è¯ï¼šç®¡ç†å‘˜å¯ä¸ºæ¯ä¸ªé˜Ÿåˆ—**è®¾ç½®èµ„æºæœ€ä½ä¿è¯å’Œèµ„æºä½¿ç”¨ä¸Šé™**

â€‹	3ã€çµæ´»æ€§ï¼šå¦‚æœä¸€ä¸ªé˜Ÿåˆ—ä¸­çš„èµ„æºæœ‰å‰©ä½™ï¼Œå¯ä»¥æš‚æ—¶å…±äº«ç»™é‚£äº›éœ€è¦èµ„æºçš„é˜Ÿåˆ—ï¼Œè€Œä¸€æ—¦è¯¥é˜Ÿåˆ—æœ‰æ–°çš„åº”ç”¨ç¨‹åºæ				äº¤ï¼Œåˆ™å…¶ä»–é˜Ÿåˆ—å€Ÿè°ƒçš„èµ„æºä¼šå½’è¿˜ç»™è¯¥é˜Ÿåˆ—ã€‚

â€‹	4ã€å¤šç§Ÿæˆ·ï¼š

   + æ”¯æŒå¤šç”¨æˆ·å…±äº«é›†ç¾¤å’Œå¤šåº”ç”¨ç¨‹åºåŒæ—¶è¿è¡Œã€‚
   + ä¸ºäº†é˜²æ­¢åŒä¸€ä¸ªç”¨æˆ·çš„ä½œä¸šç‹¬å é˜Ÿåˆ—ä¸­çš„èµ„æºï¼Œè¯¥è°ƒåº¦å™¨ä¼šå¯¹**åŒä¸€ç”¨æˆ·æäº¤çš„ä½œä¸šæ‰€å èµ„æºé‡è¿›è¡Œé™å®š**ã€‚

   



> å®¹é‡è°ƒåº¦å™¨èµ„æºåˆ†é…ç®—æ³•
>



**1**ï¼‰**é˜Ÿåˆ—èµ„æºåˆ†é…**

ä»rootå¼€å§‹ï¼Œä½¿ç”¨æ·±åº¦ä¼˜å…ˆç®—æ³•ï¼Œ==ä¼˜å…ˆé€‰æ‹©èµ„æºå ç”¨ç‡æœ€ä½çš„é˜Ÿåˆ—åˆ†é…èµ„æºã€‚==



**2**ï¼‰**ä½œä¸šèµ„æºåˆ†é…**

é»˜è®¤æŒ‰ç…§æäº¤ä½œä¸šçš„==ä¼˜å…ˆçº§å’Œæäº¤æ—¶é—´é¡ºåºåˆ†é…èµ„æºã€‚==



**3**ï¼‰**å®¹å™¨èµ„æºåˆ†é…**

+ æŒ‰ç…§å®¹å™¨çš„==ä¼˜å…ˆçº§==åˆ†é…èµ„æºï¼›

+ å¦‚æœä¼˜å…ˆçº§ç›¸åŒï¼ŒæŒ‰ç…§==æ•°æ®æœ¬åœ°æ€§åŸåˆ™ï¼š==

  + ä»»åŠ¡å’Œæ•°æ®åœ¨åŒä¸€èŠ‚ç‚¹

  + ä»»åŠ¡å’Œæ•°æ®åœ¨åŒä¸€æœºæ¶

  + ä»»åŠ¡å’Œæ•°æ®ä¸åœ¨åŒä¸€èŠ‚ç‚¹ä¹Ÿä¸åœ¨åŒä¸€æœºæ¶

    





#### å…¬å¹³è°ƒåº¦å™¨ï¼ˆFair Schedulerï¼‰

------

Fair Schedulereæ˜¯==Facebook==å¼€å‘çš„å¤šç”¨æˆ·è°ƒåº¦å™¨ã€‚



**ç‰¹ç‚¹ğŸ‘Š**

![image-20220113155454822](../image/image-20220113155454822.png)



**1**ï¼‰**ä¸å®¹é‡è°ƒåº¦å™¨ç›¸åŒç‚¹**

  + å¤šé˜Ÿåˆ—ï¼šæ”¯æŒå¤šé˜Ÿåˆ—å¤šä½œä¸š
  + å®¹é‡ä¿è¯ï¼šç®¡ç†å‘˜å¯ä¸ºæ¯ä¸ªé˜Ÿåˆ—è®¾ç½®**èµ„æºæœ€ä½ä¿è¯å’Œèµ„æºä½¿ç”¨ä¸Šçº¿**
  + çµæ´»æ€§ï¼šå¦‚æœä¸€ä¸ªé˜Ÿåˆ—ä¸­çš„èµ„æºæœ‰å‰©ä½™ï¼Œå¯ä»¥æš‚æ—¶å…±äº«ç»™é‚£äº›éœ€è¦èµ„æºçš„é˜Ÿåˆ—ï¼Œè€Œä¸€æ—¦è¯¥é˜Ÿåˆ—æœ‰æ–°çš„åº”ç”¨ç¨‹åºæäº¤ï¼Œåˆ™å…¶ä»–é˜Ÿåˆ—**å€Ÿè°ƒ**çš„èµ„æºä¼šå½’è¿˜ç»™è¯¥é˜Ÿåˆ—ã€‚
  + å¤šç§Ÿæˆ·ï¼š
    + æ”¯æŒå¤šç”¨æˆ·å…±äº«é›†ç¾¤å’Œå¤šåº”ç”¨ç¨‹åºåŒæ—¶è¿è¡Œï¼›
    + ä¸ºäº†é˜²æ­¢åŒä¸€ä¸ªç”¨æˆ·çš„ä½œä¸šç‹¬å é˜Ÿåˆ—ä¸­çš„èµ„æºï¼Œè¯¥è°ƒåº¦å™¨ä¼šå¯¹åŒä¸€ç”¨æˆ·æäº¤çš„ä½œä¸šæ‰€å èµ„æºé‡è¿›è¡Œé™å®šã€‚



**2**ï¼‰**ä¸å®¹é‡è°ƒåº¦å™¨ä¸åŒç‚¹**

| ï¼ˆ1ï¼‰æ ¸å¿ƒè°ƒåº¦ç­–ç•¥ä¸åŒ                        | ï¼ˆ2ï¼‰æ¯ä¸ªé˜Ÿåˆ—å¯ä»¥å•ç‹¬è®¾ç½®èµ„æºåˆ†é…æ–¹å¼ |
| -------------------------------------------- | ------------------------------------- |
| å®¹é‡è°ƒåº¦å™¨ï¼šä¼˜å…ˆé€‰æ‹©==èµ„æºåˆ©ç”¨ç‡ä½==çš„é˜Ÿåˆ—   | å®¹é‡è°ƒåº¦å™¨ï¼šFIFOã€ DRF                |
| å…¬å¹³è°ƒåº¦å™¨ï¼šä¼˜å…ˆé€‰æ‹©å¯¹èµ„æºçš„==ç¼ºé¢==æ¯”ä¾‹å¤§çš„ | å…¬å¹³è°ƒåº¦å™¨ï¼šFIFOã€==FAIRã€DRF==       |





> ç¼ºé¢
>



![image-20220113160306035](../image/image-20220113160306035.png)



+ å…¬å¹³è°ƒåº¦å™¨è®¾è®¡ç›®æ ‡æ˜¯ï¼šåœ¨æ—¶é—´å°ºåº¦ä¸Šï¼Œæ‰€æœ‰ä½œä¸šè·å¾—å…¬å¹³çš„èµ„æºã€‚**æŸä¸€æ—¶åˆ»ä¸€ä¸ªä½œä¸šåº”è·èµ„æºå’Œå®é™…è·å–èµ„æºçš„å·®è·å«â€œç¼ºé¢â€**

+ è°ƒåº¦å™¨ä¼šä¼˜å…ˆä¸º==ç¼ºé¢å¤§çš„ä½œä¸šåˆ†é…èµ„æº==





> ***\*å…¬å¹³è°ƒåº¦å™¨é˜Ÿåˆ—èµ„æºåˆ†é…æ–¹å¼\****

##### ä¸‰å¤§ç­–ç•¥

###### **1**ï¼‰**FIFOç­–ç•¥**

------

 å…¬å¹³è°ƒåº¦å™¨æ¯ä¸ªé˜Ÿåˆ—èµ„æºåˆ†é…ç­–ç•¥**å¦‚æœé€‰æ‹©FIFO**çš„è¯ï¼Œæ­¤æ—¶å…¬å¹³è°ƒåº¦å™¨**ç›¸å½“äº**ä¸Šé¢è®²è¿‡çš„**å®¹é‡è°ƒåº¦å™¨ã€‚**



###### **2**ï¼‰**Fairç­–ç•¥**

------

Fair ç­–ç•¥ï¼ˆé»˜è®¤ï¼‰æ˜¯ä¸€ç§åŸºäº**æœ€å¤§æœ€å°å…¬å¹³ç®—æ³•**å®ç°çš„èµ„æºå¤šè·¯å¤ç”¨æ–¹å¼ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œæ¯ä¸ªé˜Ÿåˆ—å†…éƒ¨é‡‡ç”¨è¯¥æ–¹å¼åˆ†é…èµ„æºã€‚è¿™æ„å‘³ç€ï¼Œå¦‚æœä¸€ä¸ªé˜Ÿåˆ—ä¸­æœ‰ä¸¤ä¸ªåº”ç”¨ç¨‹åºåŒæ—¶è¿è¡Œï¼Œåˆ™æ¯ä¸ªåº”ç”¨ç¨‹åºå¯å¾—åˆ°1/2çš„èµ„æºï¼›å¦‚æœä¸‰ä¸ªåº”ç”¨ç¨‹åºåŒæ—¶è¿è¡Œï¼Œåˆ™æ¯ä¸ªåº”ç”¨ç¨‹åºå¯å¾—åˆ°1/3çš„èµ„æºã€‚

**å…·ä½“èµ„æºåˆ†é…æµç¨‹å’Œå®¹é‡è°ƒåº¦å™¨ä¸€è‡´ï¼›**

**ï¼ˆ1ï¼‰é€‰æ‹©é˜Ÿåˆ—**

**ï¼ˆ2ï¼‰é€‰æ‹©ä½œä¸š**

**ï¼ˆ3ï¼‰é€‰æ‹©å®¹å™¨**

**ä»¥ä¸Šä¸‰æ­¥ï¼Œæ¯ä¸€æ­¥éƒ½æ˜¯æŒ‰ç…§å…¬å¹³ç­–ç•¥åˆ†é…èµ„æº**







![image-20220113161345733](../image/image-20220113161345733.png)

â€‹																			==4==                     ==2==

Ã˜**å®é™…æœ€å°èµ„æºä»½é¢**ï¼šmindshare = Minï¼ˆèµ„æºéœ€æ±‚é‡ï¼Œé…ç½®çš„æœ€å°èµ„æºï¼‰

â€‹											==1==						==2==

Ã˜**æ˜¯å¦é¥¥é¥¿**ï¼šisNeedy = èµ„æºä½¿ç”¨é‡ < mindshareï¼ˆå®é™…æœ€å°èµ„æºä»½é¢ï¼‰

â€‹															==1==					==2==

Ã˜**èµ„æºåˆ†é…æ¯”**ï¼šminShareRatio = èµ„æºä½¿ç”¨é‡ / Maxï¼ˆmindshare, 1ï¼‰

â€‹																			

Ã˜**èµ„æºä½¿ç”¨æƒé‡æ¯”**ï¼šuseToWeightRatio = èµ„æºä½¿ç”¨é‡ / æƒé‡[^å¯è‡ªå·±é…ç½®]







###### **3ï¼‰DRFç­–ç•¥**

-------

 DRFï¼ˆDominant Resource Fairnessï¼‰ï¼Œæˆ‘ä»¬**ä¹‹å‰è¯´çš„èµ„æºï¼Œéƒ½æ˜¯å•ä¸€æ ‡å‡†**ï¼Œä¾‹å¦‚åªè€ƒè™‘å†…å­˜ï¼ˆä¹Ÿæ˜¯Yarné»˜è®¤çš„æƒ…å†µï¼‰ã€‚ä½†æ˜¯å¾ˆå¤šæ—¶å€™æˆ‘ä»¬èµ„æºæœ‰å¾ˆå¤šç§ï¼Œä¾‹å¦‚å†…å­˜ï¼ŒCPUï¼Œç½‘ç»œå¸¦å®½ç­‰ï¼Œè¿™æ ·æˆ‘ä»¬å¾ˆéš¾è¡¡é‡ä¸¤ä¸ªåº”ç”¨åº”è¯¥åˆ†é…çš„èµ„æºæ¯”ä¾‹ã€‚





é‚£ä¹ˆåœ¨YARNä¸­ï¼Œæˆ‘ä»¬ç”¨DRFæ¥å†³å®šå¦‚ä½•è°ƒåº¦ï¼š





å‡è®¾é›†ç¾¤ä¸€å…±æœ‰100 CPUå’Œ10T å†…å­˜ï¼Œè€Œåº”ç”¨Aéœ€è¦ï¼ˆ2 CPU, 300GBï¼‰ï¼Œåº”ç”¨Béœ€è¦ï¼ˆ6 CPUï¼Œ100GBï¼‰ã€‚åˆ™ä¸¤ä¸ªåº”ç”¨åˆ†åˆ«éœ€è¦Aï¼ˆ2%CPU, 3%å†…å­˜ï¼‰å’ŒBï¼ˆ6%CPU, 1%å†…å­˜ï¼‰çš„èµ„æºï¼Œè¿™å°±æ„å‘³ç€Aæ˜¯å†…å­˜ä¸»å¯¼çš„, Bæ˜¯CPUä¸»å¯¼çš„ï¼Œé’ˆå¯¹è¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©DRFç­–ç•¥**å¯¹ä¸åŒåº”ç”¨è¿›è¡Œä¸åŒèµ„æºï¼ˆCPUå’Œå†…å­˜ï¼‰çš„ä¸€ä¸ªä¸åŒæ¯”ä¾‹çš„é™åˆ¶ã€‚** 







##### é˜Ÿåˆ—èµ„æºåˆ†é…

------

`åŸºäºFair`

éœ€æ±‚ï¼šé›†ç¾¤æ€»èµ„æº100ï¼Œæœ‰ä¸‰ä¸ªé˜Ÿåˆ—ï¼Œå¯¹èµ„æºçš„éœ€æ±‚åˆ†åˆ«æ˜¯ï¼š

â€‹					queueA -> 20ï¼Œ queueB ->50ï¼Œ queueC -> 30



| ç¬¬ä¸€æ¬¡ç®—ï¼š100 / 3 = 33.33 | ç¬¬äºŒæ¬¡ç®—ï¼š ï¼ˆ13.33 + 3.33ï¼‰/ 1 = 16.66 |
| ------------------------- | -------------------------------------- |
| queueAï¼šåˆ†33.33 Ã  å¤š13.33 | queueAï¼šåˆ†20                           |
| queueBï¼šåˆ†33.33 Ã  å°‘16.67 | queueBï¼šåˆ†33.33 + 16.66 = 50           |
| queueCï¼šåˆ†33.33 Ã  å¤š3.33  | queueCï¼šåˆ†30                           |





##### **ä½œä¸šèµ„æºåˆ†é…**



> ä¸åŠ æƒ

ğŸ€ï¼ˆå…³æ³¨ç‚¹æ˜¯Jobçš„ä¸ªæ•°ï¼‰



éœ€æ±‚ï¼šæœ‰ä¸€æ¡é˜Ÿåˆ—æ€»èµ„æº12ä¸ª, æœ‰4ä¸ªjobï¼Œå¯¹èµ„æºçš„éœ€æ±‚åˆ†åˆ«æ˜¯:

ğŸ‘²		`job1â¡1`, 			`job2â¡2 ,` 			`job3â¡6,` 			`job4â¡5`



| ç¬¬ä¸€æ¬¡ç®—: 12 / 4 = 3 | ç¬¬äºŒæ¬¡ç®—: 3 / 2 = 1.5                       | ç¬¬næ¬¡ç®—: ä¸€ç›´ç®—åˆ°æ²¡æœ‰ç©ºé—²èµ„æº |
| :------------------: | ------------------------------------------- | ----------------------------- |
| job1: åˆ†3 --> å¤š2ä¸ª  | job1: åˆ†1                                   | job1                          |
| job2: åˆ†3 --> å¤š1ä¸ª  | job2: åˆ†2                                   | job2                          |
| job3: åˆ†3 --> å·®3ä¸ª  | job3: åˆ†3 --> å·®3ä¸ª --> åˆ†1.5 --> æœ€ç»ˆ: 4.5 | job3                          |
| job4: åˆ†3 --> å·®2ä¸ª  | job4: åˆ†3 --> å·®2ä¸ª --> åˆ†1.5 --> æœ€ç»ˆ: 4.5 | job4                          |





> åŠ æƒ
>

ğŸ€ï¼ˆå…³æ³¨ç‚¹æ˜¯Jobçš„æƒé‡ï¼‰



éœ€æ±‚ï¼šæœ‰ä¸€æ¡é˜Ÿåˆ—æ€»èµ„æº16ï¼Œæœ‰4ä¸ªjob

å¯¹èµ„æºçš„éœ€æ±‚åˆ†åˆ«æ˜¯

â€‹			job1â¡4			  job2â¡2		 job3â¡0			 job4â¡4 

æ¯ä¸ªjobçš„æƒé‡ä¸º

â€‹			job1â¡5 		 job2â¡8 			job3â¡1  			job4â¡2



| ç¬¬ä¸€æ¬¡ç®—: 16 / (5+8+1+2) = 1 | ç¬¬äºŒæ¬¡ç®—: 7 / (1+2) = 7/3             | ç¬¬ä¸‰æ¬¡ç®—:2.66/1=2.66              |
| ---------------------------- | ------------------------------------- | --------------------------------- |
| job1: åˆ†5 --> å¤š1            | job1: åˆ†4                             | job1: åˆ†4                         |
| job2: åˆ†8 --> å¤š6            | job2: åˆ†2                             | job2: åˆ†2                         |
| job3: åˆ†1 --> å°‘9            | job3: åˆ†1 --> åˆ†7/3ï¼ˆ2.33ï¼‰ -->å°‘6.67 | job3: åˆ†1 --> åˆ†2.66/1 --> åˆ†2.66 |
| job4: åˆ†2 --> å°‘2            | job4: åˆ†2 --> åˆ†14/3(4.66) -->å¤š2.66  | job4: åˆ†4                         |









### Yarnå¸¸ç”¨å‘½ä»¤



------



**Yarn applicationæŸ¥çœ‹ä»»åŠ¡**

   + åˆ—å‡ºæ‰€æœ‰Applicationï¼š
     + `yarn application -list`





   + æ ¹æ®ApplicationçŠ¶æ€è¿‡æ»¤
     + `yarn application -list -appStates ` 
     + ï¼ˆæ‰€æœ‰çŠ¶æ€ï¼šALLã€NEWã€NEW_SAVINGã€SUBMITTEDã€ACCEPTEDã€RUNNINGã€FINISHEDã€FAILEDã€KILLEDï¼‰
     + ä¾‹å­ï¼š`yarn application -list -appStates FINISHED`





   + Killæ‰Application
     +  `yarn application -kill application_1612577921195_0001`





==ä¸webç«¯ç›¸å¯¹åº”==







 **Yarn logsæŸ¥çœ‹æ—¥å¿—**



   + æŸ¥è¯¢Applicationæ—¥å¿—
     + `yarn logs -applicationId <ApplicationId>`[^ApplicationId]:é€šè¿‡æŸ¥è¯¢çŠ¶æ€å¯å¾—





   + æŸ¥è¯¢Containeræ—¥å¿—

     + `yarn logs -applicationId <ApplicationId> -containerId <ContainerId> `[^ContainerId]:é€šè¿‡**applicationattemp**tå¯å¾—

     





**Yarn applicationattemptæŸ¥çœ‹å°è¯•è¿è¡Œçš„ä»»åŠ¡**



+ åˆ—å‡ºæ‰€æœ‰Applicationå°è¯•çš„åˆ—è¡¨
  + `yarn applicationattempt -list <ApplicationId>`





+ æ‰“å°ApplicationAttempçŠ¶æ€

  + `yarn applicationattempt -status <ApplicationAttemptId>`

  + [^status]:æ‰€æœ‰çŠ¶æ€ï¼šALLã€NEWã€NEW_SAVINGã€SUBMITTEDã€ACCEPTEDã€RUNNINGã€FINISHEDã€FAILEDã€KILLEDï¼‰







**Yarn containeræŸ¥çœ‹å®¹å™¨**



[^PS]:å®¹å™¨å¿…é¡»åœ¨è¿è¡Œæ—¶æ‰èƒ½æŸ¥è¯¢åˆ°



+ åˆ—å‡ºæ‰€æœ‰Container
  + `yarn container -list <ApplicationAttemptId>`





+ æ‰“å°ContainerçŠ¶æ€

  + `    yarn container -status <ContainerId>`

  + [^status]:æ‰€æœ‰çŠ¶æ€ï¼šALLã€NEWã€NEW_SAVINGã€SUBMITTEDã€ACCEPTEDã€RUNNINGã€FINISHEDã€FAILEDã€KILLEDï¼‰







**Yarn nodeæŸ¥çœ‹èŠ‚ç‚¹çŠ¶æ€**



+ åˆ—å‡ºæ‰€æœ‰èŠ‚ç‚¹
  + `yarn node -list -all`





**Yarn rmadminæ›´æ–°é…ç½®**



+ åŠ è½½é˜Ÿåˆ—é…ç½®
  + `yarn rmadmin -refreshQueues`





**Yarn queueæŸ¥çœ‹é˜Ÿåˆ—**





+ æ‰“å°é˜Ÿåˆ—ä¿¡æ¯
  + `yarn queue -status <QueueName>`



==webï¼š8088ä¹Ÿå¯ä»¥æŸ¥çœ‹==ï¼ˆæ›´è¯¦ç»†ï¼‰

SchedulerğŸ‘‰Application QueuesğŸ‘‰Queue ï¼šdefault







### Yarnç”Ÿäº§ç¯å¢ƒæ ¸å¿ƒå‚æ•°

------



> â€‹	Apacheï¼šé»˜è®¤å®¹é‡è°ƒåº¦å™¨ï¼ˆå°å…¬å¸ï¼‰  CDHï¼šé»˜è®¤å…¬å¹³è°ƒåº¦å™¨ï¼ˆå¤§å…¬å¸ï¼Œè§£å†³é«˜å¹¶å‘ï¼‰

**1ï¼‰ResourceManagerç›¸å…³**



|             yarn.resourcemanager.scheduler.class             | ==é…ç½®è°ƒåº¦å™¨ï¼Œé»˜è®¤å®¹é‡è°ƒåº¦å™¨==                               |
| :----------------------------------------------------------: | ------------------------------------------------------------ |
| yarn.resourcemanager.scheduler.client.thread-count   ResourceManager | å¤„ç†è°ƒåº¦å™¨è¯·æ±‚çš„çº¿ç¨‹æ•°é‡ï¼Œ**é»˜è®¤**50ï¼š==RMä¸€æ¬¡å¤„ç†çš„ä»»åŠ¡æ•°é‡== |





**2)NodeManagerç›¸å…³**

ğŸš©

| yarn.nodemanager.resource.detect-hardware-capabilities      |          æ˜¯å¦è®©yarnè‡ªå·±æ£€æµ‹ç¡¬ä»¶è¿›è¡Œé…ç½®ï¼Œé»˜è®¤false           |
| ----------------------------------------------------------- | :----------------------------------------------------------: |
| yarn.nodemanager.resource.count-logical-processors-as-cores |             æ˜¯å¦å°†è™šæ‹Ÿæ ¸æ•°å½“ä½œCPUæ ¸æ•°ï¼Œé»˜è®¤false             |
| yarn.nodemanager.resource.pcores-vcores-multiplier          | è™šæ‹Ÿæ ¸æ•°å’Œç‰©ç†æ ¸æ•°ä¹˜æ•°ï¼Œä¾‹å¦‚ï¼š4æ ¸8çº¿ç¨‹ï¼Œè¯¥å‚æ•°å°±åº”è®¾ä¸º2ï¼Œé»˜è®¤1.0 |
| yarn.nodemanager.resource.cpu-vcores     NodeManager        |                   ==ä½¿ç”¨CPUæ ¸æ•°ï¼Œé»˜è®¤8ä¸ª==                   |
| yarn.nodemanager.pmem-check-enabled                         |       æ˜¯å¦==å¼€å¯ç‰©ç†å†…å­˜æ£€æŸ¥é™åˆ¶==containerï¼Œé»˜è®¤æ‰“å¼€        |
| yarn.nodemanager.vmem-check-enabled                         |         æ˜¯å¦å¼€å¯è™šæ‹Ÿå†…å­˜æ£€æŸ¥é™åˆ¶containerï¼Œé»˜è®¤æ‰“å¼€          |
| yarn.nodemanager.vmem-pmem-ratio                            |            ==è™šæ‹Ÿå†…å­˜ç‰©ç†å†…å­˜æ¯”ä¾‹ï¼Œ**é»˜è®¤**2.1==             |

**è™šæ‹Ÿæ ¸ä½œç”¨ï¼š å°†ä¸»æœºä¸€ä¸ªå½“Xä¸ªæ¥ç”¨**

**é›†ç¾¤é…ç½®ç›¸åŒåˆ™ä¸å¼€è™šæ‹Ÿæ ¸**

**è™šæ‹Ÿæ ¸æ•°å’Œç‰©ç†æ ¸æ•°ä¹˜æ•°ï¼š è¯¥å‚æ•°ç”¨æ¥è®¾ç½®ä¸€ä¸ªNodeManagerå½“å‡ ä¸ªæ¥ç”¨**

==è¯¥é…ç½®æ˜¯å¯¹å•èŠ‚ç‚¹çš„é…ç½®==





**3ï¼‰Containerç›¸å…³**



| yarn.scheduler.minimum-allocation-mb     | å®¹å™¨æœ€æœ€å°å†…å­˜ï¼Œ==é»˜è®¤1G==   |
| ---------------------------------------- | ---------------------------- |
| yarn.scheduler.maximum-allocation-mb     | å®¹å™¨æœ€æœ€å¤§å†…å­˜ï¼Œ==é»˜è®¤8G==   |
| yarn.scheduler.minimum-allocation-vcores | å®¹å™¨æœ€å°CPUæ ¸æ•°ï¼Œ==é»˜è®¤1ä¸ª== |
| yarn.scheduler.maximum-allocation-vcores | å®¹å™¨æœ€å¤§CPUæ ¸æ•°ï¼Œ==é»˜è®¤4ä¸ª== |







## äºŒã€Yarnæ¡ˆä¾‹å®æ“



==VMæå‰ä¿å­˜å¿«ç…§ï¼Œä¾¿äºåç»­æ¢å¤==





### Yarnç”Ÿäº§ç¯å¢ƒæ ¸å¿ƒå‚æ•°é…ç½®æ¡ˆä¾‹



#### é…ç½®å‰

**1ï¼‰éœ€æ±‚ï¼šä»1Gæ•°æ®ä¸­ï¼Œç»Ÿè®¡æ¯ä¸ªå•è¯å‡ºç°æ¬¡æ•°ã€‚æœåŠ¡å™¨3å°ï¼Œæ¯å°é…ç½®4Gå†…å­˜ï¼Œ4æ ¸CPUï¼Œ4çº¿ç¨‹ã€‚**

**2ï¼‰éœ€æ±‚åˆ†æï¼š**

> 1G â—128m =8ä¸ªMapTaskï¼›1ä¸ªReduceTaskï¼›1ä¸ªmrAppMaster   8+1+1=10  **10ä¸ªå®¹å™¨**
>
> â€‹				å¹³å‡æ¯ä¸ªèŠ‚ç‚¹è¿è¡Œ10ä¸ª / 3å° â‰ˆ 3ä¸ªä»»åŠ¡ï¼ˆ4   3   3ï¼‰å¤šä¸€ç‚¹æ— æ‰€è°“



**3ï¼‰ä¿®æ”¹yarn-site.xmlé…ç½®å‚æ•°å¦‚ä¸‹ï¼š**

~~~xml
		ğŸ¥•<!-- é€‰æ‹©è°ƒåº¦å™¨ï¼Œé»˜è®¤å®¹é‡ -->
<property>
	<description>The class to use as the resource scheduler.</description>
	<name>yarn.resourcemanager.scheduler.class</name>
	<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
</property>

ğŸ¥•<!-- ResourceManagerå¤„ç†è°ƒåº¦å™¨è¯·æ±‚çš„çº¿ç¨‹æ•°é‡,é»˜è®¤50ï¼›å¦‚æœæäº¤çš„ä»»åŠ¡æ•°å¤§äº50ï¼Œå¯ä»¥å¢åŠ è¯¥å€¼ï¼Œä½†æ˜¯ä¸èƒ½è¶…è¿‡3å° * 4çº¿ç¨‹ = 12çº¿ç¨‹ï¼ˆå»é™¤å…¶ä»–åº”ç”¨ç¨‹åºå®é™…ä¸èƒ½è¶…è¿‡8ï¼‰ -->
<property>
	<description>Number of threads to handle scheduler interface.</description>
	<name>yarn.resourcemanager.scheduler.client.thread-count</name>
	<value>8</value>
</property>

ğŸ¥•<!-- æ˜¯å¦è®©yarnè‡ªåŠ¨æ£€æµ‹ç¡¬ä»¶è¿›è¡Œé…ç½®ï¼Œé»˜è®¤æ˜¯falseï¼Œå¦‚æœè¯¥èŠ‚ç‚¹æœ‰å¾ˆå¤šå…¶ä»–åº”ç”¨ç¨‹åºï¼Œå»ºè®®æ‰‹åŠ¨é…ç½®ã€‚å¦‚æœè¯¥èŠ‚ç‚¹æ²¡æœ‰å…¶ä»–åº”ç”¨ç¨‹åºï¼Œå¯ä»¥é‡‡ç”¨è‡ªåŠ¨ -->
<property>
	<description>Enable auto-detection of node capabilities such as
	memory and CPU.
	</description>
	<name>yarn.nodemanager.resource.detect-hardware-capabilities</name>
	<value>false</value>
</property>

ğŸ¥•<!-- æ˜¯å¦å°†è™šæ‹Ÿæ ¸æ•°å½“ä½œCPUæ ¸æ•°ï¼Œé»˜è®¤æ˜¯falseï¼Œé‡‡ç”¨ç‰©ç†CPUæ ¸æ•° -->
<property>
	<description>Flag to determine if logical processors(such as
	hyperthreads) should be counted as cores. Only applicable on Linux
	when yarn.nodemanager.resource.cpu-vcores is set to -1 and
	yarn.nodemanager.resource.detect-hardware-capabilities is true.
	</description>
	<name>yarn.nodemanager.resource.count-logical-processors-as-cores</name>
	<value>false</value>
</property>

ğŸ¥•<!-- è™šæ‹Ÿæ ¸æ•°å’Œç‰©ç†æ ¸æ•°ä¹˜æ•°ï¼Œé»˜è®¤æ˜¯1.0 -->
<property>
	<description>Multiplier to determine how to convert phyiscal cores to
	vcores. This value is used if yarn.nodemanager.resource.cpu-vcores
	is set to -1(which implies auto-calculate vcores) and
	yarn.nodemanager.resource.detect-hardware-capabilities is set to true. The	number of vcores will be calculated as	number of CPUs * multiplier.
	</description>
	<name>yarn.nodemanager.resource.pcores-vcores-multiplier</name>
	<value>1.0</value>
</property>

ğŸ¥•<!-- NodeManagerä½¿ç”¨å†…å­˜æ•°ï¼Œé»˜è®¤8Gï¼Œä¿®æ”¹ä¸º4Gå†…å­˜ -->
<property>
	<description>Amount of physical memory, in MB, that can be allocated 
	for containers. If set to -1 and
	yarn.nodemanager.resource.detect-hardware-capabilities is true, it is
	automatically calculated(in case of Windows and Linux).
	In other cases, the default is 8192MB.
	</description>
	<name>yarn.nodemanager.resource.memory-mb</name>
	<value>4096</value>
</property>

ğŸ¥•<!-- nodemanagerçš„CPUæ ¸æ•°ï¼Œä¸æŒ‰ç…§ç¡¬ä»¶ç¯å¢ƒè‡ªåŠ¨è®¾å®šæ—¶é»˜è®¤æ˜¯8ä¸ªï¼Œä¿®æ”¹ä¸º4ä¸ª -->
<property>
	<description>Number of vcores that can be allocated
	for containers. This is used by the RM scheduler when allocating
	resources for containers. This is not used to limit the number of
	CPUs used by YARN containers. If it is set to -1 and
	yarn.nodemanager.resource.detect-hardware-capabilities is true, it is
	automatically determined from the hardware in case of Windows and Linux.
	In other cases, number of vcores is 8 by default.</description>
	<name>yarn.nodemanager.resource.cpu-vcores</name>
	<value>4</value>
</property>

ğŸ¥•<!-- å®¹å™¨æœ€å°å†…å­˜ï¼Œé»˜è®¤1G -->
<property>
	<description>The minimum allocation for every container request at the RM	in MBs. Memory requests lower than this will be set to the value of this	property. Additionally, a node manager that is configured to have less memory	than this value will be shut down by the resource manager.
	</description>
	<name>yarn.scheduler.minimum-allocation-mb</name>
	<value>1024</value>
</property>

ğŸ¥•<!-- å®¹å™¨æœ€å¤§å†…å­˜ï¼Œé»˜è®¤8Gï¼Œä¿®æ”¹ä¸º2G -->
<property>
	<description>The maximum allocation for every container request at the RM	in MBs. Memory requests higher than this will throw an	InvalidResourceRequestException.
	</description>
	<name>yarn.scheduler.maximum-allocation-mb</name>
	<value>2048</value>
</property>

ğŸ¥•<!-- å®¹å™¨æœ€å°CPUæ ¸æ•°ï¼Œé»˜è®¤1ä¸ª -->
<property>
	<description>The minimum allocation for every container request at the RM	in terms of virtual CPU cores. Requests lower than this will be set to the	value of this property. Additionally, a node manager that is configured to	have fewer virtual cores than this value will be shut down by the resource	manager.
	</description>
	<name>yarn.scheduler.minimum-allocation-vcores</name>
	<value>1</value>
</property>

ğŸ¥•<!-- å®¹å™¨æœ€å¤§CPUæ ¸æ•°ï¼Œé»˜è®¤4ä¸ªï¼Œä¿®æ”¹ä¸º2ä¸ª -->
<property>
	<description>The maximum allocation for every container request at the RM	in terms of virtual CPU cores. Requests higher than this will throw an
	InvalidResourceRequestException.</description>
	<name>yarn.scheduler.maximum-allocation-vcores</name>
	<value>2</value>
</property>

ğŸ¥•<!-- è™šæ‹Ÿå†…å­˜æ£€æŸ¥ï¼Œé»˜è®¤æ‰“å¼€ï¼Œä¿®æ”¹ä¸ºå…³é—­ -->
<property>
	<description>Whether virtual memory limits will be enforced for
	containers.</description>
	<name>yarn.nodemanager.vmem-check-enabled</name>
	<value>false</value>
</property>

ğŸ¥•<!-- è™šæ‹Ÿå†…å­˜å’Œç‰©ç†å†…å­˜è®¾ç½®æ¯”ä¾‹,é»˜è®¤2.1 -->
<property>
	<description>Ratio between virtual memory to physical memory when	setting memory limits for containers. Container allocations are	expressed in terms of physical memory, and virtual memory usage	is allowed to exceed this allocation by this ratio.
	</description>
	<name>yarn.nodemanager.vmem-pmem-ratio</name>
	<value>2.1</value>
</property>

~~~

#### é…ç½®å

**4ï¼‰åˆ†å‘é…ç½®**

==æ³¨æ„ï¼šå¦‚æœé›†ç¾¤çš„ç¡¬ä»¶èµ„æºä¸ä¸€è‡´ï¼Œè¦æ¯ä¸ªNodeManagerå•ç‹¬é…ç½®==



**5ï¼‰é‡å¯é›†ç¾¤**

`sbin/stop-yarn.sh`

`sbin/stop-yarn.sh`



**6ï¼‰æ‰§è¡ŒWordCountç¨‹åº**

~~~sh
[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output
~~~



**7ï¼‰è§‚å¯ŸYarnä»»åŠ¡æ‰§è¡Œé¡µé¢**

http://hadoop103:8088/cluster/apps





### ğŸš©å®¹é‡è°ƒåº¦å™¨å¤šé˜Ÿåˆ—æäº¤æ¡ˆä¾‹

------

#### ğŸ€

**1ï¼‰åœ¨ç”Ÿäº§ç¯å¢ƒæ€ä¹ˆåˆ›å»ºé˜Ÿåˆ—ï¼Ÿ**

â€‹		==ï¼ˆ1ï¼‰è°ƒåº¦å™¨é»˜è®¤å°±1ä¸ªdefaulté˜Ÿåˆ—ï¼Œä¸èƒ½æ»¡è¶³ç”Ÿäº§è¦æ±‚ã€‚==

   	 ï¼ˆ2ï¼‰**æŒ‰ç…§æ¡†æ¶**ï¼šhive /spark/ flink æ¯ä¸ªæ¡†æ¶çš„ä»»åŠ¡æ”¾å…¥æŒ‡å®šçš„é˜Ÿåˆ—ï¼ˆä¼ä¸šç”¨çš„ä¸æ˜¯ç‰¹åˆ«å¤šï¼‰

â€‹		ï¼ˆ3ï¼‰**æŒ‰ç…§ä¸šåŠ¡æ¨¡å—**ï¼šç™»å½•æ³¨å†Œã€è´­ç‰©è½¦ã€ä¸‹å•ã€ä¸šåŠ¡éƒ¨é—¨1ã€ä¸šåŠ¡éƒ¨é—¨2

**2ï¼‰åˆ›å»ºå¤šé˜Ÿåˆ—çš„å¥½å¤„ï¼Ÿ**

â€‹		ï¼ˆ1ï¼‰å› ä¸ºæ‹…å¿ƒå‘˜å·¥ä¸å°å¿ƒï¼Œå†™é€’å½’æ­»å¾ªç¯ä»£ç ï¼ŒæŠŠæ‰€æœ‰èµ„æºå…¨éƒ¨è€—å°½ã€‚

â€‹		ï¼ˆ2ï¼‰å®ç°ä»»åŠ¡çš„**é™çº§**ä½¿ç”¨ï¼Œç‰¹æ®Šæ—¶æœŸä¿è¯é‡è¦çš„ä»»åŠ¡é˜Ÿåˆ—èµ„æºå……è¶³ã€‚11.11       6.18

ä¸šåŠ¡éƒ¨é—¨1ï¼ˆé‡è¦ï¼‰=ã€‹ä¸šåŠ¡éƒ¨é—¨2ï¼ˆæ¯”è¾ƒé‡è¦ï¼‰=ã€‹ä¸‹å•ï¼ˆä¸€èˆ¬ï¼‰=ã€‹è´­ç‰©è½¦ï¼ˆä¸€èˆ¬ï¼‰=ã€‹ç™»å½•æ³¨å†Œï¼ˆæ¬¡è¦ï¼‰





#### éœ€æ±‚

    + defaulté˜Ÿåˆ—å æ€»å†…å­˜çš„40%ï¼Œæœ€å¤§èµ„æºå®¹é‡å æ€»èµ„æº60%ï¼Œhiveé˜Ÿåˆ—å æ€»å†…å­˜çš„60%ï¼Œæœ€å¤§èµ„æºå®¹é‡å æ€»èµ„æº80%ã€‚

+ é…ç½®é˜Ÿåˆ—ä¼˜å…ˆçº§





#### é…ç½®å¤šé˜Ÿåˆ—çš„å®¹é‡è°ƒåº¦å™¨



+ åœ¨capacity-scheduler.xmlä¸­é…ç½®å¦‚ä¸‹ï¼š

  + ä¿®æ”¹å¦‚ä¸‹é…ç½®

    ~~~xml
    ğŸ‚<!-- æŒ‡å®šå¤šé˜Ÿåˆ—ï¼Œå¢åŠ hiveé˜Ÿåˆ— -->
    <property>
        <name>yarn.scheduler.capacity.root.queues</name>
        <value>default,hive</value>
        <description>
          The queues at the this level (root is the root queue).
        </description>
    </property>
    
    ğŸ‚<!-- é™ä½defaulté˜Ÿåˆ—èµ„æºé¢å®šå®¹é‡ä¸º40%ï¼Œé»˜è®¤100% -->
    <property>
        <name>yarn.scheduler.capacity.root.default.capacity</name>
        <value>40</value>
    </property>
    
    ğŸ‚<!-- é™ä½defaulté˜Ÿåˆ—èµ„æºæœ€å¤§å®¹é‡ä¸º60%ï¼Œé»˜è®¤100% -->
    <property>
        <name>yarn.scheduler.capacity.root.default.maximum-capacity</name>
        <value>60</value>
    </property>
    
    ~~~

    

  + ä¸ºæ–°åŠ é˜Ÿåˆ—æ·»åŠ å¿…è¦å±æ€§ï¼š

    ~~~xml
    ğŸ‘‰<!-- æŒ‡å®šhiveé˜Ÿåˆ—çš„èµ„æºé¢å®šå®¹é‡ -->
    <property>
        <name>yarn.scheduler.capacity.root.hive.capacity</name>
        <value>60</value>
    </property>
    
    ğŸ‘‰<!-- ç”¨æˆ·æœ€å¤šå¯ä»¥ä½¿ç”¨é˜Ÿåˆ—å¤šå°‘èµ„æºï¼Œ1è¡¨ç¤º -->
    <property>
        <name>yarn.scheduler.capacity.root.hive.user-limit-factor</name>
        <value>1</value>
    </property>
    
    ğŸ‘‰<!-- æŒ‡å®šhiveé˜Ÿåˆ—çš„èµ„æºæœ€å¤§å®¹é‡ -->
    <property>
        <name>yarn.scheduler.capacity.root.hive.maximum-capacity</name>
        <value>80</value>
    </property>
    
    ğŸ‘‰<!-- å¯åŠ¨hiveé˜Ÿåˆ— -->
    <property>
        <name>yarn.scheduler.capacity.root.hive.state</name>
        <value>RUNNING</value>
    </property>
    
    ğŸ‘‰<!-- å“ªäº›ç”¨æˆ·æœ‰æƒå‘é˜Ÿåˆ—æäº¤ä½œä¸š -->
    <property>
        <name>yarn.scheduler.capacity.root.hive.acl_submit_applications</name>
        <value>*</value>
    </property>
    
    ğŸ‘‰<!-- å“ªäº›ç”¨æˆ·æœ‰æƒæ“ä½œé˜Ÿåˆ—ï¼Œç®¡ç†å‘˜æƒé™ï¼ˆæŸ¥çœ‹/æ€æ­»ï¼‰ -->
    <property>
        <name>yarn.scheduler.capacity.root.hive.acl_administer_queue</name>
        <value>*</value>
    </property>
    
    ğŸ‘‰<!-- å“ªäº›ç”¨æˆ·æœ‰æƒé…ç½®æäº¤ä»»åŠ¡ä¼˜å…ˆçº§ -->
    <property>
        <name>yarn.scheduler.capacity.root.hive.acl_application_max_priority</name>
        <value>*</value>
    </property>
    
    ğŸ‘‰<!-- ä»»åŠ¡çš„è¶…æ—¶æ—¶é—´è®¾ç½®ï¼šyarn application -appId appId -updateLifetime Timeout
    å‚è€ƒèµ„æ–™ï¼šhttps://blog.cloudera.com/enforcing-application-lifetime-slas-yarn/ -->
    
    ğŸ‘‰<!-- å¦‚æœapplicationæŒ‡å®šäº†è¶…æ—¶æ—¶é—´ï¼Œåˆ™æäº¤åˆ°è¯¥é˜Ÿåˆ—çš„applicationèƒ½å¤ŸæŒ‡å®šçš„æœ€å¤§è¶…æ—¶æ—¶é—´ä¸èƒ½è¶…è¿‡è¯¥å€¼ã€‚ 
    -->
    <property>
        <name>yarn.scheduler.capacity.root.hive.maximum-application-lifetime</name>
        <value>-1</value>
    </property>
    
    ğŸ‘‰<!-- å¦‚æœapplicationæ²¡æŒ‡å®šè¶…æ—¶æ—¶é—´ï¼Œåˆ™ç”¨default-application-lifetimeä½œä¸ºé»˜è®¤å€¼ -->
    <property>
        <name>yarn.scheduler.capacity.root.hive.default-application-lifetime</name>
        <value>-1</value>
    </property>
    
    ~~~



+ åˆ†å‘é…ç½®æ–‡ä»¶

+ é‡å¯Yarnæˆ–è€…æ‰§è¡Œyarn rmadmin -refreshQueuesåˆ·æ–°é˜Ÿåˆ—ï¼Œå°±å¯ä»¥çœ‹åˆ°ä¸¤æ¡é˜Ÿåˆ—ï¼š

  `yarn rmadmin -refreshQueues`



#### å‘Hiveé˜Ÿåˆ—æäº¤ä»»åŠ¡



**1ï¼‰**hadoop jarçš„æ–¹å¼

~~~xml
$hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -D mapreduce.job.queuename=hive /input /output
~~~

==-Dè¡¨ç¤ºè¿è¡Œæ—¶æ”¹å˜å‚æ•°å€¼==



**2ï¼‰**æ‰“jaråŒ…çš„æ–¹å¼

**é»˜è®¤çš„ä»»åŠ¡æäº¤éƒ½æ˜¯æäº¤åˆ°defaulté˜Ÿåˆ—çš„**ã€‚å¦‚æœå¸Œæœ›å‘å…¶ä»–é˜Ÿåˆ—æäº¤ä»»åŠ¡ï¼Œéœ€è¦åœ¨Driverä¸­å£°æ˜ï¼š

~~~java
public class WcDrvier {

    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {

        Configuration conf = new Configuration();
		ğŸ“Œ
        conf.set("mapreduce.job.queuename","hive");

        //1. è·å–ä¸€ä¸ªJobå®ä¾‹
        Job job = Job.getInstance(conf);

        ã€‚ã€‚ã€‚ ã€‚ã€‚ã€‚

        //6. æäº¤Job
        boolean b = job.waitForCompletion(true);
        System.exit(b ? 0 : 1);
    }
}
~~~



#### ä»»åŠ¡ä¼˜å…ˆçº§

> å®¹é‡è°ƒåº¦å™¨ï¼Œæ”¯æŒä»»åŠ¡ä¼˜å…ˆçº§çš„é…ç½®ï¼Œåœ¨èµ„æºç´§å¼ æ—¶ï¼Œä¼˜å…ˆçº§é«˜çš„ä»»åŠ¡å°†ä¼˜å…ˆè·å–èµ„æºã€‚**é»˜è®¤æƒ…å†µï¼ŒYarnå°†æ‰€æœ‰ä»»åŠ¡çš„ä¼˜å…ˆçº§é™åˆ¶ä¸º0ï¼Œ**è‹¥æƒ³ä½¿ç”¨ä»»åŠ¡çš„ä¼˜å…ˆçº§åŠŸèƒ½ï¼Œé¡»å¼€æ”¾è¯¥é™åˆ¶ã€‚
>



ğŸŒ´	ä¿®æ”¹yarn-site.xmlæ–‡ä»¶ï¼Œå¢åŠ ä»¥ä¸‹å‚æ•°

~~~xml
<property>
    <name>yarn.cluster.max-application-priority</name>
    <value>5</value>
</property>
~~~



ğŸŒ´	åˆ†å‘é…ç½®ï¼Œå¹¶é‡å¯Yarn	

`xsync yarn-site.xml`

`sbin/stop-yarn.sh`

`sbin/start-yarn.sh`





ğŸŒ´	æäº¤ä¼˜å…ˆçº§é«˜çš„ä»»åŠ¡

~~~sh
[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi  -D mapreduce.job.priority=5 5 2000000
~~~



ğŸŒ´  ä¹Ÿå¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤**ä¿®æ”¹æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡çš„ä¼˜å…ˆçº§ã€‚**

`yarn application -appID <ApplicationID> -updatePriority ä¼˜å…ˆçº§`



ä¾‹ï¼š

~~~shell
[atguigu@hadoop102 hadoop-3.1.3]$ yarn application -appID application_1611133087930_0009 -updatePriority 5
~~~





### å…¬å¹³è°ƒåº¦å™¨æ¡ˆä¾‹



##### éœ€æ±‚

åˆ›å»ºä¸¤ä¸ªé˜Ÿåˆ—ï¼Œåˆ†åˆ«æ˜¯testå’Œatguiguï¼ˆä»¥ç”¨æˆ·æ‰€å±ç»„å‘½åï¼‰ã€‚æœŸæœ›å®ç°ä»¥ä¸‹æ•ˆæœï¼š==è‹¥ç”¨æˆ·æäº¤ä»»åŠ¡æ—¶æŒ‡å®šé˜Ÿåˆ—ï¼Œåˆ™ä»»åŠ¡æäº¤åˆ°æŒ‡å®šé˜Ÿåˆ—è¿è¡Œï¼›è‹¥æœªæŒ‡å®šé˜Ÿåˆ—ï¼Œ==testç”¨æˆ·æäº¤çš„ä»»åŠ¡åˆ°==root.group.test==é˜Ÿåˆ—è¿è¡Œï¼Œatguiguæäº¤çš„ä»»åŠ¡åˆ°==root.group.atguigu==é˜Ÿåˆ—è¿è¡Œï¼ˆæ³¨ï¼šgroupä¸ºç”¨æˆ·æ‰€å±ç»„ï¼‰ã€‚

å…¬å¹³è°ƒåº¦å™¨çš„é…ç½®æ¶‰åŠåˆ°ä¸¤ä¸ªæ–‡ä»¶ï¼Œä¸€ä¸ªæ˜¯==**yarn-site.xml**==ï¼Œå¦ä¸€ä¸ªæ˜¯å…¬å¹³è°ƒåº¦å™¨é˜Ÿåˆ—åˆ†é…æ–‡ä»¶==fair-scheduler.xml==ï¼ˆæ–‡ä»¶åå¯è‡ªå®šä¹‰ï¼‰ã€‚

ï¼ˆ1ï¼‰é…ç½®æ–‡ä»¶å‚è€ƒèµ„æ–™ï¼š

https://hadoop.apache.org/docs/r3.1.3/hadoop-yarn/hadoop-yarn-site/FairScheduler.html

ï¼ˆ2ï¼‰ä»»åŠ¡é˜Ÿåˆ—æ”¾ç½®è§„åˆ™å‚è€ƒèµ„æ–™ï¼š

https://blog.cloudera.com/untangling-apache-hadoop-yarn-part-4-fair-scheduler-queue-basics/



#### é…ç½®å¤šé˜Ÿåˆ—çš„å…¬å¹³è°ƒåº¦å™¨



**1ï¼‰**ä¿®æ”¹yarn-site.xmlæ–‡ä»¶ï¼Œ**åŠ å…¥ä»¥ä¸‹å‚æ•°**



~~~xml
<property>
    <name>yarn.resourcemanager.scheduler.class</name>
   											ğŸ‘‡ <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>
    <description>é…ç½®ä½¿ç”¨å…¬å¹³è°ƒåº¦å™¨</description>
</property>

<property>
    <name>yarn.scheduler.fair.allocation.file</name>	ğŸ‘‡ 
    <value>/opt/module/hadoop-3.1.3/etc/hadoop/fair-scheduler.xml</value>
    <description>æŒ‡æ˜å…¬å¹³è°ƒåº¦å™¨é˜Ÿåˆ—åˆ†é…é…ç½®æ–‡ä»¶</description>
</property>

<property>
    <name>yarn.scheduler.fair.preemption</name>
  ğŸ‘‰<value>false</value>
    <description>ç¦æ­¢é˜Ÿåˆ—é—´èµ„æºæŠ¢å </description>
</property>

~~~



**2ï¼‰**é…ç½®fair-scheduler.xml

~~~xml
<?xml version="1.0"?>
<allocations>
  <!-- å•ä¸ªé˜Ÿåˆ—ä¸­Application Masterå ç”¨èµ„æºçš„æœ€å¤§æ¯”ä¾‹,å–å€¼0-1 ï¼Œä¼ä¸šä¸€èˆ¬é…ç½®0.1 -->
  <queueMaxAMShareDefault>0.5</queueMaxAMShareDefault>
  <!-- å•ä¸ªé˜Ÿåˆ—æœ€å¤§èµ„æºçš„é»˜è®¤å€¼ test atguigu default -->
  <queueMaxResourcesDefault>4096mb,4vcores</queueMaxResourcesDefault>

  <!-- å¢åŠ ä¸€ä¸ªé˜Ÿåˆ—test -->
  <queue name="test">
    <!-- é˜Ÿåˆ—æœ€å°èµ„æº -->
    <minResources>2048mb,2vcores</minResources>
    <!-- é˜Ÿåˆ—æœ€å¤§èµ„æº -->
    <maxResources>4096mb,4vcores</maxResources>
    <!-- é˜Ÿåˆ—ä¸­æœ€å¤šåŒæ—¶è¿è¡Œçš„åº”ç”¨æ•°ï¼Œé»˜è®¤50ï¼Œæ ¹æ®çº¿ç¨‹æ•°é…ç½® -->
    <maxRunningApps>4</maxRunningApps>
    <!-- é˜Ÿåˆ—ä¸­Application Masterå ç”¨èµ„æºçš„æœ€å¤§æ¯”ä¾‹ -->
    <maxAMShare>0.5</maxAMShare>
    <!-- è¯¥é˜Ÿåˆ—èµ„æºæƒé‡,é»˜è®¤å€¼ä¸º1.0 -->
    <weight>1.0</weight>
    <!-- é˜Ÿåˆ—å†…éƒ¨çš„èµ„æºåˆ†é…ç­–ç•¥ -->
    <schedulingPolicy>fair</schedulingPolicy>
  </queue>
  <!-- å¢åŠ ä¸€ä¸ªé˜Ÿåˆ—atguigu -->
  <queue name="atguigu" type="parent">
    <!-- é˜Ÿåˆ—æœ€å°èµ„æº -->
    <minResources>2048mb,2vcores</minResources>
    <!-- é˜Ÿåˆ—æœ€å¤§èµ„æº -->
    <maxResources>4096mb,4vcores</maxResources>
    <!-- é˜Ÿåˆ—ä¸­æœ€å¤šåŒæ—¶è¿è¡Œçš„åº”ç”¨æ•°ï¼Œé»˜è®¤50ï¼Œæ ¹æ®çº¿ç¨‹æ•°é…ç½® -->
    <maxRunningApps>4</maxRunningApps>
    <!-- é˜Ÿåˆ—ä¸­Application Masterå ç”¨èµ„æºçš„æœ€å¤§æ¯”ä¾‹ -->
    <maxAMShare>0.5</maxAMShare>
    <!-- è¯¥é˜Ÿåˆ—èµ„æºæƒé‡,é»˜è®¤å€¼ä¸º1.0 -->
    <weight>1.0</weight>
    <!-- é˜Ÿåˆ—å†…éƒ¨çš„èµ„æºåˆ†é…ç­–ç•¥ -->
    <schedulingPolicy>fair</schedulingPolicy>
  </queue>

  <!-- ä»»åŠ¡é˜Ÿåˆ—åˆ†é…ç­–ç•¥,å¯é…ç½®å¤šå±‚è§„åˆ™,ä»ç¬¬ä¸€ä¸ªè§„åˆ™å¼€å§‹åŒ¹é…,ç›´åˆ°åŒ¹é…æˆåŠŸ -->
  <queuePlacementPolicy>
    <!-- æäº¤ä»»åŠ¡æ—¶æŒ‡å®šé˜Ÿåˆ—,å¦‚æœªæŒ‡å®šæäº¤é˜Ÿåˆ—,åˆ™ç»§ç»­åŒ¹é…ä¸‹ä¸€ä¸ªè§„åˆ™; falseè¡¨ç¤ºï¼šå¦‚æœæŒ‡å®šé˜Ÿåˆ—ä¸å­˜åœ¨,ä¸å…è®¸è‡ªåŠ¨åˆ›å»º-->
    <rule name="specified" create="false"/>
    <!-- æäº¤åˆ°root.group.usernameé˜Ÿåˆ—,è‹¥root.groupä¸å­˜åœ¨,ä¸å…è®¸è‡ªåŠ¨åˆ›å»ºï¼›è‹¥root.group.userä¸å­˜åœ¨,å…è®¸è‡ªåŠ¨åˆ›å»º -->
    <rule name="nestedUserQueue" create="true">
        <rule name="primaryGroup" create="false"/>
    </rule>
    ğŸš©<!-- æœ€åä¸€ä¸ªè§„åˆ™å¿…é¡»ä¸ºrejectæˆ–è€…defaultã€‚Rejectè¡¨ç¤ºæ‹’ç»åˆ›å»ºæäº¤å¤±è´¥ï¼Œdefaultè¡¨ç¤ºæŠŠä»»åŠ¡æäº¤åˆ°defaulté˜Ÿåˆ— -->
    <rule name="reject" />
  </queuePlacementPolicy>
</allocations>

~~~



**3ï¼‰**åˆ†å‘é…ç½®å¹¶é‡å¯Yarn





### Yarnçš„Toolæ¥å£æ¡ˆä¾‹

------



**èƒŒæ™¯**

~~~shell
[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar wc.jar com.atguigu.mapreduce.wordcount2.WordCountDriver -Dmapreduce.job.queuename=root.test /input /output1
#æœŸæœ›å¯ä»¥åŠ¨æ€ä¼ å‚ï¼Œç»“æœæŠ¥é”™ï¼Œè¯¯è®¤ä¸ºæ˜¯ç¬¬ä¸€ä¸ªè¾“å…¥å‚æ•°ã€‚
~~~



==wordcountç¨‹åºä¸­æŒ‡å®šargs[0]ä¸ºè¾“å…¥è·¯å¾„  ï¼Œargs[1]è¾“å‡ºè·¯å¾„ï¼Œæ— æ³•åŠ¨æ€ä¼ å‚==



**è§£å†³æ–¹æ¡ˆ**

è‡ªå·±å†™çš„ç¨‹åºä¸ºå¯ä»¥åŠ¨æ€ä¿®æ”¹å‚æ•°ã€‚ç¼–å†™Yarnçš„Toolæ¥å£ã€‚





**ä¸šåŠ¡å®ç°**



ï¼ˆ1ï¼‰æ–°å»ºMavené¡¹ç›®YarnDemoï¼Œpomå¦‚ä¸‹ï¼š

~~~xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.atguigu.hadoop</groupId>
    <artifactId>yarn_tool_test</artifactId>
    <version>1.0-SNAPSHOT</version>

    <dependencies>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
            <version>3.1.3</version>
        </dependency>
    </dependencies>
</project>

~~~

ï¼ˆ2ï¼‰æ–°å»ºcom.atguigu.yarnæŠ¥å

ï¼ˆ3ï¼‰åˆ›å»ºç±»WordCountå¹¶å®ç°Toolæ¥å£ï¼š

~~~java
public class WordCount implements Tool {

    private Configuration conf;

   ğŸš©@Override
    public int run(String[] args) throws Exception {

        Job job = Job.getInstance(conf);

        job.setJarByClass(WordCountDriver.class);

        job.setMapperClass(WordCountMapper.class);
        job.setReducerClass(WordCountReducer.class);

        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(IntWritable.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        FileInputFormat.setInputPaths(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        return job.waitForCompletion(true) ? 0 : 1;
    }

   ğŸš© @Override
    public void setConf(Configuration conf) {
        this.conf = conf;
    }

    ğŸš©@Override
    public Configuration getConf() {
        return conf;
    }
		
    //ä¸‹é¢ä¸º  mapreduceæ–¹æ³•ğŸ‘‡
    public static class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {

        private Text outK = new Text();
        private IntWritable outV = new IntWritable(1);

        @Override
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {

            String line = value.toString();
            String[] words = line.split(" ");

            for (String word : words) {
                outK.set(word);

                context.write(outK, outV);
            }
        }
    }

    public static class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
        private IntWritable outV = new IntWritable();

        @Override
        protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {

            int sum = 0;

            for (IntWritable value : values) {
                sum += value.get();
            }
            outV.set(sum);

            context.write(key, outV);
        }
    }
}
~~~



ï¼ˆ4ï¼‰æ–°å»ºWordCountDriver



~~~java
public class WordCountDriver {

    private static Tool tool;

    public static void main(String[] args) throws Exception {
        // 1. åˆ›å»ºé…ç½®æ–‡ä»¶
        Configuration conf = new Configuration();

        // 2. åˆ¤æ–­æ˜¯å¦æœ‰toolæ¥å£
        switch (args[0]){
            case "wordcount":
                tool = new WordCount();
                break;
            default:
                throw new RuntimeException(" No such tool: "+ args[0] );
        }
        // 3. ç”¨Toolæ‰§è¡Œç¨‹åº
        // Arrays.copyOfRange å°†è€æ•°ç»„çš„å…ƒç´ æ”¾åˆ°æ–°æ•°ç»„é‡Œé¢
        int run = ToolRunner.run(conf, tool, Arrays.copyOfRange(args, 1, args.length));

        System.exit(run);
    }
}

~~~



==æ³¨æ„ï¼šç¬¬ä¸€ä¸ªargsä¸ºæ¥å£çš„ï¼Œ       Toolå¯¹è±¡çš„argsæ‰æ˜¯å¯¹åŠ¨æ€å‚æ•°çš„è¿‡æ»¤==



**3ï¼‰**åœ¨HDFSä¸Šå‡†å¤‡è¾“å…¥æ–‡ä»¶ï¼Œå‡è®¾ä¸º/inputç›®å½•ï¼Œå‘é›†ç¾¤æäº¤è¯¥JaråŒ…

~~~shell
[atguigu@hadoop102 hadoop-3.1.3]$ yarn jar YarnDemo.jar com.atguigu.yarn.WordCountDriver wordcount /input /output
~~~



==æ³¨æ„==ï¼šæ­¤æ—¶æäº¤çš„3ä¸ªå‚æ•°ï¼Œ**ç¬¬ä¸€ä¸ªç”¨äºç”Ÿæˆç‰¹å®šçš„Toolï¼Œç¬¬äºŒä¸ªå’Œç¬¬ä¸‰ä¸ªä¸ºè¾“å…¥è¾“å‡ºç›®å½•ã€‚**æ­¤æ—¶å¦‚æœæˆ‘ä»¬å¸Œæœ›åŠ å…¥è®¾ç½®å‚æ•°ï¼Œå¯ä»¥åœ¨wordcountåé¢æ·»åŠ å‚æ•°ï¼Œ

~~~sh
[atguigu@hadoop102 hadoop-3.1.3]$ yarn jar YarnDemo.jar com.atguigu.yarn.WordCountDriver wordcount -Dmapreduce.job.queuename=root.test /input /output1
~~~



## ä¸‰ã€æ€»ç»“



#### 1ã€Yarnçš„å·¥ä½œæœºåˆ¶ï¼ˆé¢è¯•é¢˜ï¼‰

#### 	2ã€Yarnçš„è°ƒåº¦å™¨

  + **FIFO/å®¹é‡/å…¬å¹³**

  + **apache é»˜è®¤è°ƒåº¦å™¨  å®¹é‡ï¼› CDHé»˜è®¤è°ƒåº¦å™¨ å…¬å¹³**

  + **å…¬å¹³/å®¹é‡é»˜è®¤ä¸€ä¸ªdefault ï¼Œéœ€è¦åˆ›å»ºå¤šé˜Ÿåˆ—**

  + **ä¸­å°ä¼ä¸šï¼šhive  spark flink  mr**  

  + **ä¸­å¤§ä¼ä¸šï¼šä¸šåŠ¡æ¨¡å—ï¼šç™»å½•/æ³¨å†Œ/è´­ç‰©è½¦/è¥é”€**

  + **å¥½å¤„ï¼šè§£è€¦  é™ä½é£é™©  11.11  6.18  é™çº§ä½¿ç”¨**

  + **æ¯ä¸ªè°ƒåº¦å™¨ç‰¹ç‚¹ï¼š**

    + **ç›¸åŒç‚¹ï¼šæ”¯æŒå¤šé˜Ÿåˆ—ï¼Œå¯ä»¥å€Ÿèµ„æºï¼Œæ”¯æŒå¤šç”¨æˆ·**

    + **ä¸åŒç‚¹ï¼šå®¹é‡è°ƒåº¦å™¨ï¼šä¼˜å…ˆæ»¡è¶³å…ˆè¿›æ¥çš„ä»»åŠ¡æ‰§è¡Œ**
      				**å…¬å¹³è°ƒåº¦å™¨ï¼Œåœ¨é˜Ÿåˆ—é‡Œé¢çš„ä»»åŠ¡å…¬å¹³äº«æœ‰é˜Ÿåˆ—èµ„æº**

+ **ç”Ÿäº§ç¯å¢ƒæ€ä¹ˆé€‰ï¼š**
        		**ä¸­å°ä¼ä¸šï¼Œå¯¹å¹¶å‘åº¦è¦æ±‚ä¸é«˜ï¼Œé€‰æ‹©å®¹é‡**
                		**ä¸­å¤§ä¼ä¸šï¼Œå¯¹å¹¶å‘åº¦è¦æ±‚æ¯”è¾ƒé«˜ï¼Œé€‰æ‹©å…¬å¹³ã€‚**

#### 3ã€å¼€å‘éœ€è¦é‡ç‚¹æŒæ¡ï¼š

 + **é˜Ÿåˆ—è¿è¡ŒåŸç†**	
 + **Yarnå¸¸ç”¨å‘½ä»¤**
 + **æ ¸å¿ƒå‚æ•°é…ç½®**
 + **é…ç½®å®¹é‡è°ƒåº¦å™¨å’Œå…¬å¹³è°ƒåº¦å™¨ã€‚**
 + **toolæ¥å£ä½¿ç”¨ã€‚**











































