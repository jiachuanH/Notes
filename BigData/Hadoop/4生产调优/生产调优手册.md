# ä¸€ã€HDFS

## æ ¸å¿ƒå‚æ•°

### NameNodeå†…å­˜ç”Ÿäº§é…ç½®



**ğŸŒ´NameNodeå†…å­˜è®¡ç®—**

â€‹    æ¯ä¸ªæ–‡ä»¶å—å¤§æ¦‚å ç”¨150byteï¼Œä¸€å°æœåŠ¡å™¨128Gå†…å­˜ä¸ºä¾‹ï¼Œèƒ½å­˜å‚¨å¤šå°‘æ–‡ä»¶å—å‘¢ï¼Ÿ

â€‹    128 * 1024 * 1024 * 1024 / 150Byte â‰ˆ **9.1äº¿**

â€‹     G       MB      KB       Byte





**ğŸŒ´Hadoop2.xç³»åˆ—ï¼Œé…ç½®NameNodeå†…å­˜**

â€‹    NameNodeå†…å­˜é»˜è®¤2000mï¼Œå¦‚æœæœåŠ¡å™¨å†…å­˜4Gï¼ŒNameNodeå†…å­˜å¯ä»¥é…ç½®3gã€‚åœ¨hadoop-env.shæ–‡ä»¶ä¸­é…ç½®å¦‚ä¸‹ã€‚

`HADOOP_NAMENODE_OPTS=-Xmx3072m`





==**ğŸŒ´Hadoop3.xç³»åˆ—ï¼Œé…ç½®NameNodeå†…å­˜**==



+ hadoop-env.sh**ä¸­æè¿°Hadoopçš„å†…å­˜æ˜¯åŠ¨æ€åˆ†é…çš„

+ æŸ¥çœ‹NameNodeå’ŒDataNodeå ç”¨å†…å­˜

  + ~~~sh
    [atguigu@hadoop102 ~]$ jps
    
        3088 NodeManager
        ğŸ‘‡
        2611 NameNode
        3271 JobHistoryServer
        
      ğŸ‘‰2744 DataNode
        3579 Jps
    									ğŸ‘‡
    [atguigu@hadoop102 ~]$ jmap -heap 2611
    
    Heap Configuration:					
       MaxHeapSize              = 1031798784 (984.0MB)
       
       
    [atguigu@hadoop102 ~]$ jmap -heap ğŸ‘‰2744
    Heap Configuration:
       MaxHeapSize              = 1031798784 (984.0MB)
    
    ~~~



ğŸ§NameNodeå’ŒDataNodeå ç”¨å†…å­˜éƒ½æ˜¯**è‡ªåŠ¨åˆ†é…**çš„ï¼Œä¸”ç›¸ç­‰ã€‚ä¸æ˜¯å¾ˆåˆç†ã€‚



â€‹	==CDHæä¾›äº†å†…å­˜è®¾ç½®çš„å‚è€ƒä¿¡æ¯==				åŸæ–‡å¦‚ä¸‹ğŸ‘‡

https://docs.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_hardware_requirements.html#concept_fzz_dq4_gbb



==NameNode==

- ==æœ€ä½ï¼š1 GBï¼ˆç”¨äºæ¦‚å¿µéªŒè¯éƒ¨ç½²ï¼‰==
- ==æ¯å¢åŠ  1,000,000 ä¸ªå—å°±å¢åŠ  1 GB==[^PS]:å¢åŠ çš„å—æ˜¯å¯¹äºæ‰€æœ‰DNçš„å—æ¥è¯´



==DataNode==

+ ==æœ€å°ï¼š4 GB  å‰¯æœ¬æˆ–æ•°å—æ•°å¢åŠ éƒ½åº”è¯¥å¢åŠ å†…å­˜==

+ ==ä½äº400ä¸‡ä¸ªå‰¯æœ¬è°ƒä¸º4G==

+ ==æ¯è¶…è¿‡ 400 ä¸‡ä¸ªå‰¯æœ¬ï¼Œæ¯ 100 ä¸‡ä¸ªå‰¯æœ¬å¢åŠ  1 GB å†…å­˜ã€‚==





å…·ä½“ä¿®æ”¹ï¼šhadoop-env.sh

+ æœ«å°¾**å¢åŠ **å¦‚ä¸‹å†…å®¹

  + ~~~sh
    export HDFS_NAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS -Xmx1024m"
    
    export HDFS_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS -Xmx1024m"
    ~~~







### NameNodeå¿ƒè·³å¹¶å‘é…ç½®



![image-20220114163457230](../image/image-20220114163457230.png)





==ä¼ä¸šç»éªŒ==ï¼š												ğŸ‘‡



[^Cluster Size]:DataNodeå°æ•°

$$
çº¿ç¨‹æ•°ï¼šdfs.namenode.handler.count=20 \times log_e^{Cluster Size}
$$



ğŸ”¥é€šè¿‡ç®€å•çš„pythonä»£ç è®¡ç®—è¯¥å€¼

~~~sh
[atguigu@hadoop102 ~]$ sudo yum install -y python
[atguigu@hadoop102 ~]$ python
Python 2.7.5 (default, Apr 11 2018, 07:36:10) 
[GCC 4.8.5 20150623 (Red Hat 4.8.5-28)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> import math
>>> print int(20*math.log(3))
21
>>> quit()
~~~



### å¼€å¯å›æ”¶ç«™é…ç½®



> å¼€å¯å›æ”¶ç«™åŠŸèƒ½ï¼Œå¯ä»¥å°†åˆ é™¤çš„æ–‡ä»¶åœ¨ä¸è¶…æ—¶çš„æƒ…å†µä¸‹ï¼Œæ¢å¤åŸæ•°æ®ï¼Œèµ·åˆ°é˜²æ­¢è¯¯åˆ é™¤ã€å¤‡ä»½ç­‰ä½œç”¨ã€‚



![image-20220114165126518](../image/image-20220114165126518.png)



**å‚æ•°è¯´æ˜**

ï¼ˆ1ï¼‰é»˜è®¤å€¼fs.trash.interval = 0ï¼Œ**0è¡¨ç¤ºç¦ç”¨å›æ”¶ç«™**ï¼›å…¶ä»–å€¼è¡¨ç¤ºè®¾ç½®æ–‡ä»¶çš„å­˜æ´»æ—¶é—´ã€‚

ï¼ˆ2ï¼‰é»˜è®¤å€¼fs.trash.checkpoint.interval = 0ï¼Œæ£€æŸ¥å›æ”¶ç«™çš„é—´éš”æ—¶é—´ã€‚**å¦‚æœè¯¥å€¼ä¸º0ï¼Œåˆ™è¯¥å€¼è®¾ç½®å’Œfs.trash.intervalçš„å‚æ•°å€¼ç›¸ç­‰ã€‚**

ï¼ˆ3ï¼‰è¦æ±‚fs.trash.checkpoint.interval <= fs.trash.intervalã€‚



==**å¯ç”¨å›æ”¶ç«™**==

ä¿®æ”¹core-site.xmlï¼Œé…ç½®åƒåœ¾å›æ”¶æ—¶é—´ä¸º1åˆ†é’Ÿã€‚

~~~xml
<property>
    <name>fs.trash.interval</name>
    <value>1</value>
</property>
~~~



**æŸ¥çœ‹å›æ”¶ç«™**

å›æ”¶ç«™ç›®å½•åœ¨HDFSé›†ç¾¤ä¸­çš„è·¯å¾„ï¼š/user/atguigu/.Trash/â€¦.



==é€šè¿‡ç½‘é¡µä¸Šç›´æ¥åˆ é™¤çš„æ–‡ä»¶ä¹Ÿä¸ä¼šèµ°å›æ”¶ç«™ã€‚==



ğŸŒ´**é€šè¿‡ç¨‹åºåˆ é™¤çš„æ–‡ä»¶ä¸ä¼šç»è¿‡å›æ”¶ç«™ï¼Œéœ€è¦è°ƒç”¨moveToTrash()æ‰è¿›å…¥å›æ”¶ç«™**

~~~java
Trash trash = New Trash(conf);
trash.moveToTrash(path);
~~~



ğŸŒ´**åªæœ‰åœ¨å‘½ä»¤è¡Œåˆ©ç”¨hadoop fs -rmå‘½ä»¤åˆ é™¤çš„æ–‡ä»¶æ‰ä¼šèµ°å›æ”¶ç«™ã€‚**



ğŸŒ´**æ¢å¤å›æ”¶ç«™æ•°æ®**

`hadoop fs -mv åƒåœ¾ç«™è·¯å¾„ æ–°è·¯å¾„` 





## é›†ç¾¤å‹æµ‹



> ä¸ºäº†ææ¸…æ¥šHDFSçš„è¯»å†™æ€§èƒ½ï¼Œç”Ÿäº§ç¯å¢ƒä¸Šéå¸¸éœ€è¦å¯¹é›†ç¾¤è¿›è¡Œå‹æµ‹





![image-20220114165803865](../image/image-20220114165803865.png)



ğŸŒ´HDFSçš„è¯»å†™æ€§èƒ½ä¸»è¦å—**ç½‘ç»œå’Œç£ç›˜**å½±å“æ¯”è¾ƒå¤§ã€‚ä¸ºäº†æ–¹ä¾¿æµ‹è¯•ï¼Œå°†hadoop102ã€hadoop103ã€hadoop104è™šæ‹Ÿæœºç½‘ç»œéƒ½è®¾ç½®ä¸º100mbpsã€‚





![image-20220114165827047](../image/image-20220114165827047.png)





### æµ‹è¯•HDFSå†™æ€§èƒ½

------

**åº•å±‚åŸç†**

![image-20220114170504722](../image/image-20220114170504722.png)



ğŸ”¥

==æµ‹è¯•æ–‡ä»¶ä¸ªæ•°=é›†ç¾¤CPUæ€»æ ¸æ•°-1==

==Throughhput=æ‰€æœ‰æ•°æ®é‡ç´¯åŠ /æ€»æ—¶é—´==

==Average IO rate=ï¼ˆmap1çš„å¹³å‡é€Ÿåº¦+ã€‚ã€‚ã€‚ã€‚ã€‚+map11çš„å¹³å‡é€Ÿåº¦ï¼‰==





ğŸŒ´åœ¨[^PS]:**/opt/module/hadoop-3.1.3/etc/hadoop/yarn-site.xml**ä¸­è®¾ç½®è™šæ‹Ÿå†…å­˜æ£€æµ‹ä¸º**false**

~~~xml
<!--æ˜¯å¦å¯åŠ¨ä¸€ä¸ªçº¿ç¨‹æ£€æŸ¥æ¯ä¸ªä»»åŠ¡æ­£ä½¿ç”¨çš„è™šæ‹Ÿå†…å­˜é‡ï¼Œå¦‚æœä»»åŠ¡è¶…å‡ºåˆ†é…å€¼ï¼Œåˆ™ç›´æ¥å°†å…¶æ€æ‰ï¼Œé»˜è®¤æ˜¯true -->
<property>
     <name>yarn.nodemanager.vmem-check-enabled</name>
     <value>false</value>
</property>

~~~



ğŸŒ´åˆ†å‘é…ç½®å¹¶é‡å¯Yarné›†ç¾¤



> æµ‹è¯•å†…å®¹ï¼šå‘HDFSé›†ç¾¤å†™10ä¸ª128Mçš„æ–‡ä»¶

~~~sh
[atguigu@hadoop102 mapreduce]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 128MB

# ğŸ€æ³¨æ„ï¼šnrFiles nä¸ºç”ŸæˆmapTaskçš„æ•°é‡ï¼Œç”Ÿäº§ç¯å¢ƒä¸€èˆ¬å¯é€šè¿‡hadoop103:8088æŸ¥çœ‹CPUæ ¸æ•°ï¼Œè®¾ç½®ä¸ºï¼ˆCPUæ ¸æ•° -  1ï¼‰


2022-01-14 17:55:49,153 INFO fs.TestDFSIO: ----- TestDFSIO ----- : write
2022-01-14 17:55:49,154 INFO fs.TestDFSIO:             Date & time: Fri Jan 14 17:55:49 CST 2022
2022-01-14 17:55:49,154 INFO fs.TestDFSIO:         Number of files: 10
2022-01-14 17:55:49,154 INFO fs.TestDFSIO: 	 Total MBytes processed: 1280
2022-01-14 17:55:49,154 INFO fs.TestDFSIO: ğŸ‘‰      Throughput mb/sec: 1.5  	ğŸ‘ˆ
2022-01-14 17:55:49,154 INFO fs.TestDFSIO: ğŸ‘‰ Average IO rate mb/sec: 1.54	ğŸ‘ˆ
2022-01-14 17:55:49,154 INFO fs.TestDFSIO:   IO rate std deviation: 0.27
2022-01-14 17:55:49,154 INFO fs.TestDFSIO:      Test exec time sec: 177.44
2022-01-14 17:55:49,154 INFO fs.TestDFSIO: 

~~~



**Ã˜ Number of filesï¼š**ç”ŸæˆmapTaskæ•°é‡ï¼Œä¸€èˆ¬æ˜¯é›†ç¾¤ä¸­ï¼ˆCPUæ ¸æ•°-1ï¼‰ï¼Œæˆ‘ä»¬æµ‹è¯•è™šæ‹Ÿæœºå°±æŒ‰ç…§å®é™…çš„ç‰©ç†å†…å­˜-1åˆ†é…å³å¯

**Ã˜ Total MBytes processedï¼š**å•ä¸ªmapå¤„ç†çš„æ–‡ä»¶å¤§å°

**Ã˜ Throughput mb/sec:**å•ä¸ªmapTakçš„ååé‡ 

â€‹					è®¡ç®—æ–¹å¼ï¼šå¤„ç†çš„**æ€»æ–‡ä»¶å¤§å°**â—**æ¯ä¸€ä¸ªmapTaskå†™æ•°æ®çš„æ—¶é—´ç´¯åŠ **

â€‹					é›†ç¾¤æ•´ä½“ååé‡ï¼š**ç”ŸæˆmapTaskæ•°é‡âœ–å•ä¸ªmapTakçš„ååé‡**

**Ã˜ Average IO rate mb/sec::**å¹³å‡mapTakçš„ååé‡

â€‹				è®¡ç®—æ–¹å¼ï¼šæ¯ä¸ªmapTaskå¤„ç†æ–‡ä»¶å¤§å°/æ¯ä¸€ä¸ªmapTaskå†™æ•°æ®çš„æ—¶é—´ 

  											 å…¨éƒ¨ç›¸åŠ é™¤ä»¥taskæ•°é‡

**Ã˜ IO rate std deviation:**æ–¹å·®ã€åæ˜ å„ä¸ªmapTaskå¤„ç†çš„å·®å€¼ï¼Œè¶Šå°è¶Šå‡è¡¡







> æµ‹è¯•ç»“æœåˆ†æ



â€‹							==ç”±äºå‰¯æœ¬1å°±åœ¨æœ¬åœ°ï¼Œæ‰€ä»¥è¯¥å‰¯æœ¬ä¸å‚ä¸æµ‹è¯•==

ğŸŒ´å‹æµ‹åé€Ÿåº¦ç”¨çš„æ˜¯Throughput

![image-20220114180300900](../image/image-20220114180300900.png)





==å¦‚æœå®¢æˆ·ç«¯ä¸åœ¨é›†ç¾¤èŠ‚ç‚¹ï¼Œé‚£å°±ä¸‰ä¸ªå‰¯æœ¬éƒ½å‚ä¸è®¡ç®—==







### æµ‹è¯•HDFSè¯»æ€§èƒ½

------

> è¯»å–HDFSé›†ç¾¤10ä¸ª128Mçš„æ–‡ä»¶



~~~sh
[atguigu@hadoop102 mapreduce]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 128MB


2022-01-14 18:07:49,952 INFO fs.TestDFSIO: ----- TestDFSIO ----- : read
2022-01-14 18:07:49,953 INFO fs.TestDFSIO:             Date & time: Fri Jan 14 18:07:49 CST 2022
2022-01-14 18:07:49,953 INFO fs.TestDFSIO:         Number of files: 10
2022-01-14 18:07:49,953 INFO fs.TestDFSIO:  Total MBytes processed: 1280
2022-01-14 18:07:49,953 INFO fs.TestDFSIO:       Throughput mb/sec: 75.85   ğŸ‘ˆ
2022-01-14 18:07:49,953 INFO fs.TestDFSIO:  Average IO rate mb/sec: 99.1	ğŸ‘ˆæ‹‰äº†
2022-01-14 18:07:49,953 INFO fs.TestDFSIO:   IO rate std deviation: 56.65
2022-01-14 18:07:49,953 INFO fs.TestDFSIO:      Test exec time sec: 21.95
2022-01-14 18:07:49,953 INFO fs.TestDFSIO: 

~~~





ğŸŒ´åˆ é™¤æµ‹è¯•ç”Ÿæˆæ•°æ®

~~~sh
$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -clean
~~~





ğŸŒ´æµ‹è¯•ç»“æœåˆ†æï¼šä¸ºä»€ä¹ˆè¯»å–æ–‡ä»¶é€Ÿåº¦å¤§äºç½‘ç»œå¸¦å®½ï¼Ÿç”±äºç›®å‰åªæœ‰ä¸‰å°æœåŠ¡å™¨ï¼Œä¸”æœ‰ä¸‰ä¸ªå‰¯æœ¬ï¼Œ**æ•°æ®è¯»å–å°±è¿‘åŸåˆ™ï¼Œç›¸å½“äºéƒ½æ˜¯è¯»å–çš„æœ¬åœ°ç£ç›˜æ•°æ®ï¼Œæ²¡æœ‰èµ°ç½‘ç»œã€‚**





## å¤šç›®å½•





#### NameNodeå¤šç›®å½•é…ç½®



ğŸŒ´NameNodeçš„æœ¬åœ°ç›®å½•å¯ä»¥é…ç½®æˆå¤šä¸ªï¼Œ==ä¸”æ¯ä¸ªç›®å½•å­˜æ”¾å†…å®¹ç›¸åŒï¼Œ==**å¢åŠ äº†å¯é æ€§**[^åŸºäºå¤šNameNodeå•NNæ— æ„ä¹‰]



**å…·ä½“é…ç½®å¦‚ä¸‹**

ğŸ€åœ¨`hdfs-site.xml`æ–‡ä»¶ä¸­æ·»åŠ å¦‚ä¸‹å†…å®¹

~~~xml
<property>
     <name>dfs.namenode.name.dir</name>	ğŸ‘‡								 ğŸ‘‡
     <value>file://${hadoop.tmp.dir}/dfs/name1,file://${hadoop.tmp.dir}/dfs/name2</value>
</property>
~~~





ğŸ€åœæ­¢é›†ç¾¤ï¼Œåˆ é™¤ä¸‰å°èŠ‚ç‚¹çš„dataå’Œlogsä¸­æ‰€æœ‰æ•°æ®ã€‚

~~~sh
[atguigu@hadoop102 hadoop-3.1.3]$ rm -rf data/ logs/
[atguigu@hadoop103 hadoop-3.1.3]$ rm -rf data/ logs/
[atguigu@hadoop104 hadoop-3.1.3]$ rm -rf data/ logs/
~~~



ğŸ€æ ¼å¼åŒ–é›†ç¾¤å¹¶å¯åŠ¨ã€‚

```sh
[atguigu@hadoop102 hadoop-3.1.3]$ bin/hdfs namenode -format
[atguigu@hadoop102 hadoop-3.1.3]$ sbin/start-dfs.sh
```





**ğŸ€æŸ¥çœ‹ç»“æœ**

~~~sh
#/opt/module/hadoop-3.1.3/data/dfs

[atguigu@hadoop102 dfs]$ ll
æ€»ç”¨é‡ 12
drwx------. 3 atguigu atguigu 4096 12æœˆ 11 08:03 data
drwxrwxr-x. 3 atguigu atguigu 4096 12æœˆ 11 08:03 name1
drwxrwxr-x. 3 atguigu atguigu 4096 12æœˆ 11 08:03 name2

~~~





#### DataNodeå¤šç›®å½•é…ç½®

ğŸ”¥==å¯åœ¨é›†ç¾¤å·¥ä½œæ—¶é…ç½®   å¯¹å•èŠ‚ç‚¹æ·»åŠ ç¡¬ç›˜æ—¶==

DataNodeå¯ä»¥é…ç½®æˆå¤šä¸ªç›®å½•ï¼Œ==æ¯ä¸ªç›®å½•å­˜å‚¨çš„æ•°æ®ä¸ä¸€æ ·==ï¼ˆæ•°æ®ä¸æ˜¯å‰¯æœ¬ï¼‰



![image-20220114214241965](../image/image-20220114214241965.png)





**âš™å…·ä½“é…ç½®å¦‚ä¸‹**

åœ¨hdfs-site.xmlæ–‡ä»¶ä¸­æ·»åŠ å¦‚ä¸‹å†…å®¹

~~~xml
<property>
     <name>dfs.datanode.data.dir</name>
     			                                                      ğŸ‘‡     																		                                  ğŸ‘‡ <value>file://${hadoop.tmp.dir}/dfs/data1,file://${hadoop.tmp.dir}/dfs/data2</value>
</property>
~~~



**æ–‡ä»¶åˆ†å‘** **é‡å¯dfs**  **é›†ç¾¤é…ç½®ä¸€è‡´çš„è¯åˆ†å‘**

**æŸ¥çœ‹ç»“æœ**

~~~sh
[atguigu@hadoop102 dfs]$ ll
æ€»ç”¨é‡ 12
drwx------. 3 atguigu atguigu 4096 4æœˆ   4 14:22 data1ğŸ‘ˆ
drwx------. 3 atguigu atguigu 4096 4æœˆ   4 14:22 data2ğŸ‘ˆ
drwxrwxr-x. 3 atguigu atguigu 4096 12æœˆ 11 08:03 name1
drwxrwxr-x. 3 atguigu atguigu 4096 12æœˆ 11 08:03 name2
~~~



**å‘é›†ç¾¤ä¸Šä¼ ä¸€ä¸ªæ–‡ä»¶ï¼Œå†æ¬¡è§‚å¯Ÿä¸¤ä¸ªæ–‡ä»¶å¤¹é‡Œé¢çš„å†…å®¹å‘ç°ä¸ä¸€è‡´ï¼ˆä¸€ä¸ªæœ‰æ•°ä¸€ä¸ªæ²¡æœ‰ï¼‰**

`hadoop    fs   -put     wcinput/word.txt     /`





#### é›†ç¾¤æ•°æ®å‡è¡¡ä¹‹ç£ç›˜é—´æ•°æ®å‡è¡¡



ç”Ÿäº§ç¯å¢ƒï¼Œç”±äºç¡¬ç›˜ç©ºé—´ä¸è¶³ï¼Œå¾€å¾€éœ€è¦å¢åŠ ä¸€å—ç¡¬ç›˜ã€‚**åˆšåŠ è½½çš„ç¡¬ç›˜æ²¡æœ‰æ•°æ®æ—¶ï¼Œå¯ä»¥æ‰§è¡Œç£ç›˜æ•°æ®å‡è¡¡å‘½ä»¤ã€‚**==ï¼ˆHadoop3.xæ–°ç‰¹æ€§ï¼‰==



~~~sh
#1ï¼‰ç”Ÿæˆå‡è¡¡è®¡åˆ’ï¼ˆæˆ‘ä»¬åªæœ‰ä¸€å—ç£ç›˜ï¼Œä¸ä¼šç”Ÿæˆè®¡åˆ’ï¼‰
hdfs diskbalancer -plan hadoop103

#ï¼ˆ2ï¼‰æ‰§è¡Œå‡è¡¡è®¡åˆ’
hdfs diskbalancer -execute hadoop103.plan.json

#ï¼ˆ3ï¼‰æŸ¥çœ‹å½“å‰å‡è¡¡ä»»åŠ¡çš„æ‰§è¡Œæƒ…å†µ
hdfs diskbalancer -query hadoop103

#ï¼ˆ4ï¼‰å–æ¶ˆå‡è¡¡ä»»åŠ¡
hdfs diskbalancer -cancel hadoop103.plan.json
~~~









## é›†ç¾¤æ‰©å®¹åŠç¼©å®¹





#### ğŸ¥•æ·»åŠ ç™½åå•



**ç™½åå•ï¼š**è¡¨ç¤ºåœ¨ç™½åå•çš„ä¸»æœºIPåœ°å€å¯ä»¥ï¼Œ**ç”¨æ¥å­˜å‚¨æ•°æ®ã€‚**

**ä¼ä¸šä¸­**ï¼šé…ç½®ç™½åå•ï¼Œå¯ä»¥å°½é‡é˜²æ­¢é»‘å®¢æ¶æ„è®¿é—®æ”»å‡»ã€‚



ğŸŒ´åœ¨**NameNode**èŠ‚ç‚¹çš„==**/opt/module/hadoop-3.1.3/etc/hadoop**==ç›®å½•ä¸‹åˆ†åˆ«åˆ›å»º**whitelist** å’Œ**blacklist**æ–‡ä»¶

~~~sh
#ï¼ˆ1ï¼‰åˆ›å»ºç™½åå•
[atguigu@hadoop102 hadoop]$ vim whitelist
#åœ¨whitelistä¸­æ·»åŠ å¦‚ä¸‹ä¸»æœºåç§°ï¼Œå‡å¦‚é›†ç¾¤æ­£å¸¸å·¥ä½œçš„èŠ‚ç‚¹ä¸º102 103 
hadoop102
hadoop103

#ï¼ˆ2ï¼‰åˆ›å»ºé»‘åå•
[atguigu@hadoop102 hadoop]$ touch blacklist
#	ä¿æŒç©ºçš„å°±å¯ä»¥

~~~





ğŸŒ´åœ¨**hdfs-site.xml**é…ç½®æ–‡ä»¶ä¸­å¢åŠ dfs.hostsé…ç½®å‚æ•°



~~~xml
<!-- ç™½åå• -->
<property>
     <name>dfs.hosts</name>		ğŸ‘‡æ–‡ä»¶åœ°å€
     <value>/opt/module/hadoop-3.1.3/etc/hadoop/whitelist</value>
</property>

<!-- é»‘åå• -->
<property>
     <name>dfs.hosts.exclude</name>  ğŸ‘‡æ–‡ä»¶åœ°å€
     <value>/opt/module/hadoop-3.1.3/etc/hadoop/blacklist</value>
</property>
~~~



ğŸŒ´åˆ†å‘é…ç½®æ–‡ä»¶whitelistï¼Œhdfs-site.xml

**ğŸŒ´ç¬¬ä¸€æ¬¡æ·»åŠ é»‘/ç™½åå•å¿…é¡»é‡å¯é›†ç¾¤ï¼Œä¸æ˜¯ç¬¬ä¸€æ¬¡ï¼Œåªéœ€è¦åˆ·æ–°NameNodeèŠ‚ç‚¹å³å¯**

ğŸŒ´åœ¨hadoop104ä¸Šæ‰§è¡Œä¸Šä¼ æ•°æ®æ•°æ®å¤±è´¥

~~~sh
[atguigu@hadoop104 hadoop-3.1.3]$ hadoop fs -put NOTICE.txt /
~~~

**ğŸ”¥äºŒæ¬¡ä¿®æ”¹ç™½åå•ï¼Œå¢åŠ hadoop104**

~~~sh
[atguigu@hadoop102 hadoop]$ vim whitelist
ä¿®æ”¹ä¸ºå¦‚ä¸‹å†…å®¹
hadoop102
hadoop103
hadoop104
~~~



**ğŸ”¥åˆ·æ–°NameNode**

`hdfs dfsadmin -refreshNodes`



ğŸ™‰åœ¨webæµè§ˆå™¨ä¸ŠæŸ¥çœ‹DNï¼Œhttp://hadoop102:9870/dfshealth.html#tab-datanode



#### ğŸ¥•æœå½¹æ–°æœåŠ¡å™¨



â€‹		éšç€å…¬å¸ä¸šåŠ¡çš„å¢é•¿ï¼Œæ•°æ®é‡è¶Šæ¥è¶Šå¤§ï¼ŒåŸæœ‰çš„æ•°æ®èŠ‚ç‚¹çš„å®¹é‡å·²ç»ä¸èƒ½æ»¡è¶³å­˜å‚¨æ•°æ®çš„éœ€æ±‚ï¼Œ**éœ€è¦åœ¨åŸæœ‰é›†ç¾¤åŸºç¡€ä¸ŠåŠ¨æ€æ·»åŠ æ–°çš„æ•°æ®èŠ‚ç‚¹ã€‚**



**ğŸ€ç¯å¢ƒå‡†å¤‡**



~~~sh
ï¼ˆ1ï¼‰åœ¨hadoop100ä¸»æœºä¸Šå†å…‹éš†ä¸€å°hadoop105ä¸»æœº

ï¼ˆ2ï¼‰ä¿®æ”¹IPåœ°å€å’Œä¸»æœºåç§°
[root@hadoop105 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33
[root@hadoop105 ~]# vim /etc/hostname

ï¼ˆ3ï¼‰æ‹·è´hadoop102çš„/opt/moduleç›®å½•å’Œ/etc/profile.d/my_env.shåˆ°hadoop105

[atguigu@hadoop102 opt]$ scp -r module/* atguigu@hadoop105:/opt/module/
[atguigu@hadoop102 opt]$ sudo scp /etc/profile.d/my_env.sh root@hadoop105:/etc/profile.d/my_env.sh

[atguigu@hadoop105 hadoop-3.1.3]$ source /etc/profile
ğŸš©ï¼ˆ4ï¼‰åˆ é™¤hadoop105ä¸ŠHadoopçš„å†å²æ•°æ®ï¼Œdataå’Œlogæ•°æ®
[atguigu@hadoop105 hadoop-3.1.3]$ rm -rf data/ logs/

ğŸš©ï¼ˆ5ï¼‰é…ç½®hadoop102å’Œhadoop103åˆ°hadoop105çš„sshæ— å¯†ç™»å½•
[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop105
[atguigu@hadoop103 .ssh]$ ssh-copy-id hadoop105
~~~



**ğŸ€åœ¨ç™½åå•ä¸­å¢åŠ æ–°æœå½¹çš„æœåŠ¡å™¨**



~~~sh
ï¼ˆ1ï¼‰åœ¨ç™½åå•whitelistä¸­å¢åŠ hadoop104ã€hadoop105ï¼Œå¹¶é‡å¯é›†ç¾¤
[atguigu@hadoop102 hadoop]$ vim whitelist
ä¿®æ”¹ä¸ºå¦‚ä¸‹å†…å®¹
hadoop102
hadoop103
hadoop104
hadoop105

 ï¼ˆ2ï¼‰åˆ†å‘
[atguigu@hadoop102 hadoop]$ xsync whitelist

ğŸš©ï¼ˆ3ï¼‰åˆ·æ–°NameNode
[atguigu@hadoop102 hadoop-3.1.3]$ hdfs dfsadmin -refreshNodes
Refresh nodes successful

~~~





#### ğŸ¥•æœåŠ¡å™¨é—´æ•°æ®å‡è¡¡

==åœ¨é›†ç¾¤éƒ¨ç½²ä¸€åˆ°ä¸¤å¹´åä½¿ç”¨==



åœ¨ä¼ä¸šå¼€å‘ä¸­ï¼Œå¦‚æœç»å¸¸åœ¨hadoop102å’Œhadoop104ä¸Šæäº¤ä»»åŠ¡ï¼Œä¸”å‰¯æœ¬æ•°ä¸º2ï¼Œ**ç”±äºæ•°æ®æœ¬åœ°æ€§åŸåˆ™ï¼Œå°±ä¼šå¯¼è‡´hadoop102å’Œhadoop104æ•°æ®è¿‡å¤šï¼Œhadoop103å­˜å‚¨çš„æ•°æ®é‡å°ã€‚**

å¦ä¸€ç§æƒ…å†µï¼Œ**å°±æ˜¯æ–°æœå½¹çš„æœåŠ¡å™¨æ•°æ®é‡æ¯”è¾ƒå°‘ï¼Œéœ€è¦æ‰§è¡Œé›†ç¾¤å‡è¡¡å‘½ä»¤ã€‚**



**å¼€å¯æ•°æ®å‡è¡¡å‘½ä»¤ï¼š**

`sbin/start-balancer.sh -threshold 10`

å¯¹äºå‚æ•°10ï¼Œä»£è¡¨çš„æ˜¯é›†ç¾¤ä¸­å„ä¸ªèŠ‚ç‚¹çš„**ç£ç›˜ç©ºé—´åˆ©ç”¨ç‡ç›¸å·®ä¸è¶…è¿‡10%**ï¼Œå¯æ ¹æ®å®é™…æƒ…å†µè¿›è¡Œè°ƒæ•´ã€‚





**åœæ­¢æ•°æ®å‡è¡¡å‘½ä»¤ï¼š**

`sbin/stop-balancer.sh`



==æ³¨æ„ï¼šç”±äºHDFSéœ€è¦å¯åŠ¨å•ç‹¬çš„Rebalance Serveræ¥æ‰§è¡ŒRebalanceæ“ä½œï¼Œæ‰€ä»¥å°½é‡ä¸è¦åœ¨NameNodeä¸Šæ‰§è¡Œstart-balancer.shï¼Œè€Œæ˜¯æ‰¾ä¸€å°æ¯”è¾ƒç©ºé—²çš„æœºå™¨ã€‚==





#### ğŸ¥•é»‘åå•é€€å½¹æœåŠ¡å™¨

**é»‘åå•ï¼š**è¡¨ç¤ºåœ¨é»‘åå•çš„ä¸»æœºIPåœ°å€ä¸å¯ä»¥ç”¨æ¥å­˜å‚¨æ•°æ®ã€‚

**ä¼ä¸šä¸­**ï¼šé…ç½®é»‘åå•ï¼Œ==ç”¨æ¥é€€å½¹æœåŠ¡å™¨ã€‚==

é»‘åå•é…ç½®æ­¥éª¤å¦‚ä¸‹ï¼š

~~~sh
1ï¼‰ç¼–è¾‘/opt/module/hadoop-3.1.3/etc/hadoopç›®å½•ä¸‹çš„blacklistæ–‡ä»¶

[atguigu@hadoop102 hadoop] vim blacklist
#æ·»åŠ å¦‚ä¸‹ä¸»æœºåç§°ï¼ˆè¦é€€å½¹çš„èŠ‚ç‚¹ï¼‰
hadoop105
#æ³¨æ„ï¼šå¦‚æœç™½åå•ä¸­æ²¡æœ‰é…ç½®ï¼Œéœ€è¦åœ¨hdfs-site.xmlé…ç½®æ–‡ä»¶ä¸­å¢åŠ dfs.hostsé…ç½®å‚æ•°
<!-- é»‘åå• -->
<property>
     <name>dfs.hosts.exclude</name>
     <value>/opt/module/hadoop-3.1.3/etc/hadoop/blacklist</value>
</property>

2ï¼‰åˆ†å‘é…ç½®æ–‡ä»¶blacklistï¼Œhdfs-site.xml
[atguigu@hadoop104 hadoop]$ xsync hdfs-site.xml blacklist


ğŸš©3ï¼‰ç¬¬ä¸€æ¬¡æ·»åŠ é»‘åå•å¿…é¡»é‡å¯é›†ç¾¤ï¼Œä¸æ˜¯ç¬¬ä¸€æ¬¡ï¼Œåªéœ€è¦åˆ·æ–°NameNodeèŠ‚ç‚¹å³å¯
[atguigu@hadoop102 hadoop-3.1.3]$ hdfs dfsadmin -refreshNodes
Refresh nodes successful


ğŸš©4ï¼‰æ£€æŸ¥Webæµè§ˆå™¨ï¼Œé€€å½¹èŠ‚ç‚¹çš„çŠ¶æ€ä¸ºdecommission in progressï¼ˆé€€å½¹ä¸­ï¼‰ï¼Œè¯´æ˜æ•°æ®èŠ‚ç‚¹æ­£åœ¨å¤åˆ¶å—åˆ°å…¶ä»–èŠ‚ç‚¹


~~~



==ç­‰å¾…é€€å½¹èŠ‚ç‚¹çŠ¶æ€ä¸ºdecommissionedï¼ˆæ‰€æœ‰å—å·²ç»å¤åˆ¶å®Œæˆï¼‰ï¼Œåœæ­¢è¯¥èŠ‚ç‚¹åŠèŠ‚ç‚¹èµ„æºç®¡ç†å™¨ã€‚æ³¨æ„ï¼šå¦‚æœå‰¯æœ¬æ•°æ˜¯3ï¼Œæœå½¹çš„èŠ‚ç‚¹å°äºç­‰äº3ï¼Œæ˜¯ä¸èƒ½é€€å½¹æˆåŠŸçš„ï¼Œéœ€è¦ä¿®æ”¹å‰¯æœ¬æ•°åæ‰èƒ½é€€å½¹==



`hdfs --daemon stop datanode`

`yarn --daemon stop nodemanager`



**ğŸ‚å¦‚æœæ•°æ®ä¸å‡è¡¡ï¼Œå¯ä»¥ç”¨å‘½ä»¤å®ç°é›†ç¾¤çš„å†å¹³è¡¡**

`sbin/start-balancer.sh -threshold 10`





## å­˜å‚¨ä¼˜åŒ–

==æ³¨ï¼š==æ¼”ç¤ºçº åˆ ç å’Œå¼‚æ„å­˜å‚¨éœ€è¦ä¸€å…±5å°è™šæ‹Ÿæœºã€‚å°½é‡æ‹¿å¦å¤–ä¸€å¥—é›†ç¾¤ã€‚æå‰å‡†å¤‡5å°æœåŠ¡å™¨çš„é›†ç¾¤ã€‚



### çº åˆ ç 



HDFSé»˜è®¤æƒ…å†µä¸‹ï¼Œä¸€ä¸ªæ–‡ä»¶æœ‰3ä¸ªå‰¯æœ¬ï¼Œè¿™æ ·æé«˜äº†æ•°æ®çš„å¯é æ€§ï¼Œä½†ä¹Ÿå¸¦æ¥äº†2å€çš„å†—ä½™å¼€é”€ã€‚Hadoop3.xå¼•å…¥äº†çº åˆ ç ï¼Œé‡‡ç”¨è®¡ç®—çš„æ–¹å¼[^æ¶ˆè€—è®¡ç®—èµ„æº]ï¼Œ**å¯ä»¥èŠ‚çœçº¦50ï¼…å·¦å³çš„å­˜å‚¨ç©ºé—´ã€‚**

![image-20220115170117034](../image/image-20220115170117034.png)

**ğŸ‘‰çº åˆ ç ç­–ç•¥è§£é‡Š**

> **1024k=1m                     2m**æ–‡ä»¶æ ¹æ®RS-3-2-1024k **ï¼š 2**ä¸ªæ•°æ®å•å…ƒ+ 2ä¸ªæ ¡éªŒå•å…ƒ

| RS-3-2-1024kï¼š            | ä½¿ç”¨RSç¼–ç ï¼Œæ¯3ä¸ªæ•°æ®å•å…ƒï¼Œç”Ÿæˆ2ä¸ªæ ¡éªŒå•å…ƒï¼Œå…±5ä¸ªå•å…ƒï¼Œä¹Ÿå°±æ˜¯è¯´ï¼šè¿™5ä¸ªå•å…ƒä¸­ï¼Œåªè¦æœ‰ä»»æ„çš„3ä¸ªå•å…ƒå­˜åœ¨ï¼ˆä¸ç®¡æ˜¯æ•°æ®å•å…ƒè¿˜æ˜¯æ ¡éªŒå•å…ƒï¼Œåªè¦æ€»æ•°=3ï¼‰ï¼Œå°±å¯ä»¥å¾—åˆ°åŸå§‹æ•°æ®ã€‚æ¯ä¸ªå•å…ƒçš„å¤§å°æ˜¯1024k=1024*1024=1048576ã€‚ |
| :------------------------ | ------------------------------------------------------------ |
| **RS-10-4-1024kï¼š**       | **ç”¨RSç¼–ç ï¼Œæ¯10ä¸ªæ•°æ®å•å…ƒï¼ˆcellï¼‰ï¼Œç”Ÿæˆ4ä¸ªæ ¡éªŒå•å…ƒï¼Œå…±14ä¸ªå•å…ƒï¼Œä¹Ÿå°±æ˜¯è¯´ï¼šè¿™14ä¸ªå•å…ƒä¸­ï¼Œåªè¦æœ‰ä»»æ„çš„10ä¸ªå•å…ƒå­˜åœ¨ï¼ˆä¸ç®¡æ˜¯æ•°æ®å•å…ƒè¿˜æ˜¯æ ¡éªŒå•å…ƒï¼Œåªè¦æ€»æ•°=10ï¼‰ï¼Œå°±å¯ä»¥å¾—åˆ°åŸå§‹æ•°æ®ã€‚æ¯ä¸ªå•å…ƒçš„å¤§å°æ˜¯1024k=1024*1024=1048576ã€‚** |
| **RS-6-3-1024kï¼š**        | **ä½¿ç”¨RSç¼–ç ï¼Œæ¯6ä¸ªæ•°æ®å•å…ƒï¼Œç”Ÿæˆ3ä¸ªæ ¡éªŒå•å…ƒï¼Œå…±9ä¸ªå•å…ƒï¼Œä¹Ÿå°±æ˜¯è¯´ï¼šè¿™9ä¸ªå•å…ƒä¸­ï¼Œåªè¦æœ‰ä»»æ„çš„6ä¸ªå•å…ƒå­˜åœ¨ï¼ˆä¸ç®¡æ˜¯æ•°æ®å•å…ƒè¿˜æ˜¯æ ¡éªŒå•å…ƒï¼Œåªè¦æ€»æ•°=6ï¼‰ï¼Œå°±å¯ä»¥å¾—åˆ°åŸå§‹æ•°æ®ã€‚æ¯ä¸ªå•å…ƒçš„å¤§å°æ˜¯1024k=1024*1024=1048576ã€‚** |
| **RS-LEGACY-6-3-1024kï¼š** | **ç•¥å’Œä¸Šé¢çš„RS-6-3-1024kä¸€æ ·ï¼Œåªæ˜¯ç¼–ç çš„ç®—æ³•ç”¨çš„æ˜¯rs-legacyã€‚** |
| **XOR-2-1-1024kï¼š**       | **ä½¿ç”¨XORç¼–ç ï¼ˆé€Ÿåº¦æ¯”RSç¼–ç å¿«ï¼‰ï¼Œæ¯2ä¸ªæ•°æ®å•å…ƒï¼Œç”Ÿæˆ1ä¸ªæ ¡éªŒå•å…ƒï¼Œå…±3ä¸ªå•å…ƒï¼Œä¹Ÿå°±æ˜¯è¯´ï¼šè¿™3ä¸ªå•å…ƒä¸­ï¼Œåªè¦æœ‰ä»»æ„çš„2ä¸ªå•å…ƒå­˜åœ¨ï¼ˆä¸ç®¡æ˜¯æ•°æ®å•å…ƒè¿˜æ˜¯æ ¡éªŒå•å…ƒï¼Œåªè¦æ€»æ•°= 2ï¼‰ï¼Œå°±å¯ä»¥å¾—åˆ°åŸå§‹æ•°æ®ã€‚æ¯ä¸ªå•å…ƒçš„å¤§å°æ˜¯1024k=1024*1024=1048576ã€‚** |



~~~sh
#1ï¼‰çº åˆ ç æ“ä½œç›¸å…³çš„å‘½ä»¤
[atguigu@hadoop102 hadoop-3.1.3]$ hdfs ec
Usage: bin/hdfs ec [COMMAND]
          [-listPolicies]
          [-addPolicies -policyFile <file>]
          [-getPolicy -path <path>]
          [-removePolicy -policy <policy>]
          [-setPolicy -path <path> [-policy <policy>] [-replicate]]
          [-unsetPolicy -path <path>]
          [-listCodecs]
          [-enablePolicy -policy <policy>]
          [-disablePolicy -policy <policy>]
          [-help <command-name>].
#2ï¼‰æŸ¥çœ‹å½“å‰æ”¯æŒçš„çº åˆ ç ç­–ç•¥
[atguigu@hadoop102 hadoop-3.1.3] hdfs ec -listPolicies

Erasure Coding Policies:		ğŸ‘‡
ErasureCodingPolicy=[Name=RS-10-4-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=10, numParityUnits=4]], CellSize=1048576, Id=5], State=DISABLED
								ğŸ‘‡
ErasureCodingPolicy=[Name=RS-3-2-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=3, numParityUnits=2]], CellSize=1048576, Id=2], State=DISABLED

ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1], State=ENABLED
 
ErasureCodingPolicy=[Name=RS-LEGACY-6-3-1024k, Schema=[ECSchema=[Codec=rs-legacy, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=3], State=DISABLED

ErasureCodingPolicy=[Name=XOR-2-1-1024k, Schema=[ECSchema=[Codec=xor, numDataUnits=2, numParityUnits=1]], CellSize=1048576, Id=4], State=DISABLED

~~~





>  çº åˆ ç æ¡ˆä¾‹å®æ“



ğŸš©==çº åˆ ç ç­–ç•¥æ˜¯ç»™å…·ä½“ä¸€ä¸ªè·¯å¾„è®¾ç½®ã€‚==æ‰€æœ‰å¾€æ­¤è·¯å¾„ä¸‹å­˜å‚¨çš„æ–‡ä»¶ï¼Œéƒ½ä¼šæ‰§è¡Œæ­¤ç­–ç•¥ã€‚

é»˜è®¤åªå¼€å¯å¯¹RS-6-3-1024kç­–ç•¥çš„æ”¯æŒï¼Œå¦‚è¦ä½¿ç”¨åˆ«çš„ç­–ç•¥éœ€è¦æå‰å¯ç”¨



~~~sh
#å°†/inputç›®å½•è®¾ç½®ä¸ºRS-3-2-1024kç­–ç•¥
#å¼€å¯å¯¹RS-3-2-1024kç­–ç•¥çš„æ”¯æŒ
[atguigu@hadoop102 hadoop-3.1.3]$  hdfs ec -enablePolicy  -policy RS-3-2-1024k
Erasure coding policy RS-3-2-1024k is enabled


#ï¼ˆ2ï¼‰åœ¨HDFSåˆ›å»ºç›®å½•ï¼Œå¹¶è®¾ç½®RS-3-2-1024kç­–ç•¥
[atguigu@hadoop102  hadoop-3.1.3]$  hdfs dfs -mkdir /input

[atguigu@hadoop202 hadoop-3.1.3]$ hdfs ec -setPolicy -path /input -policy RS-3-2-1024k

#ï¼ˆ3ï¼‰ä¸Šä¼ æ–‡ä»¶ï¼Œå¹¶æŸ¥çœ‹æ–‡ä»¶ç¼–ç åçš„å­˜å‚¨æƒ…å†µ
[atguigu@hadoop102 hadoop-3.1.3]$ hdfs dfs -put web.log /input

ğŸš©æ³¨ï¼šä½ æ‰€ä¸Šä¼ çš„æ–‡ä»¶éœ€è¦å¤§äº2Mæ‰èƒ½çœ‹å‡ºæ•ˆæœã€‚ï¼ˆä½äº2Mï¼Œåªæœ‰ä¸€ä¸ªæ•°æ®å•å…ƒå’Œä¸¤ä¸ªæ ¡éªŒå•å…ƒï¼‰

#ï¼ˆ4ï¼‰æŸ¥çœ‹å­˜å‚¨è·¯å¾„çš„æ•°æ®å•å…ƒå’Œæ ¡éªŒå•å…ƒï¼Œå¹¶ä½œç ´åå®éªŒ 
#ğŸ‘‰æ•°æ®å•å…ƒæˆ–æ ¡éªŒå•å…ƒå­˜å‚¨ä½ç½®
rm -rf /opt/module/hadoop-3.1.3/data/dfs/data/current/BP-2006466531-192.168.10.102-1642161395591/current/finalized/subdir/subdir0/
~~~

åˆ é™¤åè¿˜å¯ä»¥åœ¨é›†ç¾¤ä¸‹è½½





### å¼‚æ„å­˜å‚¨ï¼ˆå†·çƒ­æ•°æ®åˆ†ç¦»ï¼‰



å¼‚æ„å­˜å‚¨ä¸»è¦è§£å†³ï¼Œä¸åŒçš„æ•°æ®ï¼Œå­˜å‚¨åœ¨ä¸åŒç±»å‹çš„ç¡¬ç›˜ä¸­ï¼Œè¾¾åˆ°æœ€ä½³æ€§èƒ½çš„é—®é¢˜ã€‚



![image-20220115171341041](../image/image-20220115171341041.png)



ğŸ§**å…³äºå­˜å‚¨ç±»å‹**

| RAM_DISKï¼š | ï¼ˆå†…å­˜é•œåƒæ–‡ä»¶ç³»ç»Ÿï¼‰                                         |
| ---------- | ------------------------------------------------------------ |
| SSDï¼š      | ï¼ˆSSDå›ºæ€ç¡¬ç›˜ï¼‰                                              |
| DISKï¼š     | ï¼ˆæ™®é€šç£ç›˜ï¼Œåœ¨HDFSä¸­ï¼Œå¦‚æœæ²¡æœ‰ä¸»åŠ¨å£°æ˜æ•°æ®ç›®å½•å­˜å‚¨ç±»å‹é»˜è®¤éƒ½æ˜¯DISKï¼‰ |
| ARCHIVEï¼š  | ï¼ˆæ²¡æœ‰ç‰¹æŒ‡å“ªç§å­˜å‚¨ä»‹è´¨ï¼Œä¸»è¦çš„æŒ‡çš„æ˜¯è®¡ç®—èƒ½åŠ›æ¯”è¾ƒå¼±è€Œå­˜å‚¨å¯†åº¦æ¯”è¾ƒé«˜çš„å­˜å‚¨ä»‹è´¨ï¼Œç”¨æ¥è§£å†³æ•°æ®é‡çš„å®¹é‡æ‰©å¢çš„é—®é¢˜ï¼Œä¸€èˆ¬ç”¨äºå½’æ¡£ï¼‰ç£ç›˜é˜µåˆ— |

ğŸ§**å…³äºå­˜å‚¨ç­–ç•¥**

**è¯´æ˜ï¼š**ä»Lazy_Persist  åˆ° Cold  åˆ†åˆ«ä»£è¡¨äº†è®¾å¤‡çš„è®¿é—®é€Ÿåº¦ä»å¿«åˆ°æ…¢

| **ç­–ç•¥**ID | **ç­–ç•¥åç§°** | **å‰¯æœ¬åˆ†å¸ƒ**         |                                                      |
| ---------- | ------------ | -------------------- | ---------------------------------------------------- |
| **15**     | Lazy_Persist | RAM_DISK:1ï¼ŒDISK:n-1 | ä¸€ä¸ªå‰¯æœ¬ä¿å­˜åœ¨å†…å­˜RAM_DISKä¸­ï¼Œå…¶ä½™å‰¯æœ¬ä¿å­˜åœ¨ç£ç›˜ä¸­ã€‚ |
| **12**     | All_SSD      | SSD:n                | æ‰€æœ‰å‰¯æœ¬éƒ½ä¿å­˜åœ¨SSDä¸­ã€‚                              |
| **10**     | One_SSD      | SSD:1ï¼ŒDISK:n-1      | ä¸€ä¸ªå‰¯æœ¬ä¿å­˜åœ¨SSDä¸­ï¼Œå…¶ä½™å‰¯æœ¬ä¿å­˜åœ¨ç£ç›˜ä¸­ã€‚          |
| **7**      | Hot(default) | DISK:n               | Hotï¼šæ‰€æœ‰å‰¯æœ¬ä¿å­˜åœ¨ç£ç›˜ä¸­ï¼Œè¿™ä¹Ÿæ˜¯é»˜è®¤çš„å­˜å‚¨ç­–ç•¥ã€‚    |
| **5**      | Warm         | DSIK:1ï¼ŒARCHIVE:n-1  | ä¸€ä¸ªå‰¯æœ¬ä¿å­˜åœ¨ç£ç›˜ä¸Šï¼Œå…¶ä½™å‰¯æœ¬ä¿å­˜åœ¨å½’æ¡£å­˜å‚¨ä¸Šã€‚     |
| **2**      | Cold         | ARCHIVE:n            | æ‰€æœ‰å‰¯æœ¬éƒ½ä¿å­˜åœ¨å½’æ¡£å­˜å‚¨ä¸Šã€‚                         |





#### å¼‚æ„å­˜å‚¨Shellæ“ä½œ



~~~sh
#1ï¼‰æŸ¥çœ‹å½“å‰æœ‰å“ªäº›å­˜å‚¨ç­–ç•¥å¯ä»¥ç”¨
[atguigu@hadoop102 hadoop-3.1.3]$ hdfs storagepolicies -listPolicies

#ï¼ˆ2ï¼‰ä¸ºæŒ‡å®šè·¯å¾„ï¼ˆæ•°æ®å­˜å‚¨ç›®å½•ï¼‰è®¾ç½®æŒ‡å®šçš„å­˜å‚¨ç­–ç•¥
hdfs storagepolicies -setStoragePolicy -path xxx -policy xxx

#ï¼ˆ3ï¼‰è·å–æŒ‡å®šè·¯å¾„ï¼ˆæ•°æ®å­˜å‚¨ç›®å½•æˆ–æ–‡ä»¶ï¼‰çš„å­˜å‚¨ç­–ç•¥
hdfs storagepolicies -getStoragePolicy -path xxx

#ï¼ˆ4ï¼‰å–æ¶ˆå­˜å‚¨ç­–ç•¥ï¼›æ‰§è¡Œæ”¹å‘½ä»¤ä¹‹åè¯¥ç›®å½•æˆ–è€…æ–‡ä»¶ï¼Œä»¥å…¶ä¸Šçº§çš„ç›®å½•ä¸ºå‡†ï¼Œå¦‚æœæ˜¯æ ¹ç›®å½•ï¼Œé‚£ä¹ˆå°±æ˜¯HOT
hdfs storagepolicies -unsetStoragePolicy -path xxx

#ï¼ˆ5ï¼‰æŸ¥çœ‹æ–‡ä»¶å—çš„åˆ†å¸ƒ
bin/hdfs fsck xxx -files -blocks -locations

#ï¼ˆ6ï¼‰æŸ¥çœ‹é›†ç¾¤èŠ‚ç‚¹
hadoop dfsadmin -report

~~~



#### æµ‹è¯•ç¯å¢ƒå‡†å¤‡



==æ¯å°èŠ‚ç‚¹ç¡¬ç›˜ä¸åŒ  é…ç½®æ–‡ä»¶ä¸åŒä¸èƒ½xsync==



**1**ï¼‰æµ‹è¯•ç¯å¢ƒæè¿°

æœåŠ¡å™¨è§„æ¨¡ï¼š5å°

é›†ç¾¤é…ç½®ï¼šå‰¯æœ¬æ•°ä¸º2ï¼Œåˆ›å»ºå¥½å¸¦æœ‰å­˜å‚¨ç±»å‹çš„ç›®å½•ï¼ˆæå‰åˆ›å»ºï¼‰

é›†ç¾¤è§„åˆ’ï¼š



| èŠ‚ç‚¹      | å­˜å‚¨ç±»å‹åˆ†é…   |
| --------- | -------------- |
| hadoop102 | RAM_DISKï¼ŒSSD  |
| hadoop103 | SSDï¼ŒDISK      |
| hadoop104 | DISKï¼ŒRAM_DISK |
| hadoop105 | ARCHIVE        |
| hadoop106 | ARCHIVE        |



**ğŸ’¿é…ç½®æ–‡ä»¶ä¿¡æ¯**

~~~properties
#ï¼ˆ1ï¼‰ä¸ºhadoop102èŠ‚ç‚¹çš„hdfs-site.xmlæ·»åŠ å¦‚ä¸‹ä¿¡æ¯
<property>
	<name>dfs.replication</name>
	<value>2</value>
</property>
<property>
	<name>dfs.storage.policy.enabled</name>
	<value>true</value>
</property>
<property>
	<name>dfs.datanode.data.dir</name> 
	<value>[SSD]file:///opt/module/hadoop-3.1.3/hdfsdata/ssd,[RAM_DISK]file:///opt/module/hadoop-3.1.3/hdfsdata/ram_disk</value>
</property>

#ï¼ˆ2ï¼‰ä¸ºhadoop103èŠ‚ç‚¹çš„hdfs-site.xmlæ·»åŠ å¦‚ä¸‹ä¿¡æ¯
<property>
	<name>dfs.replication</name>
	<value>2</value>
</property>
<property>
	<name>dfs.storage.policy.enabled</name>
	<value>true</value>
</property>
<property>
	<name>dfs.datanode.data.dir</name>
	<value>[SSD]file:///opt/module/hadoop-3.1.3/hdfsdata/ssd,[DISK]file:///opt/module/hadoop-3.1.3/hdfsdata/disk</value>
</property>

#ï¼ˆ3ï¼‰ä¸ºhadoop104èŠ‚ç‚¹çš„hdfs-site.xmlæ·»åŠ å¦‚ä¸‹ä¿¡æ¯
<property>
	<name>dfs.replication</name>
	<value>2</value>
</property>
<property>
	<name>dfs.storage.policy.enabled</name>
	<value>true</value>
</property>
<property>
	<name>dfs.datanode.data.dir</name>
	<value>[RAM_DISK]file:///opt/module/hdfsdata/ram_disk,[DISK]file:///opt/module/hadoop-3.1.3/hdfsdata/disk</value>
</property>

#ï¼ˆ4ï¼‰ä¸ºhadoop105èŠ‚ç‚¹çš„hdfs-site.xmlæ·»åŠ å¦‚ä¸‹ä¿¡æ¯
<property>
	<name>dfs.replication</name>
	<value>2</value>
</property>
<property>
	<name>dfs.storage.policy.enabled</name>
	<value>true</value>
</property>
<property>
	<name>dfs.datanode.data.dir</name>
	<value>[ARCHIVE]file:///opt/module/hadoop-3.1.3/hdfsdata/archive</value>
</property>

#ï¼ˆ5ï¼‰ä¸ºhadoop106èŠ‚ç‚¹çš„hdfs-site.xmlæ·»åŠ å¦‚ä¸‹ä¿¡æ¯
<property>
	<name>dfs.replication</name>
	<value>2</value>
</property>
<property>
	<name>dfs.storage.policy.enabled</name>
	<value>true</value>
</property>
<property>
	<name>dfs.datanode.data.dir</name>
	<value>[ARCHIVE]file:///opt/module/hadoop-3.1.3/hdfsdata/archive</value>
</property>
~~~



**ğŸ´æ•°æ®å‡†å¤‡**

~~~sh
#å¯åŠ¨é›†ç¾¤
[atguigu@hadoop102 hadoop-3.1.3]$ hdfs namenode -format
[atguigu@hadoop102 hadoop-3.1.3]$ myhadoop.sh start
#ï¼ˆ1ï¼‰å¹¶åœ¨HDFSä¸Šåˆ›å»ºæ–‡ä»¶ç›®å½•
[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -mkdir /hdfsdata
#ï¼ˆ2ï¼‰å¹¶å°†æ–‡ä»¶èµ„æ–™ä¸Šä¼ 
[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -put /opt/module/hadoop-3.1.3/NOTICE.txt /hdfsdata
~~~





#### å„ä¸ªå­˜å‚¨ç­–ç•¥æ¡ˆä¾‹



è·å–ç›®å½•çš„å­˜å‚¨ç­–ç•¥  `hdfs storagepolicies -getStoragePolicy -path è®¾ç½®æ–‡ä»¶è·¯å¾„`[^/hdfsdata]

æŸ¥çœ‹ä¸Šä¼ çš„æ–‡ä»¶å—åˆ†å¸ƒ  `hdfs fsck è®¾ç½®çš„è·¯å¾„ -files -blocks -locations`[^/hdfsdata]



æ•°æ®é™æ¸©   `hdfs storagepolicies -setStoragePolicy -path /hdfsdata -policy WARM`



æŒ‰ç…§å­˜å‚¨ç­–ç•¥è‡ªè¡Œç§»åŠ¨æ–‡ä»¶å—  `hdfs mover /hdfsdata`

| HOTå­˜å‚¨ç­–ç•¥æ¡ˆä¾‹      | æ‰€æœ‰æ–‡ä»¶å—éƒ½å­˜å‚¨åœ¨DISKä¸‹ã€‚æ‰€ä»¥ï¼Œé»˜è®¤å­˜å‚¨ç­–ç•¥ä¸ºHOTã€‚          |
| -------------------- | ------------------------------------------------------------ |
| WARMå­˜å‚¨ç­–ç•¥æµ‹è¯•     | ï¼ˆ1ï¼‰å°†æ•°æ®é™æ¸©ï¼ˆ2ï¼‰å†æ¬¡æŸ¥çœ‹æ–‡ä»¶å—åˆ†å¸ƒï¼Œï¼ˆ3ï¼‰è½¬ç§»æ–‡ä»¶å—ï¼ˆ4ï¼‰å†æ¬¡æŸ¥çœ‹æ–‡ä»¶å—åˆ†å¸ƒ |
| COLDç­–ç•¥æµ‹è¯•         | ï¼ˆ1ï¼‰å°†æ•°æ®é™æ¸©ä¸ºcoldï¼ˆ2ï¼‰è½¬ç§»æ–‡ä»¶å—ï¼ˆ3ï¼‰æ£€æŸ¥æ–‡ä»¶å—çš„åˆ†å¸ƒ    |
| ONE_SSDç­–ç•¥æµ‹è¯•      | ï¼ˆ1ï¼‰æ”¹å˜ç­–ç•¥ä¸ºone_SSDï¼ˆ2ï¼‰è½¬ç§»æ–‡ä»¶å—ï¼ˆ3ï¼‰æŸ¥çœ‹æ–‡ä»¶å—åˆ†å¸ƒ     |
| ALL_SSDç­–ç•¥æµ‹è¯•      | ï¼ˆ1ï¼‰æ”¹å˜ç­–ç•¥ä¸ºAll_SSDï¼ˆ2ï¼‰è½¬ç§»æ–‡ä»¶å— 3ï¼‰æŸ¥çœ‹æ–‡ä»¶å—åˆ†å¸ƒ      |
| LAZY_PERSISTç­–ç•¥æµ‹è¯• | ï¼ˆ1ï¼‰æ”¹å˜ç­–ç•¥ä¸ºï¼ˆ2ï¼‰è½¬ç§»æ–‡ä»¶å—ï¼ˆ3ï¼‰æŸ¥çœ‹æ–‡ä»¶å—åˆ†å¸ƒ            |



==æ³¨æ„ï¼š==å½“æˆ‘ä»¬å°†ç›®å½•è®¾ç½®ä¸ºCOLDå¹¶ä¸”æˆ‘ä»¬æœªé…ç½®ARCHIVEå­˜å‚¨ç›®å½•çš„æƒ…å†µä¸‹ï¼Œä¸å¯ä»¥å‘è¯¥ç›®å½•ç›´æ¥ä¸Šä¼ æ–‡ä»¶ï¼Œä¼šæŠ¥å‡ºå¼‚å¸¸ã€‚



LAZY_PERSISTç­–ç•¥æµ‹è¯•é—®é¢˜ğŸ‘‡

è¿™é‡Œæˆ‘ä»¬å‘ç°æ‰€æœ‰çš„æ–‡ä»¶å—éƒ½æ˜¯å­˜å‚¨åœ¨DISKï¼ŒæŒ‰ç…§ç†è®ºä¸€ä¸ªå‰¯æœ¬å­˜å‚¨åœ¨RAM_DISKï¼Œå…¶ä»–å‰¯æœ¬å­˜å‚¨åœ¨DISKä¸­ï¼Œè¿™æ˜¯å› ä¸ºï¼Œæˆ‘ä»¬è¿˜éœ€è¦é…ç½®â€œdfs.datanode.max.locked.memoryâ€ï¼Œâ€œdfs.block.sizeâ€å‚æ•°ã€‚ğŸ‘ˆé»˜è®¤å­˜å‚¨åˆ°å†…å­˜æ•°æ®çš„å¤§å°

**å†…å­˜ä¸­å­˜å‚¨æ•°æ®é£é™©é«˜**   

é‚£ä¹ˆå‡ºç°å­˜å‚¨ç­–ç•¥ä¸ºLAZY_PERSISTæ—¶ï¼Œæ–‡ä»¶å—å‰¯æœ¬éƒ½å­˜å‚¨åœ¨DISKä¸Šçš„åŸå› æœ‰å¦‚ä¸‹ä¸¤ç‚¹ï¼š

ï¼ˆ1ï¼‰å½“å®¢æˆ·ç«¯æ‰€åœ¨çš„DataNodeèŠ‚ç‚¹æ²¡æœ‰RAM_DISKæ—¶ï¼Œåˆ™ä¼šå†™å…¥å®¢æˆ·ç«¯æ‰€åœ¨çš„DataNodeèŠ‚ç‚¹çš„DISKç£ç›˜ï¼Œå…¶ä½™å‰¯æœ¬ä¼šå†™å…¥å…¶ä»–èŠ‚ç‚¹çš„DISKç£ç›˜ã€‚

ï¼ˆ2ï¼‰å½“å®¢æˆ·ç«¯æ‰€åœ¨çš„DataNodeæœ‰RAM_DISKï¼Œä½†â€œdfs.datanode.max.locked.memoryâ€å‚æ•°å€¼æœªè®¾ç½®æˆ–è€…è®¾ç½®è¿‡å°ï¼ˆå°äºâ€œdfs.block.sizeâ€å‚æ•°å€¼ï¼‰æ—¶ï¼Œåˆ™ä¼šå†™å…¥å®¢æˆ·ç«¯æ‰€åœ¨çš„DataNodeèŠ‚ç‚¹çš„DISKç£ç›˜ï¼Œå…¶ä½™å‰¯æœ¬ä¼šå†™å…¥å…¶ä»–èŠ‚ç‚¹çš„DISKç£ç›˜ã€‚

ä½†æ˜¯ç”±äºè™šæ‹Ÿæœºçš„â€œmax locked memoryâ€ä¸º64KBï¼Œæ‰€ä»¥ï¼Œå¦‚æœå‚æ•°é…ç½®è¿‡å¤§ï¼Œè¿˜ä¼šæŠ¥å‡ºé”™è¯¯ï¼š



é€šè¿‡è¯¥å‘½ä»¤æŸ¥è¯¢æ­¤å‚æ•°çš„å†…å­˜

`ulimit -a`



## æ•…éšœæ’é™¤



==æ³¨æ„ï¼šé‡‡ç”¨ä¸‰å°æœåŠ¡å™¨å³å¯ï¼Œæ¢å¤åˆ°Yarnå¼€å§‹çš„æœåŠ¡å™¨å¿«ç…§ã€‚==

### NameNodeæ•…éšœå¤„ç†

> NameNodeè¿›ç¨‹æŒ‚äº†å¹¶ä¸”å­˜å‚¨çš„æ•°æ®ä¹Ÿä¸¢å¤±äº†ï¼Œå¦‚ä½•æ¢å¤NameNode



~~~SH
#1ï¼‰éœ€æ±‚ï¼š
NameNodeè¿›ç¨‹æŒ‚äº†å¹¶ä¸”å­˜å‚¨çš„æ•°æ®ä¹Ÿä¸¢å¤±äº†ï¼Œå¦‚ä½•æ¢å¤NameNode

#2ï¼‰æ•…éšœæ¨¡æ‹Ÿ
ï¼ˆ1ï¼‰kill -9 NameNodeè¿›ç¨‹
[atguigu@hadoop102 current]$ kill -9 19886
ï¼ˆ2ï¼‰åˆ é™¤NameNodeå­˜å‚¨çš„æ•°æ®ï¼ˆ/opt/module/hadoop-3.1.3/data/tmp/dfs/nameï¼‰
[atguigu@hadoop102 hadoop-3.1.3]$ rm -rf /opt/module/hadoop-3.1.3/data/dfs/name/*

#3ï¼‰é—®é¢˜è§£å†³

ï¼ˆ1ï¼‰æ‹·è´SecondaryNameNodeä¸­æ•°æ®åˆ°åŸNameNodeå­˜å‚¨æ•°æ®ç›®å½•
[atguigu@hadoop102 dfs]$ scp -r atguigu@hadoop104:/opt/module/hadoop-3.1.3/data/dfs/namesecondary/* ./name/
ï¼ˆ2ï¼‰é‡æ–°å¯åŠ¨NameNode
[atguigu@hadoop102 hadoop-3.1.3]$ hdfs --daemon start namenode
ï¼ˆ3ï¼‰å‘é›†ç¾¤ä¸Šä¼ ä¸€ä¸ªæ–‡ä»¶

~~~



### é›†ç¾¤å®‰å…¨æ¨¡å¼&ç£ç›˜ä¿®å¤



==å®‰å…¨æ¨¡å¼==ï¼šæ–‡ä»¶ç³»ç»Ÿåªæ¥å—è¯»æ•°æ®è¯·æ±‚ï¼Œè€Œä¸æ¥å—åˆ é™¤ã€ä¿®æ”¹ç­‰å˜æ›´è¯·æ±‚

è¿›å…¥å®‰å…¨æ¨¡å¼åœºæ™¯

Ã˜ NameNodeåœ¨åŠ è½½é•œåƒæ–‡ä»¶å’Œç¼–è¾‘æ—¥å¿—æœŸé—´å¤„äºå®‰å…¨æ¨¡å¼ï¼›

Ã˜ NameNodeå†æ¥æ”¶DataNodeæ³¨å†Œæ—¶ï¼Œå¤„äºå®‰å…¨æ¨¡å¼

â€‹                                   

é€€å‡ºå®‰å…¨æ¨¡å¼æ¡ä»¶

+ dfs.namenode.safemode.min.datanodes:æœ€å°å¯ç”¨datanodeæ•°é‡ï¼Œé»˜è®¤0

+ dfs.namenode.safemode.threshold-pct:å‰¯æœ¬æ•°è¾¾åˆ°æœ€å°è¦æ±‚çš„blockå ç³»ç»Ÿæ€»blockæ•°çš„ç™¾åˆ†æ¯”ï¼Œé»˜è®¤0.999fã€‚ï¼ˆåªå…è®¸ä¸¢ä¸€ä¸ªå—ï¼‰

+ dfs.namenode.safemode.extension:ç¨³å®šæ—¶é—´ï¼Œé»˜è®¤å€¼30000æ¯«ç§’ï¼Œå³30ç§’

åŸºæœ¬è¯­æ³•

é›†ç¾¤å¤„äºå®‰å…¨æ¨¡å¼ï¼Œä¸èƒ½æ‰§è¡Œé‡è¦æ“ä½œï¼ˆå†™æ“ä½œï¼‰ã€‚é›†ç¾¤å¯åŠ¨å®Œæˆåï¼Œè‡ªåŠ¨é€€å‡ºå®‰å…¨æ¨¡å¼ã€‚

| bin/hdfs dfsadmin -safemode get       | ï¼ˆåŠŸèƒ½æè¿°ï¼šæŸ¥çœ‹å®‰å…¨æ¨¡å¼çŠ¶æ€ï¼‰     |
| ------------------------------------- | ---------------------------------- |
| **bin/hdfs dfsadmin -safemode enter** | **åŠŸèƒ½æè¿°ï¼šè¿›å…¥å®‰å…¨æ¨¡å¼çŠ¶æ€ï¼‰**   |
| **bin/hdfs dfsadmin -safemode leave** | **ï¼ˆåŠŸèƒ½æè¿°ï¼šç¦»å¼€å®‰å…¨æ¨¡å¼çŠ¶æ€ï¼‰** |
| **bin/hdfs dfsadmin -safemode wait**  | **åŠŸèƒ½æè¿°ï¼šç­‰å¾…å®‰å…¨æ¨¡å¼çŠ¶æ€ï¼‰**   |





> å¯åŠ¨é›†ç¾¤ è¿›å…¥å®‰å…¨æ¨¡å¼



é›†ç¾¤å¯åŠ¨åï¼Œç«‹å³æ¥åˆ°é›†ç¾¤ä¸Šåˆ é™¤æ•°æ®ï¼Œæç¤ºé›†ç¾¤å¤„äºå®‰å…¨æ¨¡å¼



> æ•°æ®å—æŸåï¼Œè¿›å…¥å®‰å…¨æ¨¡å¼

![image-20220115183325500](../image/image-20220115183325500.png)

è§£å†³æ–¹æ³•ï¼šå°†å…ƒæ•°æ®åˆ é™¤ï¼ˆåŸæ–‡ä»¶ï¼‰



> æ¨¡æ‹Ÿ     ç­‰å¾…å®‰å…¨æ¨¡å¼



~~~sh
#ï¼ˆ1ï¼‰æŸ¥çœ‹å½“å‰æ¨¡å¼
[atguigu@hadoop102 hadoop-3.1.3]$ hdfs dfsadmin -safemode get
Safe mode is OFF
#ï¼ˆ2ï¼‰å…ˆè¿›å…¥å®‰å…¨æ¨¡å¼
[atguigu@hadoop102 hadoop-3.1.3]$ bin/hdfs dfsadmin -safemode enter
#ï¼ˆ3ï¼‰åˆ›å»ºå¹¶æ‰§è¡Œä¸‹é¢çš„è„šæœ¬
#åœ¨/opt/module/hadoop-3.1.3è·¯å¾„ä¸Šï¼Œç¼–è¾‘ä¸€ä¸ªè„šæœ¬safemode.sh
[atguigu@hadoop102 hadoop-3.1.3]$ vim safemode.sh

#!/bin/bash
hdfs dfsadmin -safemode wait
hdfs dfs -put /opt/module/hadoop-3.1.3/README.txt /

[atguigu@hadoop102 hadoop-3.1.3]$ chmod 777 safemode.sh

[atguigu@hadoop102 hadoop-3.1.3]$ ./safemode.sh 
#ï¼ˆ4ï¼‰å†æ‰“å¼€ä¸€ä¸ªçª—å£ï¼Œæ‰§è¡Œ
[atguigu@hadoop102 hadoop-3.1.3]$ bin/hdfs dfsadmin -safemode leave
#ï¼ˆ5ï¼‰å†è§‚å¯Ÿä¸Šä¸€ä¸ªçª—å£
Safe mode is OFF
#ï¼ˆ6ï¼‰HDFSé›†ç¾¤ä¸Šå·²ç»æœ‰ä¸Šä¼ çš„æ•°æ®äº†

~~~





### æ…¢ç£ç›˜ç›‘æ§

==æ‰¾åˆ°æ…¢ç£ç›˜==

â€œæ…¢ç£ç›˜â€æŒ‡çš„æ—¶å†™å…¥æ•°æ®éå¸¸æ…¢çš„ä¸€ç±»ç£ç›˜ã€‚å…¶å®æ…¢æ€§ç£ç›˜å¹¶ä¸å°‘è§ï¼Œå½“æœºå™¨è¿è¡Œæ—¶é—´é•¿äº†ï¼Œä¸Šé¢è·‘çš„ä»»åŠ¡å¤šäº†ï¼Œç£ç›˜çš„è¯»å†™æ€§èƒ½è‡ªç„¶ä¼šé€€åŒ–ï¼Œä¸¥é‡æ—¶å°±ä¼šå‡ºç°å†™å…¥æ•°æ®å»¶æ—¶çš„é—®é¢˜ã€‚ä¸€ä¸ªç£ç›˜æ–‡ä»¶å½±å“æ•´ä¸ªjobçš„è¿è¡Œ

**å¦‚ä½•å‘ç°æ…¢ç£ç›˜ï¼Ÿ**

æ­£å¸¸åœ¨HDFSä¸Šåˆ›å»ºä¸€ä¸ªç›®å½•ï¼Œåªéœ€è¦ä¸åˆ°1sçš„æ—¶é—´ã€‚å¦‚æœä½ å‘ç°åˆ›å»ºç›®å½•è¶…è¿‡1åˆ†é’ŸåŠä»¥ä¸Šï¼Œè€Œä¸”è¿™ä¸ªç°è±¡å¹¶ä¸æ˜¯æ¯æ¬¡éƒ½æœ‰ã€‚åªæ˜¯å¶å°”æ…¢äº†ä¸€ä¸‹ï¼Œå°±å¾ˆæœ‰å¯èƒ½å­˜åœ¨æ…¢ç£ç›˜ã€‚

å¯ä»¥é‡‡ç”¨å¦‚ä¸‹æ–¹æ³•æ‰¾å‡ºæ˜¯å“ªå—ç£ç›˜æ…¢ï¼š



ğŸŒ´é€šè¿‡å¿ƒè·³æœªè”ç³»æ—¶é—´

ä¸€èˆ¬å‡ºç°æ…¢ç£ç›˜ç°è±¡ï¼Œä¼šå½±å“åˆ°DataNodeä¸NameNodeä¹‹é—´çš„å¿ƒè·³ã€‚æ­£å¸¸æƒ…å†µå¿ƒè·³æ—¶é—´é—´éš”æ˜¯3sã€‚è¶…è¿‡3sè¯´æ˜æœ‰å¼‚å¸¸ã€‚



![image-20220115183710128](../image/image-20220115183710128.png)



**ğŸŒ´fio**å‘½ä»¤ï¼Œæµ‹è¯•ç£ç›˜çš„è¯»å†™æ€§èƒ½

~~~sh
#ï¼ˆ1ï¼‰é¡ºåºè¯»æµ‹è¯•
[atguigu@hadoop102 ~]# sudo yum install -y fio
[atguigu@hadoop102 ~]# sudo fio -filename=/home/atguigu/test.log -direct=1 -iodepth 1 -thread -rw=read -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=test_r

Run status group 0 (all jobs):
   READ: bw=360MiB/s (378MB/s), 360MiB/s-360MiB/s (378MB/s-378MB/s), io=20.0GiB (21.5GB), run=56885-56885msec
ç»“æœæ˜¾ç¤ºï¼Œç£ç›˜çš„æ€»ä½“é¡ºåºè¯»é€Ÿåº¦ä¸º360MiB/sã€‚


#ï¼ˆ2ï¼‰é¡ºåºå†™æµ‹è¯•
[atguigu@hadoop102 ~]# sudo fio -filename=/home/atguigu/test.log -direct=1 -iodepth 1 -thread -rw=write -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=test_w


Run status group 0 (all jobs):
  WRITE: bw=341MiB/s (357MB/s), 341MiB/s-341MiB/s (357MB/s-357MB/s), io=19.0GiB (21.4GB), run=60001-60001msec
ç»“æœæ˜¾ç¤ºï¼Œç£ç›˜çš„æ€»ä½“é¡ºåºå†™é€Ÿåº¦ä¸º341MiB/sã€‚

#ï¼ˆ3ï¼‰éšæœºå†™æµ‹è¯•
[atguigu@hadoop102 ~]# sudo fio -filename=/home/atguigu/test.log -direct=1 -iodepth 1 -thread -rw=randwrite -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=test_randw

Run status group 0 (all jobs):
  WRITE: bw=309MiB/s (324MB/s), 309MiB/s-309MiB/s (324MB/s-324MB/s), io=18.1GiB (19.4GB), run=60001-60001msec
ç»“æœæ˜¾ç¤ºï¼Œç£ç›˜çš„æ€»ä½“éšæœºå†™é€Ÿåº¦ä¸º309MiB/sã€‚

#ï¼ˆ4ï¼‰æ··åˆéšæœºè¯»å†™ï¼š
[atguigu@hadoop102 ~]# sudo fio -filename=/home/atguigu/test.log -direct=1 -iodepth 1 -thread -rw=randrw -rwmixread=70 -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=test_r_w -ioscheduler=noop

Run status group 0 (all jobs):
   READ: bw=220MiB/s (231MB/s), 220MiB/s-220MiB/s (231MB/s-231MB/s), io=12.9GiB (13.9GB), run=60001-60001msec
  WRITE: bw=94.6MiB/s (99.2MB/s), 94.6MiB/s-94.6MiB/s (99.2MB/s-99.2MB/s), io=5674MiB (5950MB), run=60001-60001msec

~~~





### å°æ–‡ä»¶å½’æ¡£

==NnameNodeä¸­æ¯ä¸ªæ–‡ä»¶å—å¤§æ¦‚å ç”¨150byteå†…å­˜==

æ¯ä¸ªæ–‡ä»¶å‡æŒ‰å—å­˜å‚¨ï¼Œæ¯ä¸ªå—çš„å…ƒæ•°æ®å­˜å‚¨åœ¨NameNodeçš„å†…å­˜ä¸­ï¼Œå› æ­¤HDFSå­˜å‚¨å°æ–‡ä»¶ä¼šéå¸¸ä½æ•ˆã€‚å› ä¸ºå¤§é‡çš„å°æ–‡ä»¶ä¼šè€—å°½NameNodeä¸­çš„å¤§éƒ¨åˆ†å†…å­˜ã€‚ä½†æ³¨æ„ï¼Œå­˜å‚¨å°æ–‡ä»¶æ‰€éœ€è¦çš„ç£ç›˜å®¹é‡å’Œæ•°æ®å—çš„å¤§å°æ— å…³ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ª1MBçš„æ–‡ä»¶è®¾ç½®ä¸º128MBçš„å—å­˜å‚¨ï¼Œå®é™…ä½¿ç”¨çš„æ˜¯1MBçš„ç£ç›˜ç©ºé—´ï¼Œè€Œä¸æ˜¯128MBã€‚



**ğŸ§è§£å†³å­˜å‚¨å°æ–‡ä»¶æ–¹æ³•**



![image-20220115184218310](../image/image-20220115184218310.png)



HDFSå­˜æ¡£æ–‡ä»¶æˆ–HARæ–‡ä»¶ï¼Œæ˜¯ä¸€ä¸ªæ›´é«˜æ•ˆçš„æ–‡ä»¶å­˜æ¡£å·¥å…·ï¼Œå®ƒå°†æ–‡ä»¶å­˜å…¥HDFSå—ï¼Œåœ¨å‡å°‘NameNodeå†…å­˜ä½¿ç”¨çš„åŒæ—¶ï¼Œå…è®¸å¯¹æ–‡ä»¶è¿›è¡Œé€æ˜çš„è®¿é—®ã€‚å…·ä½“è¯´æ¥ï¼ŒHDFSå­˜æ¡£æ–‡ä»¶å¯¹å†…è¿˜æ˜¯ä¸€ä¸ªä¸€ä¸ªç‹¬ç«‹æ–‡ä»¶**ï¼Œå¯¹NameNodeè€Œè¨€å´æ˜¯ä¸€ä¸ªæ•´ä½“ï¼Œå‡å°‘äº†NameNodeçš„å†…å­˜ã€‚**



~~~sh
#ï¼ˆ1ï¼‰éœ€è¦å¯åŠ¨YARNè¿›ç¨‹
[atguigu@hadoop102 hadoop-3.1.3]$ start-yarn.sh

#ï¼ˆ2ï¼‰å½’æ¡£æ–‡ä»¶
	æŠŠ/inputç›®å½•é‡Œé¢çš„æ‰€æœ‰æ–‡ä»¶å½’æ¡£æˆä¸€ä¸ªå«input.harçš„å½’æ¡£æ–‡ä»¶ï¼Œå¹¶æŠŠå½’æ¡£åæ–‡ä»¶å­˜å‚¨åˆ°/outputè·¯å¾„ä¸‹ã€‚
[atguigu@hadoop102 hadoop-3.1.3]$ hadoop archive -archiveName input.har -p  /input   /output

#ï¼ˆ3ï¼‰æŸ¥çœ‹å½’æ¡£
[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -ls /output/input.har
[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -ls har:///output/input.har

#ï¼ˆ4ï¼‰è§£å½’æ¡£æ–‡ä»¶
[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -cp har:///output/input.har/*    /

~~~



## é›†ç¾¤è¿ç§»

==2~3å¹´åå¯èƒ½éœ€æ±‚==

### Apacheå’ŒApacheé›†ç¾¤é—´æ•°æ®æ‹·è´

**1ï¼‰**scpå®ç°ä¸¤ä¸ªè¿œç¨‹ä¸»æœºä¹‹é—´çš„æ–‡ä»¶å¤åˆ¶

 scp -r        hello.txt      root@hadoop103:/user/atguigu/hello.txt    // æ¨ push

 scp -r       root@hadoop103:/user/atguigu/hello.txt       hello.txt  // æ‹‰ pull

 scp -r      root@hadoop103:/user/atguigu/hello.txt        root@hadoop104:/user/atguigu 

 //æ˜¯é€šè¿‡æœ¬åœ°ä¸»æœºä¸­è½¬å®ç°ä¸¤ä¸ªè¿œç¨‹ä¸»æœºçš„æ–‡ä»¶å¤åˆ¶ï¼›å¦‚æœåœ¨ä¸¤ä¸ªè¿œç¨‹ä¸»æœºä¹‹é—´sshæ²¡æœ‰é…ç½®çš„æƒ…å†µä¸‹å¯ä»¥ä½¿ç”¨è¯¥æ–¹å¼ã€‚



**2ï¼‰**é‡‡ç”¨distcpå‘½ä»¤å®ç°ä¸¤ä¸ªHadoopé›†ç¾¤ä¹‹é—´çš„é€’å½’æ•°æ®å¤åˆ¶

~~~sh
[atguigu@hadoop102 hadoop-3.1.3]$ bin/hadoop distcp 
          ğŸ‘‡#ä¸€å°é›†ç¾¤çš„NN
hdfs://hadoop102:8020/user/atguigu/hello.txt 
		  ğŸ‘‡#å¦ä¸€å°é›†ç¾¤çš„NN
hdfs://hadoop105:8020/user/atguigu/hello.txt
~~~



### Apacheå’ŒCDHé›†ç¾¤é—´æ•°æ®æ‹·è´



+ apacheé›†ç¾¤å’ŒCDHé›†ç¾¤ã€‚

+ å¯åŠ¨é›†ç¾¤

+ å¯åŠ¨å®Œæ¯•åï¼Œå°†apacheé›†ç¾¤ä¸­ï¼Œhiveåº“é‡Œdwdï¼Œdwsï¼Œadsä¸‰ä¸ªåº“çš„æ•°æ®è¿ç§»åˆ°CDHé›†ç¾¤

+ åœ¨apacheé›†ç¾¤é‡ŒhostsåŠ ä¸ŠCDH Namenodeå¯¹åº”åŸŸåå¹¶åˆ†å‘ç»™hadoopå„æœºå™¨

+ å› ä¸ºé›†ç¾¤éƒ½æ˜¯HAæ¨¡å¼ï¼Œæ‰€ä»¥éœ€è¦åœ¨apacheé›†ç¾¤ä¸Šé…ç½®CDHé›†ç¾¤,è®©distcpèƒ½è¯†åˆ«å‡ºCDHçš„nameservice

  + ~~~xml
    [root@hadoop101 hadoop]# vim /opt/module/hadoop-3.1.3/etc/hadoop/hdfs-site.xml 
    <!--é…ç½®nameservice-->
    <property>
      <name>dfs.nameservices</name>
      <value>mycluster,nameservice1</value>
    </property>
    
    <!--æŒ‡å®šæœ¬åœ°æœåŠ¡-->
    <property>
      <name>dfs.internal.nameservices</name>
      <value>mycluster,nameservice1</value>
    </property>
    <!--é…ç½®å¤šNamenNode-->
    <property>
      <name>dfs.ha.namenodes.mycluster</name>
      <value>nn1,nn2,nn3</value>
    </property>
    <property>
      <name>dfs.namenode.rpc-address.mycluster.nn1</name>
      <value>hadoop101:8020</value>
    </property>
    <property>
    <property>
      <name>dfs.namenode.rpc-address.mycluster.nn2</name>
      <value>hadoop102:8020</value>
    </property>
    <property>
      <name>dfs.namenode.rpc-address.mycluster.nn3</name>
      <value>hadoop103:8020</value>
    </property>
    <!--é…ç½®nameservice1çš„namenodeæœåŠ¡-->
    <property>
        <name>dfs.ha.namenodes.nameservice1</name>
        <value>namenode30,namenode37</value>
      </property>
     <property>
        <name>dfs.namenode.rpc-address.nameservice1.namenode30</name>
        <value>hadoop104:8020</value>
      </property>
    <property>
        <name>dfs.namenode.rpc-address.nameservice1.namenode37</name>
        <value>hadoop106:8020</value>
      </property>
    <property>
        <name>dfs.namenode.http-address.nameservice1.namenode30</name>
        <value>hadoop104:9870</value>
      </property>
      <property>
        <name>dfs.namenode.http-address.nameservice1.namenode37</name>
        <value>hadoop106:9870</value>
      </property>
      <property>
        <name>dfs.client.failover.proxy.provider.nameservice1</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
      </property>
    <!--ä¸ºNamneNodeè®¾ç½®HTTPæœåŠ¡ç›‘å¬-->
    <property>
      <name>dfs.namenode.http-address.mycluster.nn1</name>
      <value>hadoop101:9870</value>
    </property>
    <property>
      <name>dfs.namenode.http-address.mycluster.nn2</name>
      <value>hadoop102:9870</value>
    </property>
    <property>
      <name>dfs.namenode.http-address.mycluster.nn3</name>
      <value>hadoop103:9870</value>
    </property>
    <!--é…ç½®HDFSå®¢æˆ·ç«¯è”ç³»Active NameNodeèŠ‚ç‚¹çš„Javaç±»-->
    <property>
      <name>dfs.client.failover.proxy.provider.mycluster</name>
      <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>
    ~~~

+ ä¿®æ”¹CDH hosts

  + Apach

    ![image-20220115185442578](../image/image-20220115185442578.png)

  + CDH

    ![image-20220115185542001](../image/image-20220115185542001.png)

+ è¿›è¡Œåˆ†å‘ï¼Œ==è¿™é‡Œçš„hadoop104ï¼Œhadoop105ï¼Œhadoop106åˆ†åˆ«å¯¹åº”apacheçš„hadoop101ï¼Œhadoop102ï¼Œhadoop103==

+ åŒæ ·ä¿®æ”¹CDHé›†ç¾¤é…ç½®ï¼Œåœ¨æ‰€æœ‰hdfs-site.xmlæ–‡ä»¶é‡Œä¿®æ”¹é…ç½®

  + ~~~xml
    <property>
    	<name>dfs.nameservices</name>
    	<value>mycluster,nameservice1</value>
    </property>
    
    <property>
    	<name>dfs.internal.nameservices</name>
    	<value>nameservice1</value>
    </property>
    
    <property>
    	<name>dfs.ha.namenodes.mycluster</name>
    	<value>nn1,nn2,nn3</value>
    </property>
    
    <property>
    	<name>dfs.namenode.rpc-address.mycluster.nn1</name>
    	<value>hadoop104:8020</value>
    </property>
    
    <property>
    	<name>dfs.namenode.rpc-address.mycluster.nn2</name>
    	<value>hadoop105:8020</value>
    </property>
    
    <property>
    	<name>dfs.namenode.rpc-address.mycluster.nn3</name>
    	<value>hadoop106:8020</value>
    </property>
    
    <property>
    	<name>dfs.namenode.http-address.mycluster.nn1</name>
    	<value>hadoop104:9870</value>
    </property>
    
    <property>
    	<name>dfs.namenode.http-address.mycluster.nn2</name>
    	<value>hadoop105:9870</value>
    </property>
    
    <property>
    	<name>dfs.namenode.http-address.mycluster.nn3</name>
    	<value>hadoop106:9870</value>
    </property>
    
    <property>
    	<name>dfs.client.failover.proxy.provider.mycluster</name>
    	<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>
    
    ~~~

+ æœ€åæ³¨æ„ï¼šé‡ç‚¹ç”±äºæˆ‘çš„Apahceé›†ç¾¤å’ŒCDHé›†ç¾¤3å°é›†ç¾¤éƒ½æ˜¯hadoop101ï¼Œhadoop102ï¼Œhadoop103æ‰€ä»¥è¦å…³é—­åŸŸåè®¿é—®ï¼Œä½¿ç”¨IPè®¿é—®

  CDHæŠŠé’©å»äº†

  + ![img](../image/clip_image002.jpg)

+ apacheè®¾ç½®ä¸ºfalse

  + ![img](../image/clip_image002-16422448381211.jpg)

+ å†ä½¿ç”¨hadoop distcpå‘½ä»¤è¿›è¡Œè¿ç§»ï¼Œ-Dmapred.job.queue.nameæŒ‡å®šé˜Ÿåˆ—ï¼Œé»˜è®¤æ˜¯defaulté˜Ÿåˆ—ã€‚ä¸Šé¢é…ç½®é›†ç¾¤éƒ½é…äº†çš„è¯ï¼Œé‚£ä¹ˆåœ¨CDHå’Œapacheé›†ç¾¤ä¸‹éƒ½å¯ä»¥æ‰§è¡Œè¿™ä¸ªå‘½ä»¤

  + ~~~sh
    [root@hadoop101 hadoop]# hadoop distcp -Dmapred.job.queue.name=hive  webhdfs://mycluster:9070/user/hive/warehouse/dwd.db/  hdfs://nameservice1/user/hive/warehouse
    ~~~

+ ä¼šå¯åŠ¨ä¸€ä¸ªMRä»»åŠ¡ï¼Œæ­£åœ¨è¿ç§»

+ æŸ¥çœ‹cdh 9870 httpåœ°å€

+ æ•°æ®å·²ç»æˆåŠŸè¿ç§»ã€‚æ•°æ®è¿ç§»æˆåŠŸä¹‹åï¼Œæ¥ä¸‹æ¥è¿ç§»hiveè¡¨ç»“æ„ï¼Œç¼–å†™shellè„šæœ¬

  + ~~~sh
    [root@hadoop101 module]# vim exportHive.sh 
    #!/bin/bash
    hive -e "use dwd;show tables">tables.txt
    cat tables.txt |while read eachline
    do
    hive -e "use dwd;show create table $eachline">>tablesDDL.txt
    echo ";" >> tablesDDL.txt
    done
    ~~~

+ æ‰§è¡Œè„šæœ¬åå°†tablesDDL.txtæ–‡ä»¶åˆ†å‘åˆ°CDHé›†ç¾¤ä¸‹

+ ç„¶åCDHä¸‹å¯¼å…¥æ­¤è¡¨ç»“æ„ï¼Œå…ˆè¿›åˆ°CDHçš„hiveé‡Œåˆ›å»ºdwdåº“

  + ~~~sh
    [root@hadoop101 module]# hive
    hive> create database dwd;
    ~~~

+ åˆ›å»ºæ•°æ®åº“åï¼Œè¾¹ç•ŒtablesDDL.txtåœ¨æœ€ä¸Šæ–¹åŠ ä¸Šuse dwd;

  + ![img](../image/clip_image002-16422450105422.jpg)

+ å¹¶ä¸”å°†createtab_stmtéƒ½æ›¿æ¢æˆç©ºæ ¼

  + ~~~sh
    [root@hadoop101 module]# sed -i s"#createtab_stmt# #g" tablesDDL.txt
    ~~~

+ æœ€åæ‰§è¡Œhive -få‘½ä»¤å°†è¡¨ç»“æ„å¯¼å…¥

  + ~~~sh
    [root@hadoop101 module]# hive -f tablesDDL.txt 
    ~~~

    











# äºŒã€MapReduce

MapReduceç¨‹åºæ•ˆç‡çš„ç“¶é¢ˆåœ¨äºä¸¤ç‚¹ï¼š

**1**ï¼‰è®¡ç®—æœºæ€§èƒ½

CPUã€å†…å­˜ã€ç£ç›˜ã€ç½‘ç»œ

**2**ï¼‰I/Oæ“ä½œä¼˜åŒ–

ï¼ˆ1ï¼‰æ•°æ®å€¾æ–œ

ï¼ˆ2ï¼‰Mapè¿è¡Œæ—¶é—´å¤ªé•¿ï¼Œå¯¼è‡´Reduceç­‰å¾…è¿‡ä¹…

ï¼ˆ3ï¼‰å°æ–‡ä»¶è¿‡å¤š

## å¸¸ç”¨è°ƒä¼˜å‚æ•°

`Mapé˜¶æ®µ`

| **1**ï¼‰è‡ªå®šä¹‰åˆ†åŒºï¼Œå‡å°‘æ•°æ®å€¾æ–œ                         | å®šä¹‰ç±»ï¼Œç»§æ‰¿Partitioneræ¥å£ï¼Œé‡å†™getPartitionæ–¹æ³•            |
| ------------------------------------------------------- | ------------------------------------------------------------ |
| **2**ï¼‰å‡å°‘æº¢å†™çš„æ¬¡æ•°                                   | mapreduce.task.io.sort.mb                   Shuffleçš„ç¯å½¢ç¼“å†²åŒºå¤§å°ï¼Œé»˜è®¤100m                  å¯ä»¥æé«˜åˆ°200m mapreduce.map.sort.spill.                    percent ç¯å½¢ç¼“å†²åŒºæº¢å‡ºçš„é˜ˆå€¼ï¼Œé»˜è®¤80% ï¼Œ      å¯ä»¥æé«˜çš„90% |
| **3**ï¼‰å¢åŠ æ¯æ¬¡Mergeåˆå¹¶æ¬¡æ•°                            | mapreduce.task.io.sort.factoré»˜è®¤10ï¼Œå¯ä»¥æé«˜åˆ°20            |
| **4**ï¼‰åœ¨ä¸å½±å“ä¸šåŠ¡ç»“æœçš„å‰ææ¡ä»¶ä¸‹å¯ä»¥æå‰é‡‡ç”¨Combiner | job.setCombinerClass(xxxReducer.class);                      |
| **5**ï¼‰ä¸ºäº†å‡å°‘ç£ç›˜IOï¼Œå¯ä»¥é‡‡ç”¨Snappyæˆ–è€…LZOå‹ç¼©        | **conf.setBoolean**("mapreduce.map.output.compress", true);       **conf.setClass**("mapreduce.map.output.compress.codec", **SnappyCodec.class**,CompressionCodec.class); |
| 6ï¼‰mapreduce.map.memory.mb é»˜è®¤MapTaskå†…å­˜ä¸Šé™1024MBã€‚  | å¯ä»¥æ ¹æ®128mæ•°æ®å¯¹åº”1Gå†…å­˜åŸåˆ™æé«˜è¯¥å†…å­˜ã€‚                   |
| 7ï¼‰mapreduce.map.java.optsï¼šæ§åˆ¶MapTaskå †å†…å­˜å¤§å°ã€‚     | ï¼ˆå¦‚æœå†…å­˜ä¸å¤Ÿï¼ŒæŠ¥ï¼šjava.lang.OutOfMemoryErrorï¼‰             |
| 8ï¼‰mapreduce.map.cpu.vcores é»˜è®¤MapTaskçš„CPUæ ¸æ•°1ã€‚     | è®¡ç®—å¯†é›†å‹ä»»åŠ¡å¯ä»¥å¢åŠ CPUæ ¸æ•°                                |
| **9**ï¼‰å¼‚å¸¸é‡è¯•                                         | mapreduce.map.maxattemptsæ¯ä¸ªMap Taskæœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œä¸€æ—¦é‡è¯•æ¬¡æ•°è¶…è¿‡è¯¥å€¼ï¼Œåˆ™è®¤ä¸ºMap Taskè¿è¡Œå¤±è´¥ï¼Œé»˜è®¤å€¼ï¼š4    æ ¹æ®æœºå™¨æ€§èƒ½é€‚å½“æé«˜ã€‚ |



`Reduceé˜¶æ®µ`



| 1ï¼‰mapreduce.reduce.shuffle.parallelcopiesæ¯ä¸ªReduceå»Mapä¸­æ‹‰å–æ•°æ®çš„å¹¶è¡Œæ•° | é»˜è®¤å€¼æ˜¯5ã€‚å¯ä»¥æé«˜åˆ°10ã€‚                                    |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| 2ï¼‰mapreduce.reduce.shuffle.input.buffer.percentBufferå¤§å°å Reduceå¯ç”¨å†…å­˜çš„æ¯”ä¾‹ï¼Œ | é»˜è®¤å€¼0.7ã€‚å¯ä»¥æé«˜åˆ°0.8                                     |
| 3ï¼‰mapreduce.reduce.shuffle.merge.percent Bufferä¸­çš„æ•°æ®è¾¾åˆ°å¤šå°‘æ¯”ä¾‹å¼€å§‹å†™å…¥ç£ç›˜ï¼Œ | é»˜è®¤å€¼0.66ã€‚å¯ä»¥æé«˜åˆ°0.75                                   |
| 4ï¼‰mapreduce.reduce.memory.mb é»˜è®¤ReduceTaskå†…å­˜ä¸Šé™1024MBï¼Œæ ¹æ®128mæ•°æ®å¯¹åº”1Gå†…å­˜åŸåˆ™ï¼Œ | é€‚å½“æé«˜å†…å­˜åˆ°4-6G                                           |
| 5ï¼‰mapreduce.reduce.java.optsï¼šæ§åˆ¶ReduceTaskå †å†…å­˜å¤§å°ã€‚    | ï¼ˆå¦‚æœå†…å­˜ä¸å¤Ÿï¼ŒæŠ¥ï¼šjava.lang.OutOfMemoryErrorï¼‰             |
| 6ï¼‰mapreduce.reduce.cpu.vcoresé»˜è®¤ReduceTaskçš„CPUæ ¸æ•°1ä¸ªã€‚   | å¯ä»¥æé«˜åˆ°2-4ä¸ª                                              |
| 7ï¼‰mapreduce.reduce.maxattemptsæ¯ä¸ªReduce Taskæœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œ | ä¸€æ—¦é‡è¯•æ¬¡æ•°è¶…è¿‡è¯¥å€¼ï¼Œåˆ™è®¤ä¸ºMap Taskè¿è¡Œå¤±è´¥ï¼Œé»˜è®¤å€¼ï¼š4ã€‚    |
| 8ï¼‰mapreduce.job.reduce.slowstart.completedmapså½“MapTaskå®Œæˆçš„æ¯”ä¾‹è¾¾åˆ°è¯¥å€¼åæ‰ä¼šä¸ºReduceTaskç”³è¯·èµ„æºã€‚ | é»˜è®¤æ˜¯0.05ã€‚                                                 |
| 9ï¼‰mapreduce.task.timeoutå¦‚æœä¸€ä¸ªTaskåœ¨ä¸€å®šæ—¶é—´å†…æ²¡æœ‰ä»»ä½•è¿›å…¥ï¼Œå³ä¸ä¼šè¯»å–æ–°çš„æ•°æ®ï¼Œä¹Ÿæ²¡æœ‰è¾“å‡ºæ•°æ®ï¼Œåˆ™è®¤ä¸ºè¯¥Taskå¤„äºBlockçŠ¶æ€ï¼Œå¯èƒ½æ˜¯å¡ä½äº†ï¼Œä¹Ÿè®¸æ°¸è¿œä¼šå¡ä½ï¼Œä¸ºäº†é˜²æ­¢å› ä¸ºç”¨æˆ·ç¨‹åºæ°¸è¿œBlockä½ä¸é€€å‡ºï¼Œåˆ™å¼ºåˆ¶è®¾ç½®äº†ä¸€ä¸ªè¯¥è¶…æ—¶æ—¶é—´ï¼ˆå•ä½æ¯«ç§’ï¼‰ï¼Œ | é»˜è®¤æ˜¯600000ï¼ˆ10åˆ†é’Ÿï¼‰ã€‚å¦‚æœä½ çš„ç¨‹åºå¯¹æ¯æ¡è¾“å…¥æ•°æ®çš„å¤„ç†æ—¶é—´è¿‡é•¿ï¼Œå»ºè®®å°†è¯¥å‚æ•°è°ƒå¤§ã€‚ |
| 10ï¼‰å¦‚æœå¯ä»¥ä¸ç”¨Reduceï¼Œå°½å¯èƒ½ä¸ç”¨                           |                                                              |



## æ•°æ®å€¾æ–œé—®é¢˜

**æ•°æ®å€¾æ–œç°è±¡**

æ•°æ®é¢‘ç‡å€¾æ–œâ€”â€”æŸä¸€ä¸ªåŒºåŸŸçš„æ•°æ®é‡è¦è¿œè¿œå¤§äºå…¶ä»–åŒºåŸŸã€‚

æ•°æ®å¤§å°å€¾æ–œâ€”â€”éƒ¨åˆ†è®°å½•çš„å¤§å°è¿œè¿œå¤§äºå¹³å‡å€¼ã€‚



**å‡å°‘æ•°æ®å€¾æ–œçš„æ–¹æ³•**

ğŸŒ´é¦–å…ˆæ£€æŸ¥æ˜¯å¦ç©ºå€¼è¿‡å¤šé€ æˆçš„æ•°æ®å€¾æ–œ

ç”Ÿäº§ç¯å¢ƒï¼Œå¯ä»¥ç›´æ¥è¿‡æ»¤æ‰ç©ºå€¼ï¼›å¦‚æœæƒ³ä¿ç•™ç©ºå€¼ï¼Œå°±è‡ªå®šä¹‰åˆ†åŒºï¼Œå°†ç©ºå€¼åŠ éšæœºæ•°æ‰“æ•£ã€‚æœ€åå†äºŒæ¬¡èšåˆã€‚

ğŸŒ´èƒ½åœ¨mapé˜¶æ®µæå‰å¤„ç†ï¼Œæœ€å¥½å…ˆåœ¨Mapé˜¶æ®µå¤„ç†ã€‚å¦‚ï¼šCombinerã€MapJoin

ğŸŒ´è®¾ç½®å¤šä¸ªreduceä¸ªæ•°









# ä¸‰ã€Yarn



## å¸¸ç”¨çš„è°ƒä¼˜å‚æ•°

**Resourcemanagerç›¸å…³**

| yarn.resourcemanager.scheduler.client.thread-count ResourceManager | å¤„ç†è°ƒåº¦å™¨è¯·æ±‚çš„çº¿ç¨‹æ•°é‡ |
| ------------------------------------------------------------ | ------------------------ |
| yarn.resourcemanager.scheduler.class                         | é…ç½®è°ƒåº¦å™¨               |



**Nodemanagerç›¸å…³**



| yarn.nodemanager.resource.memory-mb                          | NodeManagerä½¿ç”¨å†…å­˜æ•°                                   |
| ------------------------------------------------------------ | ------------------------------------------------------- |
| yarn.nodemanager.resource.system-reserved-memory-mb NodeManager | ä¸ºç³»ç»Ÿä¿ç•™å¤šå°‘å†…å­˜ï¼Œå’Œä¸Šä¸€ä¸ªå‚æ•°äºŒè€…å–ä¸€å³å¯            |
|                                                              |                                                         |
| yarn.nodemanager.resource.cpu-vcores                         | NodeManagerä½¿ç”¨CPUæ ¸æ•°                                  |
| yarn.nodemanager.resource.count-logical-processors-as-cores  | æ˜¯å¦å°†è™šæ‹Ÿæ ¸æ•°å½“ä½œCPUæ ¸æ•°                               |
| yarn.nodemanager.resource.pcores-vcores-multiplier           | è™šæ‹Ÿæ ¸æ•°å’Œç‰©ç†æ ¸æ•°ä¹˜æ•°ï¼Œä¾‹å¦‚ï¼š4æ ¸8çº¿ç¨‹ï¼Œè¯¥å‚æ•°å°±åº”è®¾ä¸º2 |
| yarn.nodemanager.resource.detect-hardware-                   | capabilities æ˜¯å¦è®©yarnè‡ªå·±æ£€æµ‹ç¡¬ä»¶è¿›è¡Œé…ç½®             |
| yarn.nodemanager.vmem-pmem-ratio                             | è™šæ‹Ÿå†…å­˜ç‰©ç†å†…å­˜æ¯”ä¾‹                                    |
| yarn.nodemanager.pmem-check-enabled                          | æ˜¯å¦å¼€å¯ç‰©ç†å†…å­˜æ£€æŸ¥é™åˆ¶container                       |
| yarn.nodemanager.vmem-check-enabled                          | æ˜¯å¦å¼€å¯è™šæ‹Ÿå†…å­˜æ£€æŸ¥é™åˆ¶container                       |



**Containerå®¹å™¨ç›¸å…³**



| yarn.scheduler.minimum-allocation-mb     | å®¹å™¨æœ€å°å†…å­˜ |
| ---------------------------------------- | ------------ |
| yarn.scheduler.maximum-allocation-mb     | å®¹å™¨æœ€å¤§å†…å­˜ |
| yarn.scheduler.minimum-allocation-vcores | å®¹å™¨æœ€å°æ ¸æ•° |
| yarn.scheduler.maximum-allocation-vcores | å®¹å™¨æœ€å¤§æ ¸æ•° |



å®¹é‡è°ƒåº¦å™¨ä½¿ç”¨

å…¬å¹³è°ƒåº¦å™¨ä½¿ç”¨





# å››ã€ç»¼åˆè°ƒä¼˜



## Hadoopå°æ–‡ä»¶ä¼˜åŒ–æ–¹æ³•



### å°æ–‡ä»¶å¼Šç«¯

HDFSä¸Šæ¯ä¸ªæ–‡ä»¶éƒ½è¦åœ¨NameNodeä¸Šåˆ›å»ºå¯¹åº”çš„å…ƒæ•°æ®ï¼Œè¿™ä¸ªå…ƒæ•°æ®çš„å¤§å°çº¦ä¸º150byteï¼Œè¿™æ ·å½“å°æ–‡ä»¶æ¯”è¾ƒå¤šçš„æ—¶å€™ï¼Œå°±ä¼šäº§ç”Ÿå¾ˆå¤šçš„å…ƒæ•°æ®æ–‡ä»¶ï¼Œ==ä¸€æ–¹é¢ä¼šå¤§é‡å ç”¨NameNodeçš„å†…å­˜ç©ºé—´ï¼Œå¦ä¸€æ–¹é¢å°±æ˜¯å…ƒæ•°æ®æ–‡ä»¶è¿‡å¤šï¼Œä½¿å¾—å¯»å€ç´¢å¼•é€Ÿåº¦å˜æ…¢ã€‚==

å°æ–‡ä»¶è¿‡å¤šï¼Œåœ¨è¿›è¡ŒMRè®¡ç®—æ—¶ï¼Œä¼šç”Ÿæˆè¿‡å¤šåˆ‡ç‰‡ï¼Œéœ€è¦å¯åŠ¨è¿‡å¤šçš„MapTaskã€‚æ¯ä¸ªMapTaskå¤„ç†çš„æ•°æ®é‡å°ï¼Œ==å¯¼è‡´MapTaskçš„å¤„ç†æ—¶é—´æ¯”å¯åŠ¨æ—¶é—´è¿˜å°ï¼Œç™½ç™½æ¶ˆè€—èµ„æºã€‚==





### è§£å†³æ–¹æ¡ˆ

ğŸ‘‰åœ¨æ•°æ®é‡‡é›†çš„æ—¶å€™ï¼Œå°±å°†å°æ–‡ä»¶æˆ–å°æ‰¹æ•°æ®åˆæˆå¤§æ–‡ä»¶å†ä¸Šä¼ HDFSï¼ˆæ•°æ®æºå¤´ï¼‰



ğŸ‘‰Hadoop Archiveå­˜å‚¨æ–¹å‘ï¼‰

æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„å°†å°æ–‡ä»¶æ”¾å…¥HDFSå—ä¸­çš„æ–‡ä»¶å­˜æ¡£å·¥å…·ï¼Œèƒ½å¤Ÿå°†å¤šä¸ªå°æ–‡ä»¶æ‰“åŒ…æˆä¸€ä¸ªHARæ–‡ä»¶ï¼Œä»è€Œè¾¾åˆ°å‡å°‘NameNodeçš„å†…å­˜ä½¿ç”¨



ğŸ‘‰CombineTextInputFormatï¼ˆè®¡ç®—æ–¹å‘ï¼‰

CombineTextInputFormatç”¨äºå°†å¤šä¸ªå°æ–‡ä»¶åœ¨åˆ‡ç‰‡è¿‡ç¨‹ä¸­ç”Ÿæˆä¸€ä¸ªå•ç‹¬çš„åˆ‡ç‰‡æˆ–è€…å°‘é‡çš„åˆ‡ç‰‡ã€‚ 



ğŸ‘‰å¼€å¯uberæ¨¡å¼ï¼Œå®ç°JVMé‡ç”¨ï¼ˆè®¡ç®—æ–¹å‘ï¼‰

é»˜è®¤æƒ…å†µä¸‹ï¼Œæ¯ä¸ªTaskä»»åŠ¡éƒ½éœ€è¦å¯åŠ¨ä¸€ä¸ªJVMæ¥è¿è¡Œï¼Œå¦‚æœTaskä»»åŠ¡è®¡ç®—çš„æ•°æ®é‡å¾ˆå°ï¼Œæˆ‘ä»¬å¯ä»¥è®©åŒä¸€ä¸ªJobçš„å¤šä¸ªTaskè¿è¡Œåœ¨ä¸€ä¸ªJVMä¸­ï¼Œä¸å¿…ä¸ºæ¯ä¸ªTaskéƒ½å¼€å¯ä¸€ä¸ªJVMã€‚

~~~sh
#ï¼ˆ1ï¼‰æœªå¼€å¯uberæ¨¡å¼ï¼Œåœ¨/inputè·¯å¾„ä¸Šä¸Šä¼ å¤šä¸ªå°æ–‡ä»¶å¹¶æ‰§è¡Œwordcountç¨‹åº
[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output2
	#ï¼ˆ2ï¼‰è§‚å¯Ÿæ§åˆ¶å°
2021-02-14 16:13:50,607 INFO mapreduce.Job: Job job_1613281510851_0002 running in uber mode : false
#ï¼ˆ3ï¼‰è§‚å¯Ÿhttp://hadoop103:8088/cluster

~~~



![image-20220115194516134](../image/image-20220115194516134.png)



ğŸ‘‰å¼€å¯uberæ¨¡å¼ï¼Œåœ¨mapred-site.xmlä¸­æ·»åŠ å¦‚ä¸‹é…ç½®



~~~xml
<!--  å¼€å¯uberæ¨¡å¼ï¼Œé»˜è®¤å…³é—­ -->
<property>
  	<name>mapreduce.job.ubertask.enable</name>
  	<value>true</value>
</property>

<!-- uberæ¨¡å¼ä¸­æœ€å¤§çš„mapTaskæ•°é‡ï¼Œå¯å‘ä¸‹ä¿®æ”¹  --> 
<property>
  	<name>mapreduce.job.ubertask.maxmaps</name>
  	<value>9</value>
</property>
<!-- uberæ¨¡å¼ä¸­æœ€å¤§çš„reduceæ•°é‡ï¼Œå¯å‘ä¸‹ä¿®æ”¹ -->
<property>
  	<name>mapreduce.job.ubertask.maxreduces</name>
  	<value>1</value>
</property>
<!-- uberæ¨¡å¼ä¸­æœ€å¤§çš„è¾“å…¥æ•°æ®é‡ï¼Œé»˜è®¤ä½¿ç”¨dfs.blocksize çš„å€¼ï¼Œå¯å‘ä¸‹ä¿®æ”¹ -->
<property>
  	<name>mapreduce.job.ubertask.maxbytes</name>
  	<value></value>
</property>




~~~



~~~sh
#ï¼ˆ5ï¼‰åˆ†å‘é…ç½®	ä¸ç”¨é‡å¯
[atguigu@hadoop102 hadoop]$ xsync mapred-site.xml
#ï¼ˆ6ï¼‰å†æ¬¡æ‰§è¡Œwordcountç¨‹åº
[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output2
#	ï¼ˆ7ï¼‰è§‚å¯Ÿæ§åˆ¶å°
2021-02-14 16:28:36,198 INFO mapreduce.Job: Job job_1613281510851_0003 running in uber mode : true
#ï¼ˆ8ï¼‰è§‚å¯Ÿhttp://hadoop103:8088/cluster

~~~

![image-20220115194614288](../image/image-20220115194614288.png)



## æµ‹è¯•MapReduceè®¡ç®—æ€§èƒ½

ä½¿ç”¨Sortç¨‹åºè¯„æµ‹MapReduce
==æ³¨ï¼šä¸€ä¸ªè™šæ‹Ÿæœºä¸è¶…è¿‡150Gç£ç›˜å°½é‡ä¸è¦æ‰§è¡Œè¿™æ®µä»£ç ==

~~~sh

#ï¼ˆ1ï¼‰ä½¿ç”¨RandomWriteræ¥äº§ç”Ÿéšæœºæ•°ï¼Œæ¯ä¸ªèŠ‚ç‚¹è¿è¡Œ10ä¸ªMapä»»åŠ¡ï¼Œæ¯ä¸ªMapäº§ç”Ÿå¤§çº¦1Gå¤§å°çš„äºŒè¿›åˆ¶éšæœºæ•°
[atguigu@hadoop102 mapreduce]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar randomwriter random-data
#ï¼ˆ2ï¼‰æ‰§è¡ŒSortç¨‹åº
[atguigu@hadoop102 mapreduce]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar sort random-data sorted-data
#ï¼ˆ3ï¼‰éªŒè¯æ•°æ®æ˜¯å¦çœŸæ­£æ’å¥½åºäº†
[atguigu@hadoop102 mapreduce]$ 
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar testmapredsort -sortInput random-data -sortOutput sorted-data

~~~







# ä¼ä¸šå¼€å‘åœºæ™¯æ¡ˆä¾‹

### éœ€æ±‚

ï¼ˆ1ï¼‰éœ€æ±‚ï¼šä»1Gæ•°æ®ä¸­ï¼Œç»Ÿè®¡æ¯ä¸ªå•è¯å‡ºç°æ¬¡æ•°ã€‚æœåŠ¡å™¨3å°ï¼Œæ¯å°é…ç½®4Gå†…å­˜ï¼Œ4æ ¸CPUï¼Œ4çº¿ç¨‹ã€‚

ï¼ˆ2ï¼‰éœ€æ±‚åˆ†æï¼š

1G / 128m = 8ä¸ªMapTaskï¼›1ä¸ªReduceTaskï¼›1ä¸ªmrAppMaster

å¹³å‡æ¯ä¸ªèŠ‚ç‚¹è¿è¡Œ10ä¸ª / 3å° â‰ˆ 3ä¸ªä»»åŠ¡ï¼ˆ4   3   3ï¼‰



## HDFS

~~~sh
#ï¼ˆ1ï¼‰ä¿®æ”¹ï¼šhadoop-env.sh
export HDFS_NAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS -Xmx1024m"

export HDFS_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS -Xmx1024m"
#ï¼ˆ2ï¼‰ä¿®æ”¹hdfs-site.xml
<!-- NameNodeæœ‰ä¸€ä¸ªå·¥ä½œçº¿ç¨‹æ± ï¼Œé»˜è®¤å€¼æ˜¯10 -->
<property>
    <name>dfs.namenode.handler.count</name>
    <value>21</value>
</property>
#ï¼ˆ3ï¼‰ä¿®æ”¹core-site.xml
<!-- é…ç½®åƒåœ¾å›æ”¶æ—¶é—´ä¸º60åˆ†é’Ÿ -->
<property>
    <name>fs.trash.interval</name>
    <value>60</value>
</property>
#ï¼ˆ4ï¼‰åˆ†å‘é…ç½®
[atguigu@hadoop102 hadoop]$ xsync hadoop-env.sh hdfs-site.xml core-site.xml

~~~



## MapReduce

~~~xml
ï¼ˆ1ï¼‰ä¿®æ”¹mapred-site.xml
<!-- ç¯å½¢ç¼“å†²åŒºå¤§å°ï¼Œé»˜è®¤100m -->
<property>
  <name>mapreduce.task.io.sort.mb</name>
  <value>100</value>
</property>

<!-- ç¯å½¢ç¼“å†²åŒºæº¢å†™é˜ˆå€¼ï¼Œé»˜è®¤0.8 -->
<property>
  <name>mapreduce.map.sort.spill.percent</name>
  <value>0.80</value>
</property>

<!-- mergeåˆå¹¶æ¬¡æ•°ï¼Œé»˜è®¤10ä¸ª -->
<property>
  <name>mapreduce.task.io.sort.factor</name>
  <value>10</value>
</property>

<!-- maptaskå†…å­˜ï¼Œé»˜è®¤1gï¼› maptaskå †å†…å­˜å¤§å°é»˜è®¤å’Œè¯¥å€¼å¤§å°ä¸€è‡´mapreduce.map.java.opts -->
<property>
  <name>mapreduce.map.memory.mb</name>
  <value>-1</value>
  <description>The amount of memory to request from the scheduler for each    map task. If this is not specified or is non-positive, it is inferred from mapreduce.map.java.opts and mapreduce.job.heap.memory-mb.ratio. If java-opts are also not specified, we set it to 1024.
  </description>
</property>

<!-- mataskçš„CPUæ ¸æ•°ï¼Œé»˜è®¤1ä¸ª -->
<property>
  <name>mapreduce.map.cpu.vcores</name>
  <value>1</value>
</property>

<!-- mataskå¼‚å¸¸é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤4æ¬¡ -->
<property>
  <name>mapreduce.map.maxattempts</name>
  <value>4</value>
</property>

<!-- æ¯ä¸ªReduceå»Mapä¸­æ‹‰å–æ•°æ®çš„å¹¶è¡Œæ•°ã€‚é»˜è®¤å€¼æ˜¯5 -->
<property>
  <name>mapreduce.reduce.shuffle.parallelcopies</name>
  <value>5</value>
</property>

<!-- Bufferå¤§å°å Reduceå¯ç”¨å†…å­˜çš„æ¯”ä¾‹ï¼Œé»˜è®¤å€¼0.7 -->
<property>
  <name>mapreduce.reduce.shuffle.input.buffer.percent</name>
  <value>0.70</value>
</property>

<!-- Bufferä¸­çš„æ•°æ®è¾¾åˆ°å¤šå°‘æ¯”ä¾‹å¼€å§‹å†™å…¥ç£ç›˜ï¼Œé»˜è®¤å€¼0.66ã€‚ -->
<property>
  <name>mapreduce.reduce.shuffle.merge.percent</name>
  <value>0.66</value>
</property>

<!-- reducetaskå†…å­˜ï¼Œé»˜è®¤1gï¼›reducetaskå †å†…å­˜å¤§å°é»˜è®¤å’Œè¯¥å€¼å¤§å°ä¸€è‡´mapreduce.reduce.java.opts -->
<property>
  <name>mapreduce.reduce.memory.mb</name>
  <value>-1</value>
  <description>The amount of memory to request from the scheduler for each    reduce task. If this is not specified or is non-positive, it is inferred
    from mapreduce.reduce.java.opts and mapreduce.job.heap.memory-mb.ratio.
    If java-opts are also not specified, we set it to 1024.
  </description>
</property>

<!-- reducetaskçš„CPUæ ¸æ•°ï¼Œé»˜è®¤1ä¸ª -->
<property>
  <name>mapreduce.reduce.cpu.vcores</name>
  <value>2</value>
</property>

<!-- reducetaskå¤±è´¥é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤4æ¬¡ -->
<property>
  <name>mapreduce.reduce.maxattempts</name>
  <value>4</value>
</property>

<!-- å½“MapTaskå®Œæˆçš„æ¯”ä¾‹è¾¾åˆ°è¯¥å€¼åæ‰ä¼šä¸ºReduceTaskç”³è¯·èµ„æºã€‚é»˜è®¤æ˜¯0.05 -->
<property>
  <name>mapreduce.job.reduce.slowstart.completedmaps</name>
  <value>0.05</value>
</property>

<!-- å¦‚æœç¨‹åºåœ¨è§„å®šçš„é»˜è®¤10åˆ†é’Ÿå†…æ²¡æœ‰è¯»åˆ°æ•°æ®ï¼Œå°†å¼ºåˆ¶è¶…æ—¶é€€å‡º -->
<property>
  <name>mapreduce.task.timeout</name>
  <value>600000</value>
</property>


ï¼ˆ2ï¼‰åˆ†å‘é…ç½®
[atguigu@hadoop102 hadoop]$ xsync mapred-site.xml

~~~



##  Yarn





~~~xml
ï¼ˆ1ï¼‰ä¿®æ”¹yarn-site.xmlé…ç½®å‚æ•°å¦‚ä¸‹ï¼š
<!-- é€‰æ‹©è°ƒåº¦å™¨ï¼Œé»˜è®¤å®¹é‡ -->
<property>
	<description>The class to use as the resource scheduler.</description>
	<name>yarn.resourcemanager.scheduler.class</name>
	<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
</property>

<!-- ResourceManagerå¤„ç†è°ƒåº¦å™¨è¯·æ±‚çš„çº¿ç¨‹æ•°é‡,é»˜è®¤50ï¼›å¦‚æœæäº¤çš„ä»»åŠ¡æ•°å¤§äº50ï¼Œå¯ä»¥å¢åŠ è¯¥å€¼ï¼Œä½†æ˜¯ä¸èƒ½è¶…è¿‡3å° * 4çº¿ç¨‹ = 12çº¿ç¨‹ï¼ˆå»é™¤å…¶ä»–åº”ç”¨ç¨‹åºå®é™…ä¸èƒ½è¶…è¿‡8ï¼‰ -->
<property>
	<description>Number of threads to handle scheduler interface.</description>
	<name>yarn.resourcemanager.scheduler.client.thread-count</name>
	<value>8</value>
</property>

<!-- æ˜¯å¦è®©yarnè‡ªåŠ¨æ£€æµ‹ç¡¬ä»¶è¿›è¡Œé…ç½®ï¼Œé»˜è®¤æ˜¯falseï¼Œå¦‚æœè¯¥èŠ‚ç‚¹æœ‰å¾ˆå¤šå…¶ä»–åº”ç”¨ç¨‹åºï¼Œå»ºè®®æ‰‹åŠ¨é…ç½®ã€‚å¦‚æœè¯¥èŠ‚ç‚¹æ²¡æœ‰å…¶ä»–åº”ç”¨ç¨‹åºï¼Œå¯ä»¥é‡‡ç”¨è‡ªåŠ¨ -->
<property>
	<description>Enable auto-detection of node capabilities such as
	memory and CPU.
	</description>
	<name>yarn.nodemanager.resource.detect-hardware-capabilities</name>
	<value>false</value>
</property>

<!-- æ˜¯å¦å°†è™šæ‹Ÿæ ¸æ•°å½“ä½œCPUæ ¸æ•°ï¼Œé»˜è®¤æ˜¯falseï¼Œé‡‡ç”¨ç‰©ç†CPUæ ¸æ•° -->
<property>
	<description>Flag to determine if logical processors(such as
	hyperthreads) should be counted as cores. Only applicable on Linux
	when yarn.nodemanager.resource.cpu-vcores is set to -1 and
	yarn.nodemanager.resource.detect-hardware-capabilities is true.
	</description>
	<name>yarn.nodemanager.resource.count-logical-processors-as-cores</name>
	<value>false</value>
</property>

<!-- è™šæ‹Ÿæ ¸æ•°å’Œç‰©ç†æ ¸æ•°ä¹˜æ•°ï¼Œé»˜è®¤æ˜¯1.0 -->
<property>
	<description>Multiplier to determine how to convert phyiscal cores to
	vcores. This value is used if yarn.nodemanager.resource.cpu-vcores
	is set to -1(which implies auto-calculate vcores) and
	yarn.nodemanager.resource.detect-hardware-capabilities is set to true. The	number of vcores will be calculated as	number of CPUs * multiplier.
	</description>
	<name>yarn.nodemanager.resource.pcores-vcores-multiplier</name>
	<value>1.0</value>
</property>

<!-- NodeManagerä½¿ç”¨å†…å­˜æ•°ï¼Œé»˜è®¤8Gï¼Œä¿®æ”¹ä¸º4Gå†…å­˜ -->
<property>
	<description>Amount of physical memory, in MB, that can be allocated 
	for containers. If set to -1 and
	yarn.nodemanager.resource.detect-hardware-capabilities is true, it is
	automatically calculated(in case of Windows and Linux).
	In other cases, the default is 8192MB.
	</description>
	<name>yarn.nodemanager.resource.memory-mb</name>
	<value>4096</value>
</property>

<!-- nodemanagerçš„CPUæ ¸æ•°ï¼Œä¸æŒ‰ç…§ç¡¬ä»¶ç¯å¢ƒè‡ªåŠ¨è®¾å®šæ—¶é»˜è®¤æ˜¯8ä¸ªï¼Œä¿®æ”¹ä¸º4ä¸ª -->
<property>
	<description>Number of vcores that can be allocated
	for containers. This is used by the RM scheduler when allocating
	resources for containers. This is not used to limit the number of
	CPUs used by YARN containers. If it is set to -1 and
	yarn.nodemanager.resource.detect-hardware-capabilities is true, it is
	automatically determined from the hardware in case of Windows and Linux.
	In other cases, number of vcores is 8 by default.</description>
	<name>yarn.nodemanager.resource.cpu-vcores</name>
	<value>4</value>
</property>

<!-- å®¹å™¨æœ€å°å†…å­˜ï¼Œé»˜è®¤1G -->
<property>
	<description>The minimum allocation for every container request at the RM	in MBs. Memory requests lower than this will be set to the value of this	property. Additionally, a node manager that is configured to have less memory	than this value will be shut down by the resource manager.
	</description>
	<name>yarn.scheduler.minimum-allocation-mb</name>
	<value>1024</value>
</property>

<!-- å®¹å™¨æœ€å¤§å†…å­˜ï¼Œé»˜è®¤8Gï¼Œä¿®æ”¹ä¸º2G -->
<property>
	<description>The maximum allocation for every container request at the RM	in MBs. Memory requests higher than this will throw an	InvalidResourceRequestException.
	</description>
	<name>yarn.scheduler.maximum-allocation-mb</name>
	<value>2048</value>
</property>

<!-- å®¹å™¨æœ€å°CPUæ ¸æ•°ï¼Œé»˜è®¤1ä¸ª -->
<property>
	<description>The minimum allocation for every container request at the RM	in terms of virtual CPU cores. Requests lower than this will be set to the	value of this property. Additionally, a node manager that is configured to	have fewer virtual cores than this value will be shut down by the resource	manager.
	</description>
	<name>yarn.scheduler.minimum-allocation-vcores</name>
	<value>1</value>
</property>

<!-- å®¹å™¨æœ€å¤§CPUæ ¸æ•°ï¼Œé»˜è®¤4ä¸ªï¼Œä¿®æ”¹ä¸º2ä¸ª -->
<property>
	<description>The maximum allocation for every container request at the RM	in terms of virtual CPU cores. Requests higher than this will throw an
	InvalidResourceRequestException.</description>
	<name>yarn.scheduler.maximum-allocation-vcores</name>
	<value>2</value>
</property>

<!-- è™šæ‹Ÿå†…å­˜æ£€æŸ¥ï¼Œé»˜è®¤æ‰“å¼€ï¼Œä¿®æ”¹ä¸ºå…³é—­ -->
<property>
	<description>Whether virtual memory limits will be enforced for
	containers.</description>
	<name>yarn.nodemanager.vmem-check-enabled</name>
	<value>false</value>
</property>

<!-- è™šæ‹Ÿå†…å­˜å’Œç‰©ç†å†…å­˜è®¾ç½®æ¯”ä¾‹,é»˜è®¤2.1 -->
<property>
	<description>Ratio between virtual memory to physical memory when	setting memory limits for containers. Container allocations are	expressed in terms of physical memory, and virtual memory usage	is allowed to exceed this allocation by this ratio.
	</description>
	<name>yarn.nodemanager.vmem-pmem-ratio</name>
	<value>2.1</value>
</property>
ï¼ˆ2ï¼‰åˆ†å‘é…ç½®
[atguigu@hadoop102 hadoop]$ xsync yarn-site.xml

~~~



æ‰§è¡Œç¨‹åºæµ‹è¯•          é€Ÿåº¦æ”¹å–„



































