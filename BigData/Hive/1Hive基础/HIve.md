# ä¸€ã€æ¦‚å¿µ



Hiveï¼šç”±Facebookå¼€æºç”¨äºè§£å†³æµ·é‡ç»“æ„åŒ–æ—¥å¿—çš„æ•°æ®ç»Ÿè®¡å·¥å…·ã€‚

Hive æ˜¯ä¸€ä¸ªæ„å»ºåœ¨ Hadoop ä¹‹ä¸Šçš„==æ•°æ®ä»“åº“å·¥å…·ï¼Œ==å®ƒå¯ä»¥**å°†ç»“æ„åŒ–çš„æ•°æ®æ–‡ä»¶æ˜ å°„æˆè¡¨ï¼Œå¹¶æä¾›ç±» SQL æŸ¥è¯¢åŠŸèƒ½ï¼Œ**ç”¨äºæŸ¥è¯¢çš„ SQL è¯­å¥ä¼šè¢«è½¬åŒ–ä¸º MapReduce ä½œä¸šï¼Œç„¶åæäº¤åˆ° Hadoop ä¸Šè¿è¡Œã€‚



**Hiveæœ¬è´¨**ï¼šå°†HQLè½¬åŒ–æˆMapReduceç¨‹åº

ï¼ˆ1ï¼‰Hiveå¤„ç†çš„æ•°æ®å­˜å‚¨åœ¨HDFS

ï¼ˆ2ï¼‰Hiveåˆ†ææ•°æ®åº•å±‚çš„å®ç°æ˜¯MapReduce

ï¼ˆ3ï¼‰æ‰§è¡Œç¨‹åºè¿è¡Œåœ¨Yarnä¸Š



## æ¶æ„

![image-20220119184545048](../image/image-20220119184545048.png)



**1**ï¼‰ç”¨æˆ·æ¥å£ï¼šClient

CLIï¼ˆcommand-line interfaceï¼‰ã€JDBC/ODBC(jdbcè®¿é—®hive)ã€WEBUIï¼ˆæµè§ˆå™¨è®¿é—®hiveï¼‰

**2**ï¼‰å…ƒæ•°æ®ï¼šMetastore

å…ƒæ•°æ®åŒ…æ‹¬ï¼šè¡¨åã€è¡¨æ‰€å±çš„æ•°æ®åº“ï¼ˆé»˜è®¤æ˜¯defaultï¼‰ã€è¡¨çš„æ‹¥æœ‰è€…ã€åˆ—/åˆ†åŒºå­—æ®µã€è¡¨çš„ç±»å‹ï¼ˆæ˜¯å¦æ˜¯å¤–éƒ¨è¡¨ï¼‰ã€è¡¨çš„æ•°æ®æ‰€åœ¨ç›®å½•ç­‰ï¼›

é»˜è®¤å­˜å‚¨åœ¨è‡ªå¸¦çš„derbyæ•°æ®åº“ä¸­ï¼Œæ¨èä½¿ç”¨MySQLå­˜å‚¨Metastore

**3**ï¼‰Hadoop

ä½¿ç”¨HDFSè¿›è¡Œå­˜å‚¨ï¼Œä½¿ç”¨MapReduceè¿›è¡Œè®¡ç®—ã€‚

**4**ï¼‰é©±åŠ¨å™¨ï¼šDriver

ï¼ˆ1ï¼‰è§£æå™¨ï¼ˆSQL Parserï¼‰ï¼šå°†SQLå­—ç¬¦ä¸²è½¬æ¢æˆæŠ½è±¡è¯­æ³•æ ‘ASTï¼Œè¿™ä¸€æ­¥ä¸€èˆ¬éƒ½ç”¨ç¬¬ä¸‰æ–¹å·¥å…·åº“å®Œæˆï¼Œæ¯”å¦‚antlrï¼›å¯¹ASTè¿›è¡Œè¯­æ³•åˆ†æï¼Œæ¯”å¦‚==è¡¨æ˜¯å¦å­˜åœ¨ã€å­—æ®µæ˜¯å¦å­˜åœ¨ã€SQLè¯­ä¹‰æ˜¯å¦æœ‰è¯¯ã€‚==

ï¼ˆ2ï¼‰ç¼–è¯‘å™¨ï¼ˆPhysical Planï¼‰ï¼šå°†ASTç¼–è¯‘==ç”Ÿæˆé€»è¾‘æ‰§è¡Œè®¡åˆ’ã€‚==

ï¼ˆ3ï¼‰ä¼˜åŒ–å™¨ï¼ˆQuery Optimizerï¼‰ï¼šå¯¹é€»è¾‘æ‰§è¡Œè®¡åˆ’è¿›è¡Œ==ä¼˜åŒ–ã€‚==

ï¼ˆ4ï¼‰æ‰§è¡Œå™¨ï¼ˆExecutionï¼‰ï¼šæŠŠé€»è¾‘æ‰§è¡Œè®¡åˆ’==è½¬æ¢==æˆå¯ä»¥è¿è¡Œçš„ç‰©ç†è®¡åˆ’ã€‚å¯¹äºHiveæ¥è¯´ï¼Œå°±æ˜¯MR/Sparkã€‚





## ä¼˜ç¼ºç‚¹



**ğŸ‘**

- æ“ä½œæ¥å£é‡‡ç”¨==ç±»SQLè¯­æ³•==ï¼Œæä¾›å¿«é€Ÿå¼€å‘çš„èƒ½åŠ›ï¼ˆç®€å•ã€å®¹æ˜“ä¸Šæ‰‹ï¼‰ã€‚

- é¿å…äº†å»å†™MapReduceï¼Œ**å‡å°‘å¼€å‘äººå‘˜çš„å­¦ä¹ æˆæœ¬ã€‚**

- Hiveçš„æ‰§è¡Œå»¶è¿Ÿæ¯”è¾ƒé«˜ï¼Œå› æ­¤Hiveå¸¸ç”¨äºæ•°æ®åˆ†æï¼Œå¯¹å®æ—¶æ€§è¦æ±‚ä¸é«˜çš„åœºåˆã€‚

- Hiveä¼˜åŠ¿åœ¨äºå¤„ç†å¤§æ•°æ®ï¼Œå¯¹äºå¤„ç†å°æ•°æ®æ²¡æœ‰ä¼˜åŠ¿ï¼Œå› ä¸ºHiveçš„æ‰§è¡Œå»¶è¿Ÿæ¯”è¾ƒé«˜ã€‚

- Hiveæ”¯æŒç”¨æˆ·è‡ªå®šä¹‰å‡½æ•°ï¼Œç”¨æˆ·å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€æ±‚æ¥å®ç°è‡ªå·±çš„å‡½æ•°ã€‚




**ğŸ‘**

**1ï¼‰**Hiveçš„HQLè¡¨è¾¾èƒ½åŠ›æœ‰é™

ï¼ˆ1ï¼‰è¿­ä»£å¼ç®—æ³•æ— æ³•è¡¨è¾¾

ï¼ˆ2ï¼‰æ•°æ®æŒ–æ˜æ–¹é¢ä¸æ“…é•¿ï¼Œç”±äºMapReduceæ•°æ®å¤„ç†æµç¨‹çš„é™åˆ¶ï¼Œæ•ˆç‡æ›´é«˜çš„ç®—æ³•å´æ— æ³•å®ç°ã€‚

**2**ï¼‰Hiveçš„æ•ˆç‡æ¯”è¾ƒä½

ï¼ˆ1ï¼‰Hiveè‡ªåŠ¨ç”Ÿæˆçš„MapReduceä½œä¸šï¼Œé€šå¸¸æƒ…å†µä¸‹ä¸å¤Ÿæ™ºèƒ½åŒ–

ï¼ˆ2ï¼‰Hiveè°ƒä¼˜æ¯”è¾ƒå›°éš¾ï¼Œç²’åº¦è¾ƒç²—





## Hiveå’Œ æ•°æ®åº“æ¯”è¾ƒ



**ç”±äº Hive é‡‡ç”¨äº†ç±»ä¼¼SQL çš„æŸ¥è¯¢è¯­è¨€ HQL(Hive Query Language)ï¼Œå› æ­¤å¾ˆå®¹æ˜“å°† Hive ç†è§£ä¸ºæ•°æ®åº“ã€‚**å…¶å®ä»ç»“æ„ä¸Šæ¥çœ‹ï¼ŒHive å’Œæ•°æ®åº“é™¤äº†æ‹¥æœ‰ç±»ä¼¼çš„æŸ¥è¯¢è¯­è¨€ï¼Œå†æ— ç±»ä¼¼ä¹‹å¤„ã€‚æœ¬æ–‡å°†ä»å¤šä¸ªæ–¹é¢æ¥é˜è¿° Hive å’Œæ•°æ®åº“çš„å·®å¼‚ã€‚æ•°æ®åº“å¯ä»¥ç”¨åœ¨ Online çš„åº”ç”¨ä¸­ï¼Œä½†æ˜¯Hive æ˜¯ä¸ºæ•°æ®ä»“åº“è€Œè®¾è®¡çš„ï¼Œæ¸…æ¥šè¿™ä¸€ç‚¹ï¼Œæœ‰åŠ©äºä»åº”ç”¨è§’åº¦ç†è§£ Hive çš„ç‰¹æ€§ã€‚



**æŸ¥è¯¢è¯­è¨€**

==ç”±äºSQLè¢«å¹¿æ³›çš„åº”ç”¨åœ¨æ•°æ®ä»“åº“ä¸­ï¼Œ==å› æ­¤ï¼Œä¸“é—¨é’ˆå¯¹Hiveçš„ç‰¹æ€§è®¾è®¡äº†ç±»SQLçš„æŸ¥è¯¢è¯­è¨€HQLã€‚ç†Ÿæ‚‰SQLå¼€å‘çš„å¼€å‘è€…å¯ä»¥å¾ˆæ–¹ä¾¿çš„ä½¿ç”¨Hiveè¿›è¡Œå¼€å‘ã€‚

**æ•°æ®æ›´æ–°**

ç”±äºHiveæ˜¯é’ˆå¯¹æ•°æ®ä»“åº“åº”ç”¨è®¾è®¡çš„ï¼Œè€Œæ•°æ®ä»“åº“çš„å†…å®¹æ˜¯==è¯»å¤šå†™å°‘çš„ã€‚==å› æ­¤ï¼Œ==Hiveä¸­ä¸å»ºè®®å¯¹æ•°æ®çš„æ”¹å†™==ï¼Œæ‰€æœ‰çš„æ•°æ®éƒ½æ˜¯åœ¨åŠ è½½çš„æ—¶å€™ç¡®å®šå¥½çš„ã€‚è€Œæ•°æ®åº“ä¸­çš„æ•°æ®é€šå¸¸æ˜¯éœ€è¦ç»å¸¸è¿›è¡Œä¿®æ”¹çš„ï¼Œå› æ­¤å¯ä»¥ä½¿ç”¨ INSERT INTO â€¦ VALUES æ·»åŠ æ•°æ®ï¼Œä½¿ç”¨ UPDATE â€¦ SETä¿®æ”¹æ•°æ®ã€‚

**æ‰§è¡Œå»¶è¿Ÿ**

Hive åœ¨æŸ¥è¯¢æ•°æ®çš„æ—¶å€™ï¼Œç”±äºæ²¡æœ‰ç´¢å¼•ï¼Œ==éœ€è¦æ‰«ææ•´ä¸ªè¡¨==ï¼Œå› æ­¤å»¶è¿Ÿè¾ƒé«˜ã€‚å¦å¤–ä¸€ä¸ªå¯¼è‡´ Hive æ‰§è¡Œå»¶è¿Ÿé«˜çš„å› ç´ æ˜¯ MapReduceæ¡†æ¶ã€‚==ç”±äºMapReduce æœ¬èº«å…·æœ‰è¾ƒé«˜çš„å»¶è¿Ÿï¼Œ==å› æ­¤åœ¨åˆ©ç”¨MapReduce æ‰§è¡ŒHiveæŸ¥è¯¢æ—¶ï¼Œä¹Ÿä¼šæœ‰è¾ƒé«˜çš„å»¶è¿Ÿã€‚ç›¸å¯¹çš„ï¼Œæ•°æ®åº“çš„æ‰§è¡Œå»¶è¿Ÿè¾ƒä½ã€‚å½“ç„¶ï¼Œè¿™ä¸ªä½æ˜¯æœ‰æ¡ä»¶çš„ï¼Œå³æ•°æ®è§„æ¨¡è¾ƒå°ï¼Œå½“æ•°æ®è§„æ¨¡å¤§åˆ°è¶…è¿‡æ•°æ®åº“çš„å¤„ç†èƒ½åŠ›çš„æ—¶å€™ï¼ŒHiveçš„å¹¶è¡Œè®¡ç®—æ˜¾ç„¶èƒ½ä½“ç°å‡ºä¼˜åŠ¿ã€‚

 **æ•°æ®è§„æ¨¡**

ç”±äºHiveå»ºç«‹åœ¨é›†ç¾¤ä¸Šå¹¶å¯ä»¥åˆ©ç”¨MapReduceè¿›è¡Œå¹¶è¡Œè®¡ç®—ï¼Œå› æ­¤==å¯ä»¥æ”¯æŒå¾ˆå¤§è§„æ¨¡çš„æ•°æ®==ï¼›å¯¹åº”çš„ï¼Œæ•°æ®åº“å¯ä»¥æ”¯æŒçš„æ•°æ®è§„æ¨¡è¾ƒå°ã€‚



# äºŒã€Hiveå®‰è£…

[**Hiveå®˜ç½‘åœ°å€** ](http://hive.apache.org/)         [**æ–‡æ¡£æŸ¥çœ‹åœ°å€**](https://cwiki.apache.org/confluence/display/Hive/GettingStarted)                         [**ä¸‹è½½åœ°å€**](http://archive.apache.org/dist/hive/)                [**githubåœ°å€**](https://github.com/apache/hive)





## MySqlå®‰è£…



==åŸå› åœ¨äºHiveé»˜è®¤ä½¿ç”¨çš„å…ƒæ•°æ®åº“ä¸ºderbyï¼Œå¼€å¯Hiveä¹‹åå°±ä¼šå ç”¨å…ƒæ•°æ®åº“==ï¼Œä¸”ä¸ä¸å…¶ä»–å®¢æˆ·ç«¯å…±äº«æ•°æ®ï¼Œå¦‚æœæƒ³å¤šçª—å£æ“ä½œå°±ä¼šæŠ¥é”™ï¼Œæ“ä½œæ¯”è¾ƒå±€é™ã€‚ä»¥æˆ‘ä»¬éœ€è¦å°†Hiveçš„å…ƒæ•°æ®åœ°å€æ”¹ä¸ºMySQLï¼Œ==å¯æ”¯æŒå¤šçª—å£æ“ä½œã€‚==



1. å¸è½½Linuxè‡ªå¸¦çš„mysql
  
   1. **æŸ¥è‡ªå¸¦çš„è½¯ä»¶** 
   
   CentOS6->mysql   CentOS7 ->mariadb
   
   â€‹                                          ğŸ‘‡**ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¯†åˆ«è½¬ä¹‰å­—ç¬¦\\**
   
   `rpm -qa | grep  -i  -E mysql\|mariadb`
   
   â€‹                                    ğŸ‘†**å¿½ç•¥å¤§å°å†™**
   
   2. **å¸è½½**
       sudo rpm -e --nodeps mariadb-libs-5.5.56-2.el7.x86_64
       sudo rpm -e --nodeps mysql-community-common-5.7.16-1.el7.x86_64
   
     â€‹                                                                                   ğŸ‘‡**ä¼ è¿›æ¥çš„å‚æ•°çš„è¿‡æ»¤å™¨**
   
     `rpm -qa | grep  -i  -E mysql\|mariadb | xargs -n1 sudo rpm -e --nodeps` 

â€‹																								  	**ğŸ‘†æŒ‰ä¸€è¡Œä¼ é€’**

~~~sh
#1ï¼‰æ£€æŸ¥å½“å‰ç³»ç»Ÿæ˜¯å¦å®‰è£…è¿‡Mysql
[atguigu@hadoop102 ~]$ rpm -qa|grep mariadb
mariadb-libs-5.5.56-2.el7.x86_64 //å¦‚æœå­˜åœ¨é€šè¿‡å¦‚ä¸‹å‘½ä»¤å¸è½½
[atguigu @hadoop102 ~]$ sudo rpm -e --nodeps  mariadb-libs   //ç”¨æ­¤å‘½ä»¤å¸è½½mariadb

#2ï¼‰å°†MySQLå®‰è£…åŒ…æ‹·è´åˆ°/opt/softwareç›®å½•ä¸‹
[atguigu @hadoop102 software]# ll
æ€»ç”¨é‡ 528384
-rw-r--r--. 1 root root 609556480 3æœˆ  21 15:41 mysql-5.7.28-1.el7.x86_64.rpm-bundle.tar

#3ï¼‰è§£å‹MySQLå®‰è£…åŒ…
[atguigu @hadoop102 software]# tar -xf mysql-5.7.28-1.el7.x86_64.rpm-bundle.tar
 
#4ï¼‰åœ¨å®‰è£…ç›®å½•ä¸‹æ‰§è¡Œrpmå®‰è£…
[atguigu @hadoop102 software]$ sudo rpm -ivh mysql-community-common-5.7.28-1.el7.x86_64.rpm
[atguigu @hadoop102 software]$ sudo rpm -ivh mysql-community-libs-5.7.28-1.el7.x86_64.rpm
[atguigu @hadoop102 software]$ sudo rpm -ivh mysql-community-libs-compat-5.7.28-1.el7.x86_64.rpm
[atguigu @hadoop102 software]$ sudo rpm -ivh mysql-community-client-5.7.28-1.el7.x86_64.rpm
[atguigu @hadoop102 software]$ sudo rpm -ivh mysql-community-server-5.7.28-1.el7.x86_64.rpm
æ³¨æ„:æŒ‰ç…§é¡ºåºä¾æ¬¡æ‰§è¡Œ
å¦‚æœLinuxæ˜¯æœ€å°åŒ–å®‰è£…çš„ï¼Œåœ¨å®‰è£…mysql-community-server-5.7.28-1.el7.x86_64.rpmæ—¶å¯èƒ½ä¼šå‡º	ç°å¦‚ä¸‹é”™è¯¯
[atguigu@hadoop102 software]$ sudo rpm -ivh mysql-community-server-5.7.28-1.el7.x86_64.rpm
è­¦å‘Šï¼šmysql-community-server-5.7.28-1.el7.x86_64.rpm: å¤´V3 DSA/SHA1 Signature, å¯†é’¥ ID 5072e1f5: NOKEY
é”™è¯¯ï¼šä¾èµ–æ£€æµ‹å¤±è´¥ï¼š
        libaio.so.1()(64bit) è¢« mysql-community-server-5.7.28-1.el7.x86_64 éœ€è¦
        libaio.so.1(LIBAIO_0.1)(64bit) è¢« mysql-community-server-5.7.28-1.el7.x86_64 éœ€è¦
        libaio.so.1(LIBAIO_0.4)(64bit) è¢« mysql-community-server-5.7.28-1.el7.x86_64 éœ€è¦
é€šè¿‡yumå®‰è£…ç¼ºå°‘çš„ä¾èµ–,ç„¶åé‡æ–°å®‰è£…mysql-community-server-5.7.28-1.el7.x86_64 å³å¯
[atguigu@hadoop102 software] yum install -y libaio

#5ï¼‰åˆ é™¤/etc/my.cnfæ–‡ä»¶ä¸­datadiræŒ‡å‘çš„ç›®å½•ä¸‹çš„æ‰€æœ‰å†…å®¹,å¦‚æœæœ‰å†…å®¹çš„æƒ…å†µä¸‹:
   æŸ¥çœ‹datadirçš„å€¼ï¼š
[mysqld]
datadir=/var/lib/mysql
   åˆ é™¤/var/lib/mysqlç›®å½•ä¸‹çš„æ‰€æœ‰å†…å®¹:
[atguigu @hadoop102 mysql]# cd /var/lib/mysql
[atguigu @hadoop102 mysql]# sudo rm -rf ./*    //æ³¨æ„æ‰§è¡Œå‘½ä»¤çš„ä½ç½®

#6ï¼‰åˆå§‹åŒ–æ•°æ®åº“
[atguigu @hadoop102 opt]$ sudo mysqld --initialize --user=mysql

#7ï¼‰æŸ¥çœ‹ä¸´æ—¶ç”Ÿæˆçš„rootç”¨æˆ·çš„å¯†ç  
[atguigu @hadoop102 opt]$ cat /var/log/mysqld.log 
  
#8ï¼‰å¯åŠ¨MySQLæœåŠ¡
[atguigu @hadoop102 opt]$ sudo systemctl start mysqld

#9ï¼‰ç™»å½•MySQLæ•°æ®åº“
[atguigu @hadoop102 opt]$ mysql -uroot -p
Enter password:   è¾“å…¥ä¸´æ—¶ç”Ÿæˆçš„å¯†ç 
   ç™»å½•æˆåŠŸ.

#10ï¼‰å¿…é¡»å…ˆä¿®æ”¹rootç”¨æˆ·çš„å¯†ç ,å¦åˆ™æ‰§è¡Œå…¶ä»–çš„æ“ä½œä¼šæŠ¥é”™
mysql> set password = password("æ–°å¯†ç ")

#11ï¼‰ä¿®æ”¹mysqlåº“ä¸‹çš„userè¡¨ä¸­çš„rootç”¨æˆ·å…è®¸ä»»æ„ipè¿æ¥
mysql> update mysql.user set host='%' where user='root';
mysql> flush privileges;

~~~





##  Hiveå®‰è£…éƒ¨ç½²



~~~sh
#1ï¼‰æŠŠapache-hive-3.1.2-bin.tar.gzä¸Šä¼ åˆ°linuxçš„/opt/softwareç›®å½•ä¸‹
#2ï¼‰è§£å‹apache-hive-3.1.2-bin.tar.gzåˆ°/opt/module/ç›®å½•ä¸‹é¢
[atguigu@hadoop102 software]$ tar -zxvf /opt/software/apache-hive-3.1.2-bin.tar.gz -C /opt/module/
			ğŸ‘†æ”¹å˜è‡³ç›®å½• 
#3ï¼‰ä¿®æ”¹apache-hive-3.1.2-bin.tar.gzçš„åç§°ä¸ºhive
[atguigu@hadoop102 software]$ mv /opt/module/apache-hive-3.1.2-bin/ /opt/module/hive

#4ï¼‰ä¿®æ”¹/etc/profile.d/my_env.shï¼Œæ·»åŠ ç¯å¢ƒå˜é‡
[atguigu@hadoop102 software]$ sudo vim /etc/profile.d/my_env.sh

#5ï¼‰æ·»åŠ å†…å®¹
#HIVE_HOME
HIVE_HOME=/opt/module/hive
PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin
export PATH JAVA_HOME HADOOP_HOME HIVE_HOME

#6ï¼‰è§£å†³æ—¥å¿—JaråŒ…å†²çª
[atguigu@hadoop102 software]$ mv $HIVE_HOME/lib/log4j-slf4j-impl-2.10.0.jar $HIVE_HOME/lib/log4j-slf4j-impl-2.10.0.bak

~~~



## Hiveå…ƒæ•°æ®é…ç½®åˆ°MySql



~~~xml
ğŸŒ´**æ‹·è´é©±åŠ¨**
å°†MySQLçš„JDBCé©±åŠ¨æ‹·è´åˆ°Hiveçš„libç›®å½•ä¸‹
[atguigu@hadoop102 software]$ cp /opt/software/mysql-connector-java-5.1.48.jar $HIVE_HOME/lib

ğŸŒ´é…ç½®Metastoreåˆ°MySql
#åœ¨$HIVE_HOME/confç›®å½•ä¸‹æ–°å»ºhive-site.xmlæ–‡ä»¶
[atguigu@hadoop102 software]$ vim $HIVE_HOME/conf/hive-site.xml
æ·»åŠ å¦‚ä¸‹å†…å®¹
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <!-- jdbcè¿æ¥çš„URL -->
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://hadoop102:3306/metastore?useSSL=false</value>
</property>

    <!-- jdbcè¿æ¥çš„Driver-->
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
</property>

	<!-- jdbcè¿æ¥çš„username-->
    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
    </property>

    <!-- jdbcè¿æ¥çš„password -->
    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>123456</value>
</property>

    <!-- Hiveé»˜è®¤åœ¨HDFSçš„å·¥ä½œç›®å½• -->
    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/user/hive/warehouse</value>
    </property>
    
   <!-- Hiveå…ƒæ•°æ®å­˜å‚¨çš„éªŒè¯ -->
    <property>
        <name>hive.metastore.schema.verification</name>
        <value>false</value>
    </property>
   
    <!-- å…ƒæ•°æ®å­˜å‚¨æˆæƒ  -->
    <property>
        <name>hive.metastore.event.db.notification.api.auth</name>
        <value>false</value>
    </property>
</configuration>

~~~



## åˆå§‹åŒ–å…ƒæ•°æ®åº“

```sh
#ç™»é™†MySQL
[atguigu@hadoop102 software]$ mysql -uroot -p000000

#æ–°å»ºHiveå…ƒæ•°æ®åº“
mysql> create database metastore;
mysql> quit;

#åˆå§‹åŒ–Hiveå…ƒæ•°æ®åº“ğŸ“Œ
[atguigu@hadoop102 software]$ schematool -initSchema -dbType mysql -verbose
```





## å¯åŠ¨çš„ä¸‰ç§æ–¹å¼Hive

> æ ¹æ®è¿æ¥æ–¹å¼å°†Hiveçš„å…¶ä¸­åˆ†ä¸ºä¸‰ç§



**ä»…é™æœ¬åœ°è¿æ¥**

- ```sh
  $bin/hive
  ```

  ==è¯¥æ–¹å¼ç›´æ¥å‘½ä»¤å¯åŠ¨å³å¯ä¸éœ€è¦å¯åŠ¨é¢å¤–çš„æœåŠ¡==

**å¤–éƒ¨è®¿é—®è¿æ¥**

- 2ã€å…ƒæ•°æ®æœåŠ¡çš„æ–¹å¼è®¿é—®Hive
- 3ã€JDBCæ–¹å¼è®¿é—®Hive



### å¤–éƒ¨è®¿é—®è¿æ¥

------

#### å…ƒæ•°æ®æœåŠ¡çš„æ–¹å¼

------

â€‹																								**åŸç†å›¾âš™**

![image-20220730134816762](../image/image-20220730134816762.png)

- **åœ¨hive-site.xmlæ–‡ä»¶ä¸­æ·»åŠ å¦‚ä¸‹é…ç½®ä¿¡æ¯**

  ```xml
  <!-- æŒ‡å®šå­˜å‚¨å…ƒæ•°æ®è¦è¿æ¥çš„åœ°å€ -->
      <property>
          <name>hive.metastore.uris</name>
          <value>thrift://hadoop102:9083</value>
      </property>
  ```

- **å¯åŠ¨metastore Server**

  ```sh
  $ hive --service metastore
  ```

- ==å¯åŠ¨HIve==

  ```sh
  hive]$ bin/hive
  ```

  





#### JDBCæ–¹å¼

------

â€‹																							**åŸç†å›¾âš™**

![image-20220730135439814](../image/image-20220730135439814.png)

- **åœ¨hive-site.xmlæ–‡ä»¶ä¸­æ·»åŠ å¦‚ä¸‹é…ç½®ä¿¡æ¯**

  ```xml
  <!-- æŒ‡å®šhiveserver2è¿æ¥çš„host -->
      <property>
          <name>hive.server2.thrift.bind.host</name>
          <value>hadoop102</value>
      </property>
  
      <!-- æŒ‡å®šhiveserver2è¿æ¥çš„ç«¯å£å· -->
      <property>
          <name>hive.server2.thrift.port</name>
          <value>10000</value>
      </property>
  ```

- **å¯åŠ¨hiveserver2**

  ```sh
  $ bin/hive --service hiveserver2
  ```

- **å¯åŠ¨beelineå®¢æˆ·ç«¯**                ==å¯åŠ¨åå¤šç­‰å¾…==

  ```sh
  $ bin/beeline -u jdbc:hive2://hadoop102:10000 -n atguigu
  ```

  

==Beeline/HS2è¿æ¥æŠ¥é”™==ï¼šCould not open client transport with JDBC Uri: jdbc:hive2://localhost:10000/default

**ä¸»è¦åŸå› ï¼š**æ˜¯hadoopå¼•å…¥äº†ä¸€ä¸ªå®‰å…¨ä¼ªè£…æœºåˆ¶ï¼Œä½¿å¾—hadoop ä¸å…è®¸ä¸Šå±‚ç³»ç»Ÿç›´æ¥å°†å®é™…ç”¨æˆ·ä¼ é€’åˆ°hadoopå±‚ï¼Œè€Œæ˜¯å°†å®é™…ç”¨æˆ·ä¼ é€’ç»™ä¸€ä¸ªè¶…çº§ä»£ç†ï¼Œç”±æ­¤ä»£ç†åœ¨hadoopä¸Šæ‰§è¡Œæ“ä½œï¼Œé¿å…ä»»æ„å®¢æˆ·ç«¯éšæ„æ“ä½œhadoop

**è§£å†³æ–¹å¼ï¼š**åœ¨hadoopçš„é…ç½®æ–‡ä»¶core-site.xmlå¢åŠ å¦‚ä¸‹é…ç½®ï¼Œé‡å¯hdfsï¼Œå…¶ä¸­â€œxxxâ€æ˜¯è¿æ¥beelineçš„ç”¨æˆ·ï¼Œå°†â€œxxxâ€æ›¿æ¢æˆè‡ªå·±çš„ç”¨æˆ·åå³å¯ã€‚æœ€å…³é”®çš„æ˜¯==ä¸€å®šè¦é‡å¯hadoop==ï¼Œå…ˆ`stop-all.sh`ï¼Œå†`start-all.sh`ï¼Œå¦åˆ™ä¸ä¼šç”Ÿæ•ˆçš„ï¼ï¼é‚£æ ·å°±è¿˜æ˜¯æŠ¥é”™ï¼



~~~xml
 <property>
        <name>hadoop.proxyuser.xxx.hosts</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.xxx.groups</name>
        <value>*</value>
    </property>
~~~

**â€œ\*â€è¡¨ç¤ºå¯é€šè¿‡è¶…çº§ä»£ç†â€œxxxâ€æ“ä½œhadoopçš„ç”¨æˆ·ã€ç”¨æˆ·ç»„å’Œä¸»æœº**

 





## ç¼–å†™metastoreå’Œhiveserver2è„šæœ¬



==ç”¨è„šæœ¬çš„è¯æ³¨é‡Šæ‰JDBCå’Œå…ƒæ•°æ®çš„é…ç½®==     <!--  -->  [^XMLæ³¨é‡Šç¬¦]



### Shellå‘½ä»¤ä»‹ç»

å‰å°å¯åŠ¨çš„æ–¹å¼å¯¼è‡´éœ€è¦æ‰“å¼€å¤šä¸ªshellçª—å£ï¼Œå¯ä»¥ä½¿ç”¨å¦‚ä¸‹æ–¹å¼åå°æ–¹å¼å¯åŠ¨

`nohup`: æ”¾åœ¨å‘½ä»¤å¼€å¤´ï¼Œè¡¨ç¤ºä¸æŒ‚èµ·,ä¹Ÿå°±æ˜¯å…³é—­ç»ˆç«¯è¿›ç¨‹ä¹Ÿç»§ç»­ä¿æŒè¿è¡ŒçŠ¶æ€

0:æ ‡å‡†è¾“å…¥

1:æ ‡å‡†è¾“å‡º

2:é”™è¯¯è¾“å‡º

`2>&1` : è¡¨ç¤ºå°†é”™è¯¯é‡å®šå‘åˆ°æ ‡å‡†è¾“å‡ºä¸Š

`&`: æ”¾åœ¨å‘½ä»¤ç»“å°¾,è¡¨ç¤ºåå°è¿è¡Œ

ä¸€èˆ¬ä¼šç»„åˆä½¿ç”¨: nohup [xxxå‘½ä»¤æ“ä½œ]> file 2>&1 & ï¼Œ è¡¨ç¤ºå°†xxxå‘½ä»¤è¿è¡Œçš„

ç»“æœè¾“å‡ºåˆ°fileä¸­ï¼Œå¹¶ä¿æŒå‘½ä»¤å¯åŠ¨çš„è¿›ç¨‹åœ¨åå°è¿è¡Œã€‚

**é€ä¸€å¯åŠ¨**

`$ nohup hive --service metastore 2>&1 &`

`$ nohup hive --service hiveserver2 2>&1 &`



### ç¼–å†™è„šæœ¬

~~~sh
[atguigu@hadoop102 hive]$ vim $HIVE_HOME/bin/hiveservices.sh

ğŸ‘‡
#!/bin/bash
HIVE_LOG_DIR=$HIVE_HOME/logs
if [ ! -d $HIVE_LOG_DIR ]
then
	mkdir -p $HIVE_LOG_DIR
fi
#æ£€æŸ¥è¿›ç¨‹æ˜¯å¦è¿è¡Œæ­£å¸¸ï¼Œå‚æ•°1ä¸ºè¿›ç¨‹åï¼Œå‚æ•°2ä¸ºè¿›ç¨‹ç«¯å£
function check_process()
{
    pid=$(ps -ef 2>/dev/null | grep -v grep | grep -i $1 | awk '{print $2}')
    ppid=$(netstat -nltp 2>/dev/null | grep $2 | awk '{print $7}' | cut -d '/' -f 1)
    echo $pid
    [[ "$pid" =~ "$ppid" ]] && [ "$ppid" ] && return 0 || return 1
}

function hive_start()
{
    metapid=$(check_process HiveMetastore 9083)
    cmd="nohup hive --service metastore >$HIVE_LOG_DIR/metastore.log 2>&1 &"
    cmd=$cmd" sleep 4; hdfs dfsadmin -safemode wait >/dev/null 2>&1"
    [ -z "$metapid" ] && eval $cmd || echo "MetastroeæœåŠ¡å·²å¯åŠ¨"
    server2pid=$(check_process HiveServer2 10000)
    cmd="nohup hive --service hiveserver2 >$HIVE_LOG_DIR/hiveServer2.log 2>&1 &"
    [ -z "$server2pid" ] && eval $cmd || echo "HiveServer2æœåŠ¡å·²å¯åŠ¨"
}

function hive_stop()
{
    metapid=$(check_process HiveMetastore 9083)
    [ "$metapid" ] && kill $metapid || echo "MetastoreæœåŠ¡æœªå¯åŠ¨"
    server2pid=$(check_process HiveServer2 10000)
    [ "$server2pid" ] && kill $server2pid || echo "HiveServer2æœåŠ¡æœªå¯åŠ¨"
}

case $1 in
"start")
    hive_start
    ;;
"stop")
    hive_stop
    ;;
"restart")
    hive_stop
    sleep 2
    hive_start
    ;;
"status")
    check_process HiveMetastore 9083 >/dev/null && echo "MetastoreæœåŠ¡è¿è¡Œæ­£å¸¸" || echo "MetastoreæœåŠ¡è¿è¡Œå¼‚å¸¸"
    check_process HiveServer2 10000 >/dev/null && echo "HiveServer2æœåŠ¡è¿è¡Œæ­£å¸¸" || echo "HiveServer2æœåŠ¡è¿è¡Œå¼‚å¸¸"
    ;;
*)
    echo Invalid Args!
    echo 'Usage: '$(basename $0)' start|stop|restart|status'
    ;;
esac

~~~



**æ·»åŠ æ‰§è¡Œæƒé™**

~~~sh
[atguigu@hadoop102 hive]$ chmod u+x $HIVE_HOME/bin/hiveservices.sh
~~~

### Hiveè„šæœ¬æ“ä½œ

~~~sh
[atguigu@hadoop102 hive]$ hiveservices.sh start  ğŸ‘ˆå¯åŠ¨
[atguigu@hadoop102 hive]$hiveservices.sh status  ğŸ‘ˆçŠ¶æ€
[atguigu@hadoop102 hive]$ hiveservices.sh restart  ğŸ‘ˆé‡å¯

~~~



## HIveåŸºæ“

###  Hiveå¸¸ç”¨äº¤äº’å‘½ä»¤



`bin/hive -help`

~~~sh
ğŸŒ¼#1ï¼‰â€œ-eâ€ä¸è¿›å…¥hiveçš„äº¤äº’çª—å£æ‰§è¡Œsqlè¯­å¥
[atguigu@hadoop102 hive]$ bin/hive -e "select id from student;"

ğŸŒ¼#2ï¼‰â€œ-fâ€æ‰§è¡Œè„šæœ¬ä¸­sqlè¯­å¥
   #ï¼ˆ1ï¼‰åœ¨/opt/module/hive/ä¸‹åˆ›å»ºdatasç›®å½•å¹¶åœ¨datasç›®å½•ä¸‹åˆ›å»ºhivef.sqlæ–‡ä»¶
[atguigu@hadoop102 datas]$ touch hivef.sql
   #ï¼ˆ2ï¼‰æ–‡ä»¶ä¸­å†™å…¥æ­£ç¡®çš„sqlè¯­å¥
select *from student;
   #ï¼ˆ3ï¼‰æ‰§è¡Œæ–‡ä»¶ä¸­çš„sqlè¯­å¥
[atguigu@hadoop102 hive]$ bin/hive -f /opt/module/hive/datas/hivef.sql
   #ï¼ˆ4ï¼‰æ‰§è¡Œæ–‡ä»¶ä¸­çš„sqlè¯­å¥å¹¶å°†ç»“æœå†™å…¥æ–‡ä»¶ä¸­
[atguigu@hadoop102 hive]$ bin/hive -f /opt/module/hive/datas/hivef.sql  > /opt/module/datas/hive_result.txt

~~~



### Hiveå…¶ä»–å‘½ä»¤æ“ä½œ

~~~sh
#1ï¼‰é€€å‡ºhiveçª—å£ï¼š
hive(default)>exit;
hive(default)>quit;
åœ¨æ–°ç‰ˆçš„hiveä¸­æ²¡åŒºåˆ«äº†ï¼Œåœ¨ä»¥å‰çš„ç‰ˆæœ¬æ˜¯æœ‰çš„ï¼š
exit:å…ˆéšæ€§æäº¤æ•°æ®ï¼Œå†é€€å‡ºï¼›
quit:ä¸æäº¤æ•°æ®ï¼Œé€€å‡ºï¼›

#2ï¼‰åœ¨hive cliå‘½ä»¤çª—å£ä¸­å¦‚ä½•æŸ¥çœ‹hdfsæ–‡ä»¶ç³»ç»Ÿ
hive(default)>dfs -ls /;

#3ï¼‰æŸ¥çœ‹åœ¨hiveä¸­è¾“å…¥çš„æ‰€æœ‰å†å²å‘½ä»¤
ï¼ˆ1ï¼‰è¿›å…¥åˆ°å½“å‰ç”¨æˆ·çš„æ ¹ç›®å½•/rootæˆ–/home/atguigu
ï¼ˆ2ï¼‰ğŸ§æŸ¥çœ‹. hivehistoryæ–‡ä»¶
[atguig2u@hadoop102 ~]$ cat .hivehistory

~~~



### Hiveå¸¸è§å±æ€§é…ç½®

#### hiveçª—å£æ‰“å°é»˜è®¤åº“å’Œè¡¨å¤´

~~~xml
åœ¨hive-site.xmlä¸­åŠ å…¥å¦‚ä¸‹ä¸¤ä¸ªé…ç½®: 
   <!--æ‰“å° å½“å‰åº“ å’Œ è¡¨å¤´ -->
  <property>
    <name>hive.cli.print.header</name>
    <value>true</value>
  </property>
   <property>
    <name>hive.cli.print.current.db</name>
    <value>true</value>
  </property>

~~~



   <!--æ‰“å° å½“å‰åº“ å’Œ è¡¨å¤´ -->

#### Hiveè¿è¡Œæ—¥å¿—ä¿¡æ¯é…ç½®



> **Hiveçš„logé»˜è®¤å­˜æ”¾åœ¨/tmp/atguigu/hive.logç›®å½•ä¸‹ï¼ˆå½“å‰ç”¨æˆ·åä¸‹ï¼‰**



ğŸ‚**ä¿®æ”¹*hiveçš„logå­˜æ”¾æ—¥å¿—åˆ°*/opt/module/hive/logs**

~~~sh
#ï¼ˆ1ï¼‰ä¿®æ”¹/opt/module/hive/conf/hive-log4j2.properties.templateæ–‡ä»¶åç§°ä¸º
hive-log4j2.properties
[atguigu@hadoop102 conf]$ pwd
/opt/module/hive/conf
[atguigu@hadoop102 conf]$ mv hive-log4j2.properties.template hive-log4j2.properties


#ï¼ˆ2ï¼‰åœ¨hive-log4j.propertiesæ–‡ä»¶ä¸­ä¿®æ”¹logå­˜æ”¾ä½ç½®
property.hive.log.dir=/opt/module/hive/logs
~~~



#### å‚æ•°é…ç½®æ–¹å¼



~~~sh
ğŸ‘‰1ï¼‰æŸ¥çœ‹å½“å‰æ‰€æœ‰çš„é…ç½®ä¿¡æ¯
hive>set;
ğŸ‘‰2ï¼‰å‚æ•°çš„é…ç½®ä¸‰ç§æ–¹å¼
     ğŸŒ´#ï¼ˆ1ï¼‰é…ç½®æ–‡ä»¶æ–¹å¼
é»˜è®¤é…ç½®æ–‡ä»¶ï¼šhive-default.xml 
ç”¨æˆ·è‡ªå®šä¹‰é…ç½®æ–‡ä»¶ï¼šhive-site.xml

     ğŸŒ´#ï¼ˆ2ï¼‰å‘½ä»¤è¡Œå‚æ•°æ–¹å¼
å¯åŠ¨Hiveæ—¶ï¼Œå¯ä»¥åœ¨å‘½ä»¤è¡Œæ·»åŠ -hiveconf param=valueæ¥è®¾å®šå‚æ•°ã€‚
[atguigu@hadoop103 hive]$ bin/hive -hiveconf mapred.reduce.tasks=10;


     ğŸŒ´#ï¼ˆ3ï¼‰å‚æ•°å£°æ˜æ–¹å¼
å¯ä»¥åœ¨HQLä¸­ä½¿ç”¨SETå…³é”®å­—è®¾å®šå‚æ•°
hive (default)> set mapred.reduce.tasks=100;
æŸ¥çœ‹å‚æ•°è®¾ç½®
hive (default)> set mapred.reduce.tasks;

~~~

â€‹																	ğŸ‘‡default.xml	ğŸ‘‡site.xml

ä¸Šè¿°ä¸‰ç§è®¾å®šæ–¹å¼çš„ä¼˜å…ˆçº§ä¾æ¬¡é€’å¢ã€‚å³**é»˜è®¤é…ç½®æ–‡ä»¶<é…ç½®æ–‡ä»¶<å‘½ä»¤è¡Œå‚æ•°<å‚æ•°å£°æ˜ã€‚**æ³¨æ„æŸäº›ç³»ç»Ÿçº§çš„å‚æ•°ï¼Œä¾‹å¦‚log4jç›¸å…³çš„è®¾å®šï¼Œå¿…é¡»ç”¨å‰ä¸¤ç§æ–¹å¼è®¾å®šï¼Œå› ä¸ºé‚£äº›å‚æ•°çš„è¯»å–åœ¨ä¼šè¯å»ºç«‹ä»¥å‰å·²ç»å®Œæˆäº†



ğŸš©==æ³¨æ„ï¼š==ç”¨æˆ·è‡ªå®šä¹‰é…ç½®ä¼šè¦†ç›–é»˜è®¤é…ç½®ã€‚å¦å¤–ï¼ŒHiveä¹Ÿä¼šè¯»å…¥Hadoopçš„é…ç½®ï¼Œå› ä¸ºHiveæ˜¯ä½œä¸ºHadoopçš„å®¢æˆ·ç«¯å¯åŠ¨çš„ï¼ŒHiveçš„é…ç½®ä¼šè¦†ç›–Hadoopçš„é…ç½®ã€‚é…ç½®æ–‡ä»¶çš„è®¾å®šå¯¹æœ¬æœºå¯åŠ¨çš„æ‰€æœ‰Hiveè¿›ç¨‹éƒ½æœ‰æ•ˆã€‚



==æ³¨æ„==ï¼š**å‘½ä»¤è¡Œå’Œå‚æ•°å£°æ˜æ–¹å¼ä»…å¯¹æœ¬æ¬¡hiveå¯åŠ¨æœ‰æ•ˆ**





# ä¸‰ã€Hiveæ•°æ®ç±»å‹



## åŸºæœ¬æ•°æ®ç±»å‹



| Hiveæ•°æ®ç±»å‹ | Javaæ•°æ®ç±»å‹ | é•¿åº¦                                                 | ä¾‹å­                                  |
| ------------ | ------------ | ---------------------------------------------------- | ------------------------------------- |
| TINYINT      | byte         | 1byteæœ‰ç¬¦å·æ•´æ•°                                      | 20                                    |
| SMALINT      | short        | 2byteæœ‰ç¬¦å·æ•´æ•°                                      | 20                                    |
| ==INT==      | int          | 4byteæœ‰ç¬¦å·æ•´æ•°                                      | 20                                    |
| ==BIGINT==   | long         | 8byteæœ‰ç¬¦å·æ•´æ•°                                      | 20                                    |
| BOOLEAN      | boolean      | å¸ƒå°”ç±»å‹ï¼Œtrueæˆ–è€…false                              | TRUE FALSE                            |
| FLOAT        | float        | å•ç²¾åº¦æµ®ç‚¹æ•°                                         | 3.14159                               |
| ==DOUBLE==   | double       | åŒç²¾åº¦æµ®ç‚¹æ•°                                         | 3.14159                               |
| ==STRING==   | string       | å­—ç¬¦ç³»åˆ—ã€‚å¯ä»¥æŒ‡å®šå­—ç¬¦é›†ã€‚å¯ä»¥ä½¿ç”¨å•å¼•å·æˆ–è€…åŒå¼•å·ã€‚ | â€˜now is the timeâ€™  â€œfor all good menâ€ |
| TIMESTAMP    |              | æ—¶é—´ç±»å‹                                             |                                       |
| BINARY       |              | å­—èŠ‚æ•°ç»„                                             |                                       |



å¯¹äºHiveçš„Stringç±»å‹ç›¸å½“äºæ•°æ®åº“çš„varcharç±»å‹ï¼Œè¯¥ç±»å‹æ˜¯ä¸€ä¸ªå¯å˜çš„å­—ç¬¦ä¸²ï¼Œä¸è¿‡å®ƒä¸èƒ½å£°æ˜å…¶ä¸­æœ€å¤šèƒ½å­˜å‚¨å¤šå°‘ä¸ªå­—ç¬¦ï¼Œç†è®ºä¸Šå®ƒå¯ä»¥å­˜å‚¨2GBçš„å­—ç¬¦æ•°ã€‚



## é›†åˆæ•°æ®ç±»å‹



| æ•°æ®ç±»å‹ | æè¿°                                                         | è¯­æ³•ç¤ºä¾‹                                         |
| -------- | ------------------------------------------------------------ | ------------------------------------------------ |
| STRUCT   | å’Œcè¯­è¨€ä¸­çš„structç±»ä¼¼ï¼Œéƒ½å¯ä»¥é€šè¿‡â€œç‚¹â€ç¬¦å·è®¿é—®å…ƒç´ å†…å®¹ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæŸä¸ªåˆ—çš„æ•°æ®ç±»å‹æ˜¯STRUCT{first STRING, last  STRING},é‚£ä¹ˆç¬¬1ä¸ªå…ƒç´ å¯ä»¥é€šè¿‡å­—æ®µ.firstæ¥å¼•ç”¨ã€‚ | struct()  ä¾‹å¦‚struct<street:string, city:string> |
| MAP      | MAPæ˜¯ä¸€ç»„é”®-å€¼å¯¹å…ƒç»„é›†åˆï¼Œä½¿ç”¨æ•°ç»„è¡¨ç¤ºæ³•å¯ä»¥è®¿é—®æ•°æ®ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæŸä¸ªåˆ—çš„æ•°æ®ç±»å‹æ˜¯MAPï¼Œå…¶ä¸­é”®->å€¼å¯¹æ˜¯â€™firstâ€™->â€™Johnâ€™å’Œâ€™lastâ€™->â€™Doeâ€™ï¼Œé‚£ä¹ˆå¯ä»¥é€šè¿‡å­—æ®µå[â€˜lastâ€™]è·å–æœ€åä¸€ä¸ªå…ƒç´  | map()  ä¾‹å¦‚map<string, int>                      |
| ARRAY    | æ•°ç»„æ˜¯ä¸€ç»„å…·æœ‰ç›¸åŒç±»å‹å’Œåç§°çš„å˜é‡çš„é›†åˆã€‚è¿™äº›å˜é‡ç§°ä¸ºæ•°ç»„çš„å…ƒç´ ï¼Œæ¯ä¸ªæ•°ç»„å…ƒç´ éƒ½æœ‰ä¸€ä¸ªç¼–å·ï¼Œç¼–å·ä»é›¶å¼€å§‹ã€‚ä¾‹å¦‚ï¼Œæ•°ç»„å€¼ä¸º[â€˜Johnâ€™, â€˜Doeâ€™]ï¼Œé‚£ä¹ˆç¬¬2ä¸ªå…ƒç´ å¯ä»¥é€šè¿‡æ•°ç»„å[1]è¿›è¡Œå¼•ç”¨ã€‚ | Array()  ä¾‹å¦‚array<string>                       |



Hiveæœ‰ä¸‰ç§å¤æ‚æ•°æ®ç±»å‹ARRAYã€MAP å’Œ STRUCTã€‚**ARRAYå’ŒMAPä¸Javaä¸­çš„Arrayå’ŒMapç±»ä¼¼ï¼Œè€ŒSTRUCTä¸Javaè¯­è¨€ä¸­çš„å¯¹è±¡ç±»ä¼¼ï¼Œ**å®ƒå°è£…äº†ä¸€ä¸ªå­—æ®µé›†åˆï¼Œå¤æ‚æ•°æ®ç±»å‹å…è®¸ä»»æ„å±‚æ¬¡çš„åµŒå¥—ã€‚



> æ¡ˆä¾‹



ç”¨JSONæ ¼å¼æ¥è¡¨ç¤ºå…¶æ•°æ®ç»“æ„ã€‚åœ¨Hiveä¸‹è®¿é—®çš„æ ¼å¼ä¸º

~~~json
{
    "name": "songsong",
    "friends": ["bingbing" , "lili"] ,        //åˆ—è¡¨Array, 
    "children": {                            //é”®å€¼Map,
        "xiao song": 19 ,
        "xiaoxiao song": 18
    },
    "address": {                            //ç»“æ„Struct,
        "street": "hui long guan" ,
        "city": "beijing" 
    }
}

~~~



åœ¨Hiveé‡Œåˆ›å»ºå¯¹åº”çš„è¡¨ï¼Œå¹¶å¯¼å…¥æ•°æ®ã€‚

Hiveä¸Šåˆ›å»ºæµ‹è¯•è¡¨test   

 ==æ³¨æ„ï¼š==nameç­‰å­—æ®µå‰ä¸è¦æœ‰**\t**å¿…é¡»ä¸ºç©ºæ ¼æˆ–ç©º[^TABä¸Linuxè‡ªåŠ¨è¡¥å…¨å†²çª]

~~~sql
create table test(
name string,
friends array<string>,
children map<string, int>,
address struct<street:string, city:string>
)
row format delimited fields terminated by ','    
collection items terminated by '_'             
map keys terminated by ':'
lines terminated by '\n';
~~~

ğŸ§

| row format delimited fields terminated by ',' | åˆ—åˆ†éš”ç¬¦                                                 |
| --------------------------------------------- | -------------------------------------------------------- |
| collection items terminated by '_'            | **MAP**   **STRUCT** å’Œ **ARRAY** çš„åˆ†éš”ç¬¦(æ•°æ®åˆ†å‰²ç¬¦å·) |
| map keys terminated by ':'                    | MAPä¸­çš„keyä¸valueçš„åˆ†éš”ç¬¦                                |
| lines terminated by '\n';                     | è¡Œåˆ†éš”ç¬¦                                                 |





âš™**å¯¼å…¥æ–‡æœ¬æ•°æ®åˆ°æµ‹è¯•è¡¨**

åˆ›å»ºæ–‡æœ¬å†™å…¥å†…å®¹ğŸ“

~~~txt
songsong   ,  bingbing_lili  ,  xiao song:18_xiaoxiao song:19  ,   hui long guan_beijing
yangyang   ,  caicai_susu    ,  xiao yang:18_xiaoxiao yang:19  ,   chao yang_beijing
~~~

å¯¼å…¥é›†ç¾¤ï¼ˆæ˜ å°„åˆ°testè¡¨ï¼‰

`load data local inpath '/opt/module/hive/datas/test.txt' into table test;` 



**è®¿é—®ä¸‰ç§é›†åˆåˆ—é‡Œçš„æ•°æ®ï¼Œ**ä»¥ä¸‹åˆ†åˆ«æ˜¯ARRAYï¼ŒMAPï¼ŒSTRUCTçš„è®¿é—®æ–¹å¼

~~~sql
hive (default)> select friends[1],children['xiao song'],address.city from test
where name="songsong";
OK
_c0     _c1     city

lili    18      beijing
Time taken: 0.076 seconds, Fetched: 1 row(s)

~~~





## ç±»å‹è½¬æ¢



Hiveçš„åŸå­æ•°æ®ç±»å‹æ˜¯å¯ä»¥è¿›è¡Œéšå¼è½¬æ¢çš„ï¼Œ**ç±»ä¼¼äºJavaçš„ç±»å‹è½¬æ¢**ï¼Œä¾‹å¦‚æŸè¡¨è¾¾å¼ä½¿ç”¨INTç±»å‹ï¼ŒTINYINTä¼šè‡ªåŠ¨è½¬æ¢ä¸ºINTç±»å‹ï¼Œä½†æ˜¯Hiveä¸ä¼šè¿›è¡Œåå‘è½¬åŒ–ï¼Œä¾‹å¦‚ï¼ŒæŸè¡¨è¾¾å¼ä½¿ç”¨TINYINTç±»å‹ï¼ŒINTä¸ä¼šè‡ªåŠ¨è½¬æ¢ä¸ºTINYINTç±»å‹ï¼Œå®ƒä¼šè¿”å›é”™è¯¯ï¼Œ==é™¤éä½¿ç”¨CASTæ“ä½œã€‚==



**éšå¼ç±»å‹è½¬æ¢è§„åˆ™å¦‚ä¸‹**

> ä»å°åˆ°å¤§è‡ªåŠ¨è½¬æ¢
>

ğŸ“ä»»ä½•æ•´æ•°ç±»å‹éƒ½å¯ä»¥éšå¼åœ°è½¬æ¢ä¸ºä¸€ä¸ªèŒƒå›´æ›´å¹¿çš„ç±»å‹ï¼Œå¦‚TINYINTå¯ä»¥è½¬æ¢æˆINTï¼ŒINTå¯ä»¥è½¬æ¢æˆBIGINTã€‚                        è‡ªåŠ¨è½¬æ¢

ğŸ“æ‰€æœ‰æ•´æ•°ç±»å‹ã€FLOATå’ŒSTRINGç±»å‹éƒ½å¯ä»¥éšå¼åœ°è½¬æ¢æˆDOUBLEã€‚

ğŸ“TINYINTã€SMALLINTã€INTéƒ½å¯ä»¥è½¬æ¢ä¸ºFLOATã€‚

ğŸ“BOOLEANç±»å‹ä¸å¯ä»¥è½¬æ¢ä¸ºä»»ä½•å…¶å®ƒçš„ç±»å‹ã€‚





**ä½¿ç”¨CASTæ“ä½œæ˜¾ç¤ºè¿›è¡Œæ•°æ®ç±»å‹è½¬æ¢**

> ä»å¤§åˆ°å°å¼ºåˆ¶è½¬æ¢
>



`CAST('1' AS INT)`å°†æŠŠå­—ç¬¦ä¸²'1' è½¬æ¢æˆæ•´æ•°1ï¼›å¦‚æœå¼ºåˆ¶ç±»å‹è½¬æ¢å¤±è´¥ï¼Œå¦‚æ‰§è¡Œ`CAST('X' AS INT)`ï¼Œè¡¨è¾¾å¼è¿”å›ç©ºå€¼ NULLã€‚






# å››ã€DDLæ•°æ®å®šä¹‰



## åº“ç›¸å…³

### åˆ›å»ºæ•°æ®åº“



> å»ºåº“è¯­æ³•
>

~~~mysql
CREATE DATABASE [IF NOT EXISTS] database_name
[COMMENT database_comment]							ğŸ‰åº“æ³¨é‡Š
[LOCATION hdfs_path]      							ğŸ‰æŒ‡å®šåˆ›å»ºåœ¨HDFSçš„è·¯å¾„
[WITH DBPROPERTIES (property_name=property_value, ...)]; 		
~~~



**åˆ›å»ºæ•°æ®åº“**

~~~mysql
create database if not exists mydb   
comment "my first db"
location '/db_hive2.db'
with dbproperties("createtime"="2021-04-24");
~~~



â€‹		==æ•°æ®åº“åœ¨HDFSä¸Šçš„é»˜è®¤å­˜å‚¨è·¯å¾„æ˜¯/user/hive/warehouse/\*.db==



### æŸ¥è¯¢æ•°æ®åº“

ğŸ

| **æ˜¾ç¤ºæ•°æ®åº“**                   | hive> show databases;                     |
| -------------------------------- | ----------------------------------------- |
| **è¿‡æ»¤æ˜¾ç¤ºæŸ¥è¯¢çš„æ•°æ®åº“**         | **hive> show databases like 'db_hive*';** |
| **æ˜¾ç¤ºæ•°æ®åº“ä¿¡æ¯**               | **hive> desc database db_hive;**          |
| **æ˜¾ç¤ºæ•°æ®åº“è¯¦ç»†ä¿¡æ¯ï¼Œextended** | **hive> desc database extended db_hive;** |
| **åˆ‡æ¢å½“å‰æ•°æ®åº“**               | **hive (default)> use db_hive;**          |





### ä¿®æ”¹æ•°æ®åº“



ç”¨æˆ·å¯ä»¥ä½¿ç”¨ALTER DATABASEå‘½ä»¤ä¸ºæŸä¸ªæ•°æ®åº“çš„DBPROPERTIES**è®¾ç½®é”®-å€¼å¯¹å±æ€§å€¼ï¼Œæ¥æè¿°è¿™ä¸ªæ•°æ®åº“çš„å±æ€§ä¿¡æ¯ã€‚**



`alter database mydb set dbproperties("createtime"="2020-04-24","author"="wyh");`







### åˆ é™¤æ•°æ®åº“

| **åˆ é™¤ç©ºæ•°æ®åº“**                                    | hive>drop database db_hive2;             |
| --------------------------------------------------- | ---------------------------------------- |
| **æ•°æ®åº“ä¸ä¸ºç©ºï¼Œå¯ä»¥é‡‡ç”¨==cascade==å‘½ä»¤ï¼Œå¼ºåˆ¶åˆ é™¤** | **hive> drop database db_hive cascade;** |



## è¡¨ç›¸å…³

### åˆ›å»ºè¡¨

==ä¸­æ‹¬å·é‡Œå³ä¸ºå¯çœç•¥å†…å®¹==



> å»ºè¡¨è¯­æ³•
>

ğŸ§

| è¯­æ³•                                                         | è§£é‡Š                                     |
| ------------------------------------------------------------ | ---------------------------------------- |
| ==CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name==       | EXTERANL: å¤–éƒ¨è¡¨                         |
| ==[(col_name data_type [COMMENT col_comment], ...)]==        | åˆ—å åˆ—ç±»å‹ åˆ—æè¿°ä¿¡æ¯                   |
| [COMMENT table_comment]                                      | è¡¨æè¿°ä¿¡æ¯                               |
| [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] | åˆ›å»ºåˆ†åŒºè¡¨æŒ‡å®šåˆ†åŒºå­—æ®µ  åˆ†åŒºåˆ—å  åˆ—ç±»å‹ |
| [CLUSTERED BY (col_name, col_name, ...)]                     | åˆ›å»ºåˆ†æ¡¶è¡¨æŒ‡å®šåˆ†æ¡¶å­—æ®µ   åˆ†æ¡¶åˆ—å        |
| [SORTED BY (col_name [ASCDESC], ...)] INTO num_buckets BUCKETS] | æŒ‡å®šåˆ†æ¡¶æ•°                               |
| ==[ROW FORMAT delimited fields terminated by ... ]==         | æŒ‡å®šä¸€æ¡æ•°æ®å­—æ®µä¸å­—æ®µçš„åˆ†å‰²ç¬¦           |
| ==[collection items terminated by  ... ]==                   | æŒ‡å®šé›†åˆå…ƒç´ ä¸å…ƒç´ çš„åˆ†å‰²ç¬¦               |
| ==[map keys terminated by ... ]==                            | æŒ‡å®šmapçš„kvçš„åˆ†å‰²ç¬¦                      |
| [STORED AS file_format]                                      | æŒ‡å®šæ–‡ä»¶å­˜å‚¨æ ¼å¼ï¼Œé»˜è®¤ä¸º textfile        |
| [LOCATION hdfs_path]                                         | æŒ‡å®šè¡¨åœ¨hdfsä¸­å¯¹åº”çš„è·¯å¾„                 |
| [TBLPROPERTIES (property_name=property_value, ...)]          | æŒ‡å®šè¡¨çš„å±æ€§                             |
| [AS select_statement]                                        | åŸºäºæŸä¸ªæŸ¥è¯¢å»ºè¡¨                         |

  

>  ç®¡ç†è¡¨(å†…éƒ¨è¡¨)



é»˜è®¤åˆ›å»ºçš„è¡¨éƒ½æ˜¯æ‰€è°“çš„ç®¡ç†è¡¨ï¼Œæœ‰æ—¶ä¹Ÿè¢«ç§°ä¸º**å†…éƒ¨è¡¨**ã€‚å› ä¸ºè¿™ç§è¡¨ï¼ŒHiveä¼šï¼ˆæˆ–å¤šæˆ–å°‘åœ°ï¼‰æ§åˆ¶ç€æ•°æ®çš„ç”Ÿå‘½å‘¨æœŸã€‚Hiveé»˜è®¤æƒ…å†µä¸‹ä¼šå°†è¿™äº›è¡¨çš„æ•°æ®å­˜å‚¨åœ¨ç”±é…ç½®é¡¹hive.metastore.warehouse.dir(ä¾‹å¦‚ï¼Œ/user/hive/warehouse)æ‰€å®šä¹‰çš„ç›®å½•çš„å­ç›®å½•ä¸‹ã€‚  ==å½“æˆ‘ä»¬åˆ é™¤ä¸€ä¸ªç®¡ç†è¡¨æ—¶ï¼ŒHiveä¹Ÿä¼šåˆ é™¤è¿™ä¸ªè¡¨ä¸­æ•°æ®==ã€‚ç®¡ç†è¡¨ä¸é€‚åˆå’Œå…¶ä»–å·¥å…·å…±äº«æ•°æ®ã€‚





> å¤–éƒ¨è¡¨



å› ä¸ºè¡¨æ˜¯å¤–éƒ¨è¡¨ï¼Œæ‰€ä»¥Hiveå¹¶éè®¤ä¸ºå…¶å®Œå…¨æ‹¥æœ‰è¿™ä»½æ•°æ®ã€‚**åˆ é™¤è¯¥è¡¨å¹¶ä¸ä¼šåˆ é™¤æ‰è¿™ä»½æ•°æ®ï¼Œä¸è¿‡æè¿°è¡¨çš„å…ƒæ•°æ®ä¿¡æ¯ä¼šè¢«åˆ é™¤æ‰ã€‚**



ğŸ”¥å¤–éƒ¨è¡¨ç”¨çš„å¤š      åˆ è¡¨ä¸åˆ æ•°æ®



------

**ç®¡ç†è¡¨å’Œå¤–éƒ¨è¡¨çš„ä½¿ç”¨åœºæ™¯**

æ¯å¤©å°†æ”¶é›†åˆ°çš„ç½‘ç«™æ—¥å¿—å®šæœŸæµå…¥HDFSæ–‡æœ¬æ–‡ä»¶ã€‚åœ¨å¤–éƒ¨è¡¨ï¼ˆåŸå§‹æ—¥å¿—è¡¨ï¼‰çš„åŸºç¡€ä¸Šåšå¤§é‡çš„ç»Ÿè®¡åˆ†æï¼Œç”¨åˆ°çš„ä¸­é—´è¡¨ã€ç»“æœè¡¨ä½¿ç”¨å†…éƒ¨è¡¨å­˜å‚¨ï¼Œæ•°æ®é€šè¿‡SELECT+INSERTè¿›å…¥å†…éƒ¨è¡¨ã€‚



**ç®¡ç†è¡¨ä¸å¤–éƒ¨è¡¨çš„äº’ç›¸è½¬æ¢**

`alter table student2 set tblproperties('EXTERNAL'='TRUE');`     å†…è½¬å¤–





âš™**å»ºè¡¨**

~~~sql
ğŸ”¨
create table if not exists test1(
  id int comment "this's id ",
  name string  comment "this   name"
)
comment "this  table"
row format delimited fields terminated by ','
STORED as textfile 
TBLPROPERTIES("createtime"="2021-04-24")

ğŸ”¨
create table if not exists test2(
  id int ,
  name string 
)
row format delimited fields terminated by ','
location "/test2.table"

ğŸ”¨
create table if not exists test3(
  id int ,
  name string 
)
row format delimited fields terminated by ','
location "/mydata" ;

ğŸ”¨
create table if not exists test4(
  id int ,
  name string 
)
row format delimited fields terminated by ',' ;

ğŸ”¨
create external table if not exists test5(
  id int ,
  name string 
)
row format delimited fields terminated by ',' ;
~~~





### ä¿®æ”¹è¡¨



**ğŸ”—é‡å‘½åè¡¨**

`ALTER TABLE è¡¨å RENAME TO æ–°è¡¨å`



columnï¼šå•ä¸ªåˆ—åé¢å‚æ•°ä¸åŠ æ‹¬å·     columnsï¼šå¤šä¸ªåˆ—ç”¨æ‹¬å·åŒ…èµ·æ¥

**ğŸ”—å¢åŠ /ä¿®æ”¹/æ›¿æ¢åˆ—ä¿¡æ¯**													

`ALTER TABLE è¡¨å CHANGE COLUMN ï¼ˆæ—§åˆ—å æ–°åˆ—å åˆ—ç±»å‹ï¼‰ [åˆ—æ³¨é‡Š] [FIRST|AFTER column_name]`

ä¾‹å­ï¼š

~~~mysql
hive (default)> alter table dept change column deptdesc desc string;
~~~

**ğŸ”—æ”¹åˆ—å**

`alter table åˆ—å rename to æ–°åˆ—å ;` 



**ğŸ”—å¢åŠ åˆ—**

â€‹													ğŸ‘‡å¢åŠ å¤šä¸ªåˆ—

`alter table åˆ—å add columns (addr string, deptno int );`



**ğŸ”—æ›¿æ¢åˆ—**

`alter table åˆ—å/è¡¨å replace columns (empid int, empname string);`



==æ³¨==ï¼šADDæ˜¯ä»£è¡¨æ–°å¢ä¸€å­—æ®µï¼Œå­—æ®µä½ç½®åœ¨æ‰€æœ‰åˆ—åé¢(partitionåˆ—å‰)ï¼Œ**REPLACEåˆ™æ˜¯è¡¨ç¤ºæ›¿æ¢è¡¨æˆ–åˆ—ä¸­æ‰€æœ‰å­—æ®µã€‚**







### åˆ é™¤è¡¨

------

`drop table è¡¨åå­—;`





# äº”ã€DMLæ•°æ®æ“ä½œ





## æ•°æ®å¯¼å…¥

==[  ]è¡¨ç¤ºå¯çœç•¥å†…å®¹==



### å‘è¡¨ä¸­è£…è½½æ•°æ®Load

**è¯­æ³•**

~~~sql
hive> load data [local] inpath 'æ•°æ®çš„path' [overwrite] into table student [partition (partcol1=val1,â€¦)];
~~~

ï¼ˆ1ï¼‰load data:è¡¨ç¤ºåŠ è½½æ•°æ®

ï¼ˆ2ï¼‰local:è¡¨ç¤ºä»æœ¬åœ°åŠ è½½æ•°æ®åˆ°hiveè¡¨ï¼›å¦åˆ™ä»HDFSåŠ è½½æ•°æ®åˆ°hiveè¡¨

ï¼ˆ3ï¼‰inpath:è¡¨ç¤ºåŠ è½½æ•°æ®çš„è·¯å¾„

ï¼ˆ4ï¼‰overwrite:è¡¨ç¤ºè¦†ç›–è¡¨ä¸­å·²æœ‰æ•°æ®ï¼Œå¦åˆ™è¡¨ç¤ºè¿½åŠ 

ï¼ˆ5ï¼‰into table:è¡¨ç¤ºåŠ è½½åˆ°å“ªå¼ è¡¨

ï¼ˆ6ï¼‰student:è¡¨ç¤ºå…·ä½“çš„è¡¨

ï¼ˆ7ï¼‰partition:è¡¨ç¤ºä¸Šä¼ åˆ°æŒ‡å®šåˆ†åŒº



> æ¡ˆä¾‹



~~~mysql
æœ¬åœ°è·¯å¾„å¯¼å…¥
load data local inpath '/opt/module/hive-3.1.2/datas/student1.txt' into table student; 

æœ¬åœ°è·¯å¾„å¯¼å…¥è¦†å†™
load data local inpath '/opt/module/hive-3.1.2/datas/student2.txt' overwrite into table student; 


load data inpath '/hivedatas/student.txt' into table student; 

~~~







### é€šè¿‡æŸ¥è¯¢è¯­å¥å‘è¡¨ä¸­æ’å…¥æ•°æ®Insert



insert  intoï¼šä»¥è¿½åŠ æ•°æ®çš„æ–¹å¼æ’å…¥åˆ°è¡¨æˆ–åˆ†åŒºï¼ŒåŸæœ‰æ•°æ®ä¸ä¼šåˆ é™¤

insert  overwriteï¼šä¼šè¦†ç›–è¡¨ä¸­å·²å­˜åœ¨çš„æ•°æ®

==æ³¨æ„ï¼šinsertä¸æ”¯æŒæ’å…¥éƒ¨åˆ†å­—æ®µ==



> æ¡ˆä¾‹

~~~mysql
insert into student values(1017,'ss17'),(1018,'ss18'),(1019,'ss19');  

insert into student2 select id, name from student ;  
~~~





### æŸ¥è¯¢è¯­å¥ä¸­åˆ›å»ºè¡¨å¹¶åŠ è½½æ•°æ®As Select



æ ¹æ®æŸ¥è¯¢ç»“æœåˆ›å»ºè¡¨ï¼ˆæŸ¥è¯¢çš„ç»“æœä¼šæ·»åŠ åˆ°æ–°åˆ›å»ºçš„è¡¨ä¸­ï¼‰



> æ¡ˆä¾‹
>

~~~sql
create table student3 as select id, name from student ;
~~~







### åˆ›å»ºè¡¨æ—¶é€šè¿‡LocationæŒ‡å®šåŠ è½½æ•°æ®è·¯å¾„





> æ¡ˆä¾‹
>



~~~sql
create table student4(id string, name string)
row format delimited fields terminated by '\t'
location '/student4' ;
~~~





### Importæ•°æ®åˆ°æŒ‡å®šHiveè¡¨ä¸­

==æ³¨æ„ï¼šå…ˆç”¨exportå¯¼å‡ºåï¼Œå†å°†æ•°æ®å¯¼å…¥ã€‚==



> æ¡ˆä¾‹

~~~sql
import table emptest2 from '/emptest';
~~~









## æ•°æ®å¯¼å‡º



### Insertå¯¼å‡º



> æ¡ˆä¾‹
>



~~~sql
/*å°†æŸ¥è¯¢çš„ç»“æœå¯¼å‡ºåˆ°æœ¬åœ° */
						ğŸ‘‡
insert overwrite local directory '/opt/module/hive-3.1.2/datas/insert-result' select * from  emptest ;  

/*å°†æŸ¥è¯¢çš„ç»“æœæ ¼å¼åŒ–å¯¼å‡ºåˆ°æœ¬åœ°  */
						ğŸ‘‡
insert overwrite local directory '/opt/module/hive-3.1.2/datas/insert-result' 
row format delimited fields terminated by ':'
select * from emptest ;

/* å°†æŸ¥è¯¢çš„ç»“æœå¯¼å‡ºåˆ°HDFS */
					ğŸ‘‡
insert overwrite  directory '/insert-result' 
row format delimited fields terminated by ':'
select * from emptest ;
~~~



==å¦‚æœè¡¨ä¸­çš„åˆ—çš„å€¼ä¸ºnull,å¯¼å‡ºåˆ°æ–‡ä»¶ä¸­ä»¥åä¼šé€šè¿‡\Næ¥è¡¨ç¤º.==



### Hadoopå‘½ä»¤å¯¼å‡ºåˆ°æœ¬åœ°

> æ¡ˆä¾‹
>

~~~sql

hive (default)> dfs -get /user/hive/warehouse/student/student.txt
/opt/module/datas/export/student3.txt;

~~~



### Hive Shell å‘½ä»¤å¯¼å‡º

**åŸºæœ¬è¯­æ³•**ï¼šï¼ˆ`hive -f/-e æ‰§è¡Œè¯­å¥æˆ–è€…è„šæœ¬ > file`ï¼‰



> æ¡ˆä¾‹
>

~~~sh
											    ğŸ‘‡
$ bin/hive -e 'select * from default.student;'   >
 /opt/module/hive/datas/export/student4.txt;

~~~





### Exportå¯¼å‡ºåˆ°HDFSä¸Š



==åŒ…å«å…ƒæ•°æ® å¯ç”¨äºé›†ç¾¤æ•°æ®è¿ç§»==

**åŸºæœ¬è¯­æ³•ï¼š**ï¼ˆ`export  table  æ•°æ®åº“.æ•°æ®è¡¨  to 'HDFSè·¯å¾„';`ï¼‰



> æ¡ˆä¾‹
>

~~~sql

export table mydb.emptest to '/emptest' ;
~~~





### æ¸…é™¤è¡¨ä¸­æ•°æ®ï¼ˆTruncateï¼‰



==æ³¨æ„ï¼šTruncateåªèƒ½åˆ é™¤å†…éƒ¨è¡¨çš„è¡¨å’Œè¡¨æ•°æ®ï¼Œä¸èƒ½åˆ é™¤å¤–éƒ¨è¡¨ä¸­æ•°æ®==



> æ¡ˆä¾‹
>

~~~sql
truncate table student;
~~~







# å…­ã€æŸ¥è¯¢

[æ–‡æ¡£é“¾æ¥](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select)



> ğŸ§æŸ¥è¯¢è¯­å¥è¯­æ³•
>

~~~sql
SELECT [ALL | DISTINCT] select_expr, select_expr, ...  -- æŸ¥å“ªäº›
  FROM table_reference  -- ä»å“ªæŸ¥
  [WHERE where_condition]  -- è¿‡æ»¤æ¡ä»¶
  [GROUP BY col_list] -- åˆ†ç»„
  [HAVING having_contiditon] -- åˆ†ç»„åè¿‡æ»¤æ¡ä»¶
  [ORDER BY col_list] -- å…¨å±€æ’åº
  [CLUSTER BY col_list  -- åˆ†åŒºæ’åº
    ||  --ç­‰äº
   [DISTRIBUTE BY col_list]  -- åˆ†åŒº
   [SORT BY col_list] -- åŒºå†…æ’åº
  ]
 [LIMIT number] -- é™åˆ¶è¿”å›çš„æ¡æ•°
~~~



## åŸºæœ¬æŸ¥è¯¢



### å…¨è¡¨æŸ¥è¯¢



~~~sql
hive (default)> select * from è¡¨å;
~~~



### é€‰æ‹©ç‰¹å®šåˆ—æŸ¥è¯¢



~~~sql
hive (default)> select åˆ—å1, åˆ—å2 from è¡¨å;
~~~



==æ³¨æ„ï¼š==

+ SQL è¯­è¨€å¤§å°å†™ä¸æ•æ„Ÿã€‚
+ SQL å¯ä»¥å†™åœ¨ä¸€è¡Œæˆ–è€…å¤šè¡Œ
+ å…³é”®å­—ä¸èƒ½è¢«ç¼©å†™ä¹Ÿä¸èƒ½åˆ†è¡Œ
+ å„å­å¥ä¸€èˆ¬è¦åˆ†è¡Œå†™ã€‚
+ ä½¿ç”¨ç¼©è¿›æé«˜è¯­å¥çš„å¯è¯»æ€§ã€‚



### åˆ—åˆ«å

 ==å…³é”®å­—â€œASâ€   åŠ ä¸åŠ éƒ½è¡Œ æ³¨æ„æ ¼å¼å°±OK==

~~~sql
select åå­— AS æ–°åå­—, åå­— æ–°åå­— from è¡¨å;
~~~





### ç®—æœ¯è¿ç®—ç¬¦



| è¿ç®—ç¬¦ | æè¿°           |
| ------ | -------------- |
| A+B    | Aå’ŒB ç›¸åŠ       |
| A-B    | Aå‡å»B         |
| A*B    | Aå’ŒB ç›¸ä¹˜      |
| A/B    | Aé™¤ä»¥B         |
| A%B    | Aå¯¹Bå–ä½™       |
| A&B    | Aå’ŒBæŒ‰ä½å–ä¸   |
| A\|B   | Aå’ŒBæŒ‰ä½å–æˆ–   |
| A^B    | Aå’ŒBæŒ‰ä½å–å¼‚æˆ– |
| ~A     | AæŒ‰ä½å–å      |



> æ¡ˆä¾‹
>

æŸ¥è¯¢å‡ºæ‰€æœ‰å‘˜å·¥çš„è–ªæ°´ååŠ 1æ˜¾ç¤ºã€‚

~~~sql
hive (default)> select sal +1 from emp;
~~~





### å¸¸ç”¨å‡½æ•°



~~~sql
1ï¼‰æ±‚æ€»è¡Œæ•°ï¼ˆcountï¼‰
hive (default)> select count(*) cnt from emp;

2ï¼‰æ±‚å·¥èµ„çš„æœ€å¤§å€¼ï¼ˆmaxï¼‰
hive (default)> select max(sal) max_sal from emp;

3ï¼‰æ±‚å·¥èµ„çš„æœ€å°å€¼ï¼ˆminï¼‰
hive (default)> select min(sal) min_sal from emp;

4ï¼‰æ±‚å·¥èµ„çš„æ€»å’Œï¼ˆsumï¼‰
hive (default)> select sum(sal) sum_sal from emp; 

5ï¼‰æ±‚å·¥èµ„çš„å¹³å‡å€¼ï¼ˆavgï¼‰
hive (default)> select avg(sal) avg_sal from emp;

~~~



### Limitè¯­å¥ï¼ˆåˆ†é¡µï¼‰



~~~sql
/*============== åˆ†é¡µ ================ 
è¯­æ³• : SELECT * FROM table LIMIT [offset,] rows | rows OFFSET offset 
å¥½å¤„ : (ç”¨æˆ·ä½“éªŒ,ç½‘ç»œä¼ è¾“,æŸ¥è¯¢å‹åŠ›)


æ¨å¯¼:
		ç¬¬ä¸€é¡µ : limit 0,5
        ç¬¬äºŒé¡µ : limit 5,5 
        ç¬¬ä¸‰é¡µ : limit 10,5 
        ...... Â  
        ç¬¬Né¡µ : limit (pageNo-1)*pageSzie,pageSzie 
        [pageNo:é¡µç ,pageSize:å•é¡µé¢æ˜¾ç¤ºæ¡æ•°] Â 
        
*/


hive (default)> select * from emp limit 3,3;
~~~





### Whereè¯­å¥åŠå…¶æ¡ä»¶åˆ¤æ–­



ä½¿ç”¨WHEREå­å¥ï¼Œå°†ä¸æ»¡è¶³æ¡ä»¶çš„è¡Œè¿‡æ»¤æ‰

==WHEREå­å¥ç´§éšFROMå­å¥==

==æ³¨æ„ï¼šwhereå­å¥ä¸­ä¸èƒ½ä½¿ç”¨å­—æ®µåˆ«åã€‚==

~~~sql
hive (default)> select * from emp where sal >1000;
~~~





#### æ¯”è¾ƒè¿ç®—ç¬¦ï¼ˆBetween/In/ Is Nullï¼‰



| æ“ä½œç¬¦                   | æ”¯æŒçš„æ•°æ®ç±»å‹ | æè¿°                                                         |
| ------------------------ | -------------- | ------------------------------------------------------------ |
| A=B                      | åŸºæœ¬æ•°æ®ç±»å‹   | å¦‚æœAç­‰äºBåˆ™è¿”å›TRUEï¼Œåä¹‹è¿”å›FALSE                          |
| A<=>B                    | åŸºæœ¬æ•°æ®ç±»å‹   | ==ä¸”==     å¦‚æœAå’ŒBéƒ½ä¸ºNULLï¼Œåˆ™è¿”å›TRUEï¼Œå¦‚æœä¸€è¾¹ä¸ºNULLï¼Œè¿”å›False |
| A<>B, A!=B               | åŸºæœ¬æ•°æ®ç±»å‹   | ==æˆ–  é==     Aæˆ–è€…Bä¸ºNULLåˆ™è¿”å›NULLï¼›å¦‚æœAä¸ç­‰äºBï¼Œåˆ™è¿”å›TRUEï¼Œåä¹‹è¿”å›FALSE |
| A<B                      | åŸºæœ¬æ•°æ®ç±»å‹   | Aæˆ–è€…Bä¸ºNULLï¼Œåˆ™è¿”å›NULLï¼›å¦‚æœAå°äºBï¼Œåˆ™è¿”å›TRUEï¼Œåä¹‹è¿”å›FALSE |
| A<=B                     | åŸºæœ¬æ•°æ®ç±»å‹   | Aæˆ–è€…Bä¸ºNULLï¼Œåˆ™è¿”å›NULLï¼›å¦‚æœAå°äºç­‰äºBï¼Œåˆ™è¿”å›TRUEï¼Œåä¹‹è¿”å›FALSE |
| A>B                      | åŸºæœ¬æ•°æ®ç±»å‹   | Aæˆ–è€…Bä¸ºNULLï¼Œåˆ™è¿”å›NULLï¼›å¦‚æœAå¤§äºBï¼Œåˆ™è¿”å›TRUEï¼Œåä¹‹è¿”å›FALSE |
| A>=B                     | åŸºæœ¬æ•°æ®ç±»å‹   | Aæˆ–è€…Bä¸ºNULLï¼Œåˆ™è¿”å›NULLï¼›å¦‚æœAå¤§äºç­‰äºBï¼Œåˆ™è¿”å›TRUEï¼Œåä¹‹è¿”å›FALSE |
| A [NOT] BETWEEN B  AND C | åŸºæœ¬æ•°æ®ç±»å‹   | ==è¡¨ç¤ºAæ˜¯å¦åœ¨BCä¹‹é—´==     å¦‚æœAï¼ŒBæˆ–è€…Cä»»ä¸€ä¸ºNULLï¼Œåˆ™ç»“æœä¸ºNULLã€‚å¦‚æœAçš„å€¼å¤§äºç­‰äºBè€Œä¸”å°äºæˆ–ç­‰äºCï¼Œåˆ™ç»“æœä¸ºTRUEï¼Œåä¹‹ä¸ºFALSEã€‚å¦‚æœä½¿ç”¨NOTå…³é”®å­—åˆ™å¯è¾¾åˆ°ç›¸åçš„æ•ˆæœã€‚ |
| A IS NULL                | æ‰€æœ‰æ•°æ®ç±»å‹   | å¦‚æœAç­‰äºNULLï¼Œåˆ™è¿”å›TRUEï¼Œåä¹‹è¿”å›FALSE                     |
| A IS NOT NULL            | æ‰€æœ‰æ•°æ®ç±»å‹   | å¦‚æœAä¸ç­‰äºNULLï¼Œåˆ™è¿”å›TRUEï¼Œåä¹‹è¿”å›FALSE                   |
| ==IN(æ•°å€¼1, æ•°å€¼2)==     | æ‰€æœ‰æ•°æ®ç±»å‹   | ä½¿ç”¨ INè¿ç®—æ˜¾ç¤ºåˆ—è¡¨ä¸­çš„å€¼                                    |
| A [NOT] LIKE B           | STRING ç±»å‹    | Bæ˜¯ä¸€ä¸ªSQLä¸‹çš„ç®€å•æ­£åˆ™è¡¨è¾¾å¼ï¼Œä¹Ÿå«é€šé…ç¬¦æ¨¡å¼ï¼Œå¦‚æœAä¸å…¶åŒ¹é…çš„è¯ï¼Œåˆ™è¿”å›TRUEï¼›åä¹‹è¿”å›FALSEã€‚Bçš„è¡¨è¾¾å¼è¯´æ˜å¦‚ä¸‹ï¼šâ€˜x%â€™è¡¨ç¤ºAå¿…é¡»ä»¥å­—æ¯â€˜xâ€™å¼€å¤´ï¼Œâ€˜%xâ€™è¡¨ç¤ºAå¿…é¡»ä»¥å­—æ¯â€™xâ€™ç»“å°¾ï¼Œè€Œâ€˜%x%â€™è¡¨ç¤ºAåŒ…å«æœ‰å­—æ¯â€™xâ€™,å¯ä»¥ä½äºå¼€å¤´ï¼Œç»“å°¾æˆ–è€…å­—ç¬¦ä¸²ä¸­é—´ã€‚å¦‚æœä½¿ç”¨NOTå…³é”®å­—åˆ™å¯è¾¾åˆ°ç›¸åçš„æ•ˆæœã€‚ |
| A RLIKE B, A  REGEXP B   | STRING ç±»å‹    | Bæ˜¯åŸºäºjavaçš„æ­£åˆ™è¡¨è¾¾å¼ï¼Œå¦‚æœAä¸å…¶åŒ¹é…ï¼Œåˆ™è¿”å›TRUEï¼›åä¹‹è¿”å›FALSEã€‚åŒ¹é…ä½¿ç”¨çš„æ˜¯JDKä¸­çš„æ­£åˆ™è¡¨è¾¾å¼æ¥å£å®ç°çš„ï¼Œå› ä¸ºæ­£åˆ™ä¹Ÿä¾æ®å…¶ä¸­çš„è§„åˆ™ã€‚ä¾‹å¦‚ï¼Œæ­£åˆ™è¡¨è¾¾å¼å¿…é¡»å’Œæ•´ä¸ªå­—ç¬¦ä¸²Aç›¸åŒ¹é…ï¼Œè€Œä¸æ˜¯åªéœ€ä¸å…¶å­—ç¬¦ä¸²åŒ¹é…ã€‚ |





==è¿™äº›æ“ä½œç¬¦åŒæ ·å¯ä»¥ç”¨äºJOINâ€¦ONå’ŒHAVINGè¯­å¥ä¸­ã€‚==





####  Likeå’ŒRLike

------

ä½¿ç”¨LIKEè¿ç®—é€‰æ‹©ç±»ä¼¼çš„å€¼ï¼ˆæ¨¡ç³ŠæŸ¥è¯¢ï¼‰

é€‰æ‹©æ¡ä»¶å¯ä»¥åŒ…å«å­—ç¬¦æˆ–æ•°å­—:

+ % ä»£è¡¨é›¶ä¸ªæˆ–å¤šä¸ªå­—ç¬¦(ä»»æ„ä¸ªå­—ç¬¦)ã€‚

+  _ ä»£è¡¨ä¸€ä¸ªå­—ç¬¦ã€‚



RLIKEå­å¥æ˜¯Hiveä¸­è¿™ä¸ªåŠŸèƒ½çš„ä¸€ä¸ªæ‰©å±•ï¼Œå…¶å¯ä»¥é€šè¿‡**Javaçš„æ­£åˆ™è¡¨è¾¾å¼**è¿™ä¸ªæ›´å¼ºå¤§çš„è¯­è¨€æ¥æŒ‡å®šåŒ¹é…æ¡ä»¶ã€‚



~~~sql
ï¼ˆ1ï¼‰æŸ¥æ‰¾åå­—ä»¥Aå¼€å¤´çš„å‘˜å·¥ä¿¡æ¯
hive (default)> select * from emp where ename LIKE 'A%';


ï¼ˆ2ï¼‰æŸ¥æ‰¾åå­—ä¸­ç¬¬äºŒä¸ªå­—æ¯ä¸ºAçš„å‘˜å·¥ä¿¡æ¯
hive (default)> select * from emp where ename LIKE '_A%';


ï¼ˆ3ï¼‰æŸ¥æ‰¾åå­—ä¸­å¸¦æœ‰Açš„å‘˜å·¥ä¿¡æ¯
hive (default)> select * from emp where ename  RLIKE '[A]';
~~~



#### é€»è¾‘è¿ç®—ç¬¦ï¼ˆAnd/Or/Notï¼‰

| æ“ä½œç¬¦ | å«ä¹‰   |
| ------ | ------ |
| AND    | é€»è¾‘å¹¶ |
| OR     | é€»è¾‘æˆ– |
| NOT    | é€»è¾‘å¦ |



~~~sql
ï¼ˆ1ï¼‰æŸ¥è¯¢è–ªæ°´å¤§äº1000ï¼Œéƒ¨é—¨æ˜¯30
hive (default)> select * from emp where sal>1000 and deptno=30;

ï¼ˆ2ï¼‰æŸ¥è¯¢è–ªæ°´å¤§äº1000ï¼Œæˆ–è€…éƒ¨é—¨æ˜¯30
hive (default)> select * from emp where sal>1000 or deptno=30;

ï¼ˆ3ï¼‰æŸ¥è¯¢é™¤äº†20éƒ¨é—¨å’Œ30éƒ¨é—¨ä»¥å¤–çš„å‘˜å·¥ä¿¡æ¯
hive (default)> select * from emp where deptno not IN(30, 20);

~~~





## åˆ†ç»„





### Group Byè¯­å¥

GROUP BYè¯­å¥é€šå¸¸ä¼šå’Œèšåˆå‡½æ•°ä¸€èµ·ä½¿ç”¨ï¼ŒæŒ‰ç…§ä¸€ä¸ªæˆ–è€…å¤šä¸ªåˆ—é˜Ÿç»“æœè¿›è¡Œåˆ†ç»„ï¼Œç„¶åå¯¹æ¯ä¸ªç»„æ‰§è¡Œèšåˆæ“ä½œã€‚



**åˆ†ç»„ä¹‹åï¼Œselectåé¢åªèƒ½è·Ÿç»„æ ‡è¯†(åˆ†ç»„å­—æ®µ) å’Œ èšåˆå‡½æ•°(åˆ†ç»„å‡½æ•°)**



~~~sql
è®¡ç®—empè¡¨æ¯ä¸ªéƒ¨é—¨çš„å¹³å‡å·¥èµ„
select deptno ,avg(sal) avg_sal from emp group by deptno 

è®¡ç®—empæ¯ä¸ªéƒ¨é—¨ä¸­æ¯ä¸ªå²—ä½çš„æœ€é«˜è–ªæ°´
select deptno ,job ,max(sal) max_sal from emp group by deptno,job ; 
~~~







### Havingè¯­å¥



`having`ä¸`where`ä¸åŒç‚¹

- whereåé¢ä¸èƒ½å†™åˆ†ç»„å‡½æ•°ï¼Œè€Œhavingåé¢å¯ä»¥ä½¿ç”¨åˆ†ç»„å‡½æ•°ã€‚

- havingåªç”¨äºgroup byåˆ†ç»„ç»Ÿè®¡è¯­å¥ã€‚



~~~sql
select
  deptno , avg(sal) avg_sal 
from 
emp
where job != 'CLERK'  	ğŸ‘ˆ
group by deptno 		ğŸ‘ˆ		
having avg_sal >2000 ;  ğŸ‘ˆ
~~~





## Joinè¯­å¥



![sql-join](../image/sql-join.png)



### ğŸš©Joinæ ¸å¿ƒæ€æƒ³

  ğŸ‘‰å†…è¿æ¥   A inner join B  on ....
     			å†…è¿æ¥çš„ç»“æœé›†å–äº¤é›†

  ğŸ‘‰å¤–è¿æ¥    
    		ä¸»è¡¨(é©±åŠ¨è¡¨) å’Œ  ä»è¡¨(åŒ¹é…è¡¨)
    		å¤–è¿æ¥çš„ç»“æœé›†ä¸»è¡¨çš„æ‰€æœ‰æ•°æ® +  ä»è¡¨ä¸­ä¸ä¸»è¡¨åŒ¹é…çš„æ•°æ®. 

```sql
å·¦è¿æ¥  A left outer join B  on ....    A ä¸» B ä»
         B right outer Join A  on.... 

å³è¿æ¥  A right outer join B  on ....   A ä» B ä¸»
         B left outer join  A  on ....
```

  ğŸ‘‰è‡ªè¿æ¥ 

â€‹					 æ»¡å¤–è¿æ¥(full outer join ) 

 		

`Outerå¯åŠ å¯ä¸åŠ `



### å†…è¿æ¥

![image-20220122201513238](../image/image-20220122201513238.png)

~~~sql
		/*emp å’Œ deptå…±æœ‰çš„æ•°æ®(å†…è¿æ¥)*/
select
  e.ename, d.deptno 
from  
 emp e inner join dept  d
on e.deptno = d.deptno 
~~~



### å·¦å³è¿æ¥

#### å·¦è¿æ¥

![image-20220122201542794](../image/image-20220122201542794.png)

~~~sql
/*empæ‰€æœ‰çš„æ•°æ®  å’Œ  deptä¸­ä¸empåŒ¹é…çš„æ•°æ®*/

select
  e.ename, d.deptno
from
 emp e left outer join dept d 
on e.deptno = d.deptno  ; 
~~~





#### å³è¿æ¥

![image-20220122201606284](../image/image-20220122201606284.png)



~~~sql
 /*deptä¸­æ‰€æœ‰çš„æ•°æ® å’Œ  empä¸­ä¸deptåŒ¹é…çš„æ•°æ®*/

select
 e.ename, d.deptno
from
emp e  right outer join dept d 
on d.deptno = e.deptno ;
~~~







#### å·¦ç‹¬è¿æ¥

![image-20220122201650199](../image/image-20220122201650199.png)



~~~sql
/*empè¡¨ç‹¬æœ‰çš„æ•°æ®*/
select
  e.ename, d.deptno
from
 emp e left outer join dept d 
on e.deptno = d.deptno  
where 
   d.deptno is null  ğŸ‘ˆ
~~~



#### å³ç‹¬è¿æ¥



![image-20220122201726786](../image/image-20220122201726786.png)

~~~sql
/*deptè¡¨ç‹¬æœ‰çš„æ•°æ®*/

select
 e.ename, d.deptno
from
dept d left outer join emp e 
on d.deptno = e.deptno 
where
   e.deptno is null ;
~~~



### å…¨è¿æ¥



#### ä¸å»é‡

`union all : å°†ç»“æœé›†æ‹¼æ¥åˆ°ä¸€èµ·ï¼Œä¸å»é‡`

![image-20220122201837740](../image/image-20220122201837740.png)



~~~sql
ğŸŒ´NO.1
select
  e.ename, d.deptno
from		ğŸ‘‡
 emp e full outer join dept d 
on e.deptno = d.deptno ;

ğŸŒ´NO.2
select
  e.ename, d.deptno
from
 emp e left outer join dept d 
on e.deptno = d.deptno  
union all    ğŸ‘ˆ
select
 e.ename, d.deptno
from
dept d left outer join emp e 
on d.deptno = e.deptno ;
~~~







#### å»é‡

`union: å°†ç»“æœé›†æ‹¼æ¥åˆ°ä¸€èµ·ï¼Œå»é‡`



![image-20220122201849065](../image/image-20220122201849065.png)



~~~sql
select
  e.ename, d.deptno
from
 emp e left outer join dept d 
on e.deptno = d.deptno  
union   	ğŸ‘ˆ
select
 e.ename, d.deptno
from
dept d left outer join emp e 
on d.deptno = e.deptno ;

~~~







### ç¬›å¡å°”ç§¯



==ä¸€èˆ¬ä¸ç”¨==

**æ‰€æœ‰è¡¨ä¸­çš„æ‰€æœ‰è¡Œäº’ç›¸è¿æ¥**

ä¼šç”Ÿæˆåºå¤§çš„æ•°æ®è¡¨ï¼Œè°¨æ…ä½¿ç”¨



~~~sql
hive (default)> select empno, dname from emp, dept;
~~~





## æ’åº



### å…¨å±€æ’åºï¼ˆOrder Byï¼‰



==Order Byï¼šå…¨å±€æ’åºï¼Œåªæœ‰ä¸€ä¸ªReducer==

| ASCï¼ˆascendï¼‰:       | å‡åºï¼ˆé»˜è®¤ï¼‰ |
| -------------------- | ------------ |
| **DESCï¼ˆdescendï¼‰:** | **é™åº**     |



~~~sql
ï¼ˆ1ï¼‰æŸ¥è¯¢å‘˜å·¥ä¿¡æ¯æŒ‰å·¥èµ„å‡åºæ’åˆ—
hive (default)> select * from emp order by sal;

ï¼ˆ2ï¼‰æŸ¥è¯¢å‘˜å·¥ä¿¡æ¯æŒ‰å·¥èµ„é™åºæ’åˆ—
hive (default)> select * from emp order by sal desc;

~~~





### åˆ«åæ’åºå’Œå¤šä¸ªåˆ—æ’åº

~~~sql
/*æŒ‰ç…§å‘˜å·¥è–ªæ°´çš„2å€æ’åºï¼ˆåˆ«åï¼‰*/
hive (default)> select ename, sal*2 twosal from emp order by twosal;


/*æŒ‰ç…§éƒ¨é—¨å’Œå·¥èµ„å‡åºæ’åº   æŒ‰ç…§å…ˆåé¡ºåºï¼ˆå¤šä¸ªåˆ—ï¼‰*/
hive (default)> select ename, deptno, sal from emp order by deptno, sal ;

~~~





### æ¯ä¸ªReduceå†…éƒ¨æ’åºï¼ˆSort Byï¼‰



==ä¸ä¼šå•ç‹¬ç”¨ å…ˆæœ‰åˆ†åŒºåœ¨å†…éƒ¨æ’åº==



Sort Byï¼šå¯¹äºå¤§è§„æ¨¡çš„æ•°æ®é›†order byçš„æ•ˆç‡éå¸¸ä½ã€‚åœ¨å¾ˆå¤šæƒ…å†µä¸‹ï¼Œå¹¶ä¸éœ€è¦å…¨å±€æ’åºï¼Œæ­¤æ—¶å¯ä»¥ä½¿ç”¨**sort by**ã€‚

Sort byä¸º**æ¯ä¸ªreduceräº§ç”Ÿä¸€ä¸ªæ’åºæ–‡ä»¶**ã€‚æ¯ä¸ªReducerå†…éƒ¨è¿›è¡Œæ’åºï¼Œ**å¯¹å…¨å±€ç»“æœé›†æ¥è¯´ä¸æ˜¯æ’åºã€‚**



~~~sql
/*1ï¼‰è®¾ç½®reduceä¸ªæ•°*/
hive (default)> set mapreduce.job.reduces=3;

/*2ï¼‰æŸ¥çœ‹è®¾ç½®reduceä¸ªæ•°*/
hive (default)> set mapreduce.job.reduces;

/*3ï¼‰æ ¹æ®éƒ¨é—¨ç¼–å·é™åºæŸ¥çœ‹å‘˜å·¥ä¿¡æ¯*/
hive (default)> select * from emp sort by deptno desc;

/*4ï¼‰å°†æŸ¥è¯¢ç»“æœå¯¼å…¥åˆ°æ–‡ä»¶ä¸­ï¼ˆæŒ‰ç…§éƒ¨é—¨ç¼–å·é™åºæ’åºï¼‰*/
hive (default)> insert overwrite local directory '/opt/module/hive/datas/sortby-result'
 select * from emp sort by deptno desc;

~~~





### åˆ†åŒºï¼ˆDistribute Byï¼‰



Distribute Byï¼š åœ¨æœ‰äº›æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦æ§åˆ¶æŸä¸ªç‰¹å®šè¡Œåº”è¯¥åˆ°å“ªä¸ªreducerï¼Œé€šå¸¸æ˜¯ä¸ºäº†è¿›è¡Œåç»­çš„èšé›†æ“ä½œã€‚**distribute by** å­å¥å¯ä»¥åšè¿™ä»¶äº‹ã€‚**distribute byç±»ä¼¼MRä¸­partition**ï¼ˆè‡ªå®šä¹‰åˆ†åŒºï¼‰ï¼Œè¿›è¡Œåˆ†åŒºï¼Œ**ç»“åˆsort byä½¿ç”¨ã€‚** 

å¯¹äºdistribute byè¿›è¡Œæµ‹è¯•ï¼Œ==ä¸€å®šè¦åˆ†é…å¤šreduceè¿›è¡Œå¤„ç†ï¼Œ==å¦åˆ™æ— æ³•çœ‹åˆ°distribute byçš„æ•ˆæœã€‚



==æ³¨æ„ï¼š==

ğŸš©distribute byçš„åˆ†åŒºè§„åˆ™æ˜¯æ ¹æ®åˆ†åŒºå­—æ®µçš„hashç ä¸reduceçš„ä¸ªæ•°è¿›è¡Œå–æ¨¡è¿ç®—åï¼Œä½™æ•°ç›¸åŒçš„åˆ†åˆ°ä¸€ä¸ªåŒºã€‚

ğŸš©Hiveè¦æ±‚DISTRIBUTE BYè¯­å¥è¦å†™åœ¨SORT BYè¯­å¥ä¹‹å‰ã€‚



~~~sql
ï¼ˆ1ï¼‰å…ˆæŒ‰ç…§éƒ¨é—¨ç¼–å·åˆ†åŒºï¼Œå†æŒ‰ç…§å‘˜å·¥ç¼–å·é™åºæ’åºã€‚
hive (default)> set mapreduce.job.reduces=3;

hive (default)> insert overwrite local directory
'/opt/module/hive/datas/distribute-result' 
select * from emp
distribute by deptno 	ğŸ‘ˆ
sort by empno desc;		ğŸ‘ˆ

~~~







### Cluster By



**å½“distribute byå’Œsort byå­—æ®µç›¸åŒæ—¶ï¼Œå¯ä»¥ä½¿ç”¨cluster byæ–¹å¼ã€‚**

cluster byé™¤äº†å…·æœ‰distribute byçš„åŠŸèƒ½å¤–è¿˜å…¼å…·sort byçš„åŠŸèƒ½ã€‚**ä½†æ˜¯æ’åºåªèƒ½æ˜¯å‡åºæ’åºï¼Œ**ä¸èƒ½æŒ‡å®šæ’åºè§„åˆ™ä¸ºASCæˆ–è€…DESCã€‚



ï¼ˆ1ï¼‰ä»¥ä¸‹ä¸¤ç§å†™æ³•ç­‰ä»·

`hive (default)> select * from emp cluster by deptno;`

`hive (default)> select * from emp distribute by deptno sort by deptno;`

==æ³¨æ„==ï¼šæŒ‰ç…§éƒ¨é—¨ç¼–å·åˆ†åŒºï¼Œä¸ä¸€å®šå°±æ˜¯å›ºå®šæ­»çš„æ•°å€¼ï¼Œå¯ä»¥æ˜¯20å·å’Œ30å·éƒ¨é—¨åˆ†åˆ°ä¸€ä¸ªåˆ†åŒºé‡Œé¢å»ã€‚







# ä¸ƒã€åˆ†åŒºè¡¨å’Œåˆ†æ¡¶è¡¨



## åˆ†åŒºè¡¨



åˆ†åŒºè¡¨å®é™…ä¸Šå°±æ˜¯å¯¹åº”ä¸€ä¸ªHDFSæ–‡ä»¶ç³»ç»Ÿä¸Šçš„ç‹¬ç«‹çš„æ–‡ä»¶å¤¹ï¼Œè¯¥æ–‡ä»¶å¤¹ä¸‹æ˜¯è¯¥åˆ†åŒºæ‰€æœ‰çš„æ•°æ®æ–‡ä»¶ã€‚==Hiveä¸­çš„åˆ†åŒºå°±æ˜¯åˆ†ç›®å½•==ï¼ŒæŠŠä¸€ä¸ªå¤§çš„æ•°æ®é›†æ ¹æ®ä¸šåŠ¡éœ€è¦åˆ†å‰²æˆå°çš„æ•°æ®é›†ã€‚åœ¨æŸ¥è¯¢æ—¶é€šè¿‡WHEREå­å¥ä¸­çš„è¡¨è¾¾å¼é€‰æ‹©æŸ¥è¯¢æ‰€éœ€è¦çš„æŒ‡å®šçš„åˆ†åŒºï¼Œè¿™æ ·çš„æŸ¥è¯¢æ•ˆç‡ä¼šæé«˜å¾ˆå¤šã€‚



**åˆ›å»ºåˆ†åŒºè¡¨è¯­æ³•**

~~~sql
create table dept_partition (
  deptno int ,
  dname string,
  loc string 
) 
partitioned by (day string ) 			ğŸ‘ˆ
row format delimited fields terminated by '\t' ;
~~~



**å¾€åˆ†åŒºè¡¨åŠ è½½æ•°æ®**

~~~sql
load data local inpath '/opt/module/hive-3.1.2/datas/dept_20200401.log'  
                   into table dept_partition partition(day='20200401') ; 

load data local inpath '/opt/module/hive-3.1.2/datas/dept_20200402.log'  
                   into table dept_partition partition(day='20200402') ;

load data local inpath '/opt/module/hive-3.1.2/datas/dept_20200403.log'  
                   into table dept_partition partition(day='20200403') ;
~~~



==æ³¨æ„ï¼šåˆ†åŒºè¡¨åŠ è½½æ•°æ®æ—¶ï¼Œå¿…é¡»æŒ‡å®šåˆ†åŒº==





### åˆ†åŒºè¡¨åŸºæœ¬æ“ä½œ



~~~sql
/*æŸ¥åˆ†åŒº:*/
show partitions dept_partition ;     

/*åŠ åˆ†åŒº: */
alter table dept_partition add partition(day='20200404');	/*å•ä¸ª*/
alter table dept_partition add partition(day='20200405') partition(day='20200406') ;		/*å¤šä¸ª*/

/*åˆ åˆ†åŒº:*/
alter table dept_partition drop partition(day='20200404');	/*å•ä¸ª*/	
alter table dept_partition drop partition(day='20200405') , partition(day='20200406');	/*å¤šä¸ªåˆ†åŒºåˆ«å¿˜åŠ é€—å·*/ 			 										  ğŸ‘†
~~~





### äºŒçº§åˆ†åŒº



**ğŸ“¢=HDFSä¸ŠäºŒçº§ç›®å½•**



#### åˆ›å»ºä¸åŠ è½½æ•°æ®



~~~sql
/*åˆ›å»º*/
create table dept_partition2(
  deptno int, 
  dname string, 
  loc string 
)
partitioned by (day string ,hour string )
row format delimited fields terminated by '\t' ;

/*åŠ è½½æ•°æ®*/
load data local inpath '/opt/module/hive-3.1.2/datas/dept_20200401.log'  
                   into table dept_partition2 partition(day='20200401',hour='12') ; 

load data local inpath '/opt/module/hive-3.1.2/datas/dept_20200402.log'  
                   into table dept_partition2 partition(day='20200401',hour='13') ;

load data local inpath '/opt/module/hive-3.1.2/datas/dept_20200403.log'  
                   into table dept_partition2 partition(day='20200401',hour='14') ;

load data local inpath '/opt/module/hive-3.1.2/datas/dept_20200401.log'  
                   into table dept_partition2 partition(day='20200402',hour='07') ; 

load data local inpath '/opt/module/hive-3.1.2/datas/dept_20200402.log'  
                   into table dept_partition2 partition(day='20200402',hour='08') ;

load data local inpath '/opt/module/hive-3.1.2/datas/dept_20200403.log'  
                   into table dept_partition2 partition(day='20200402',hour='09') ;                   

~~~



#### æ•°æ®å’Œåˆ†åŒºè¡¨å…³è”çš„æ–¹å¼



~~~sql
 /*ğŸ”§æ‰‹åŠ¨åˆ›å»ºåˆ†åŒºç›®å½•ï¼Œæ‰‹åŠ¨ä¸Šä¼ æ•°æ® , è¿›è¡Œåˆ†åŒºä¿®å¤æ“ä½œ*/
hadoop fs -mkdir /user/hive/warehouse/mydb.db/dept_partition/day=20200404 
hadoop fs -put /opt/module/hive-3.1.2/datas/dept_20200403.log  /user/hive/warehouse/mydb.db/dept_partition/day=20200404
msck repair table dept_partition;

  /*ğŸ”§æ‰‹åŠ¨åˆ›å»ºåˆ†åŒºç›®å½•ï¼Œæ‰‹åŠ¨ä¸Šä¼ æ•°æ® , æ‰‹åŠ¨æ·»åŠ åˆ†åŒº*/
hadoop fs -mkdir /user/hive/warehouse/mydb.db/dept_partition/day=20200405 
hadoop fs -put /opt/module/hive-3.1.2/datas/dept_20200403.log  /user/hive/warehouse/mydb.db/dept_partition/day=20200405
alter table dept_partition add partition(day='20200405');

 /*ğŸ”§æ‰‹åŠ¨åˆ›å»ºåˆ†åŒºç›®å½•,loadæ•°æ®åˆ°åˆ†åŒºä¸­*/
hadoop fs -mkdir /user/hive/warehouse/mydb.db/dept_partition/day=20200406
load data local inpath '/opt/module/hive-3.1.2/datas/dept_20200403.log' into table dept_partition partition(day='20200406')

~~~





### åŠ¨æ€åˆ†åŒº



`åˆ†åŒºåŠæ™ºèƒ½åŒ–`

å…³ç³»å‹æ•°æ®åº“ä¸­ï¼Œå¯¹åˆ†åŒºè¡¨Insertæ•°æ®æ—¶å€™ï¼Œæ•°æ®åº“è‡ªåŠ¨ä¼šæ ¹æ®åˆ†åŒºå­—æ®µçš„å€¼ï¼Œå°†æ•°æ®æ’å…¥åˆ°ç›¸åº”çš„åˆ†åŒºä¸­ï¼ŒHiveä¸­ä¹Ÿæä¾›äº†ç±»ä¼¼çš„æœºåˆ¶ï¼Œå³åŠ¨æ€åˆ†åŒº(Dynamic Partition)ï¼Œåªä¸è¿‡ï¼Œä½¿ç”¨Hiveçš„åŠ¨æ€åˆ†åŒºï¼Œéœ€è¦è¿›è¡Œç›¸åº”çš„é…ç½®ã€‚



**å¼€å¯åŠ¨æ€åˆ†åŒºå‚æ•°è®¾ç½®**



~~~sql
	ğŸŒ¼å¼€å¯åŠ¨æ€åˆ†åŒºåŠŸèƒ½ï¼ˆé»˜è®¤trueï¼Œå¼€å¯ï¼‰
hive.exec.dynamic.partition=true

	ğŸŒ¼è®¾ç½®ä¸ºéä¸¥æ ¼æ¨¡å¼ï¼ˆåŠ¨æ€åˆ†åŒºçš„æ¨¡å¼ï¼Œé»˜è®¤strictï¼Œè¡¨ç¤ºå¿…é¡»æŒ‡å®šè‡³å°‘ä¸€ä¸ªåˆ†åŒºä¸ºé™æ€åˆ†åŒºï¼Œnonstrictæ¨¡å¼è¡¨ç¤ºå…è®¸æ‰€æœ‰çš„åˆ†åŒºå­—æ®µéƒ½å¯ä»¥ä½¿ç”¨åŠ¨æ€åˆ†åŒºã€‚ï¼‰
hive.exec.dynamic.partition.mode=nonstrict

	ğŸŒ¼åœ¨æ‰€æœ‰æ‰§è¡ŒMRçš„èŠ‚ç‚¹ä¸Šï¼Œæœ€å¤§ä¸€å…±å¯ä»¥åˆ›å»ºå¤šå°‘ä¸ªåŠ¨æ€åˆ†åŒºã€‚é»˜è®¤1000
hive.exec.max.dynamic.partitions=1000

	ğŸŒ¼åœ¨æ¯ä¸ªæ‰§è¡ŒMRçš„èŠ‚ç‚¹ä¸Šï¼Œæœ€å¤§å¯ä»¥åˆ›å»ºå¤šå°‘ä¸ªåŠ¨æ€åˆ†åŒºã€‚è¯¥å‚æ•°éœ€è¦æ ¹æ®å®é™…çš„æ•°æ®æ¥è®¾å®šã€‚æ¯”å¦‚ï¼šæºæ•°æ®ä¸­åŒ…å«äº†ä¸€å¹´çš„æ•°æ®ï¼Œå³dayå­—æ®µæœ‰365ä¸ªå€¼ï¼Œé‚£ä¹ˆè¯¥å‚æ•°å°±éœ€è¦è®¾ç½®æˆå¤§äº365ï¼Œå¦‚æœä½¿ç”¨é»˜è®¤å€¼100ï¼Œåˆ™ä¼šæŠ¥é”™ã€‚
hive.exec.max.dynamic.partitions.pernode=100

	ğŸŒ¼æ•´ä¸ªMR Jobä¸­ï¼Œæœ€å¤§å¯ä»¥åˆ›å»ºå¤šå°‘ä¸ªHDFSæ–‡ä»¶ã€‚é»˜è®¤100000
hive.exec.max.created.files=100000

	ğŸŒ¼å½“æœ‰ç©ºåˆ†åŒºç”Ÿæˆæ—¶ï¼Œæ˜¯å¦æŠ›å‡ºå¼‚å¸¸ã€‚ä¸€èˆ¬ä¸éœ€è¦è®¾ç½®ã€‚é»˜è®¤false
hive.error.on.empty.partition=false

~~~







## åˆ†æ¡¶è¡¨



==åˆ‡åˆ†æ–‡ä»¶==

åˆ†åŒºæä¾›ä¸€ä¸ªéš”ç¦»æ•°æ®å’Œä¼˜åŒ–æŸ¥è¯¢çš„ä¾¿åˆ©æ–¹å¼ã€‚ä¸è¿‡ï¼Œ**å¹¶éæ‰€æœ‰çš„æ•°æ®é›†éƒ½å¯å½¢æˆåˆç†çš„åˆ†åŒºã€‚**å¯¹äºä¸€å¼ è¡¨æˆ–è€…åˆ†åŒºï¼ŒHive å¯ä»¥è¿›ä¸€æ­¥ç»„ç»‡æˆæ¡¶ï¼Œä¹Ÿå°±æ˜¯æ›´ä¸ºç»†ç²’åº¦çš„æ•°æ®èŒƒå›´åˆ’åˆ†ã€‚

åˆ†æ¡¶æ˜¯å°†æ•°æ®é›†åˆ†è§£æˆæ›´å®¹æ˜“ç®¡ç†çš„è‹¥å¹²éƒ¨åˆ†çš„å¦ä¸€ä¸ªæŠ€æœ¯ã€‚

**åˆ†åŒºé’ˆå¯¹çš„æ˜¯æ•°æ®çš„å­˜å‚¨è·¯å¾„ï¼›åˆ†æ¡¶é’ˆå¯¹çš„æ˜¯æ•°æ®æ–‡ä»¶ã€‚**





**ğŸŒ´åˆ†æ¡¶è§„åˆ™ï¼š**

æ ¹æ®ç»“æœå¯çŸ¥ï¼šHiveçš„åˆ†æ¡¶é‡‡ç”¨å¯¹åˆ†æ¡¶å­—æ®µçš„å€¼è¿›è¡Œå“ˆå¸Œï¼Œç„¶åé™¤ä»¥æ¡¶çš„ä¸ªæ•°æ±‚ä½™çš„æ–¹ å¼å†³å®šè¯¥æ¡è®°å½•å­˜æ”¾åœ¨å“ªä¸ªæ¡¶å½“ä¸­

**ğŸŒ´åˆ†æ¡¶è¡¨æ“ä½œéœ€è¦æ³¨æ„çš„äº‹é¡¹**

+ reduceçš„ä¸ªæ•°è®¾ç½®ä¸º-1,è®©Jobè‡ªè¡Œå†³å®šéœ€è¦ç”¨å¤šå°‘ä¸ªreduceæˆ–è€…å°†reduceçš„ä¸ªæ•°è®¾ç½®ä¸ºå¤§äºç­‰äºåˆ†æ¡¶è¡¨çš„æ¡¶æ•°

+ ä»hdfsä¸­loadæ•°æ®åˆ°åˆ†æ¡¶è¡¨ä¸­ï¼Œé¿å…æœ¬åœ°æ–‡ä»¶æ‰¾ä¸åˆ°é—®é¢˜

+ ==ä¸è¦ä½¿ç”¨æœ¬åœ°æ¨¡å¼==



 

~~~sql
   1ï¼‰ åˆ›å»ºåˆ†æ¡¶è¡¨
create table student_bucket(
  id int, 
  name string 
)
clustered by (id)
into 4 buckets 
row format delimited fields terminated by '\t' ;   

    2ï¼‰locdå¯¼å…¥æ•°æ®
load  data  inpath '/student.txt' into table student_bucket ;
		
	  insertå¯¼å…¥æ•°æ®
hive(default)>insert into table stu_buck select * from student_insert ;	  
~~~



```sql
ğŸŒ´æŸ¥è¯¢æ•°æ®
>select * from stu_buck;
			
1016    ss16
1012    ss12
1008    ss8
1004    ss4

1009    ss9
1005    ss5
1001    ss1
1013    ss13

1010    ss10
1002    ss2
1006    ss6
1014    ss14

1003    ss3
1011    ss11
1007    ss7
1015    ss15
```



==å¯ä»¥çœ‹å‡ºåˆ†æ¡¶æ“ä½œæ˜¯å¯¹æ•°æ®èŒƒå›´çš„å‡åˆ†==





## æŠ½æ ·æŸ¥è¯¢



å¯¹äºéå¸¸å¤§çš„æ•°æ®é›†ï¼Œæœ‰æ—¶ç”¨æˆ·éœ€è¦ä½¿ç”¨çš„æ˜¯ä¸€ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„æŸ¥è¯¢ç»“æœè€Œä¸æ˜¯å…¨éƒ¨ç»“æœã€‚Hiveå¯ä»¥é€šè¿‡å¯¹è¡¨è¿›è¡ŒæŠ½æ ·æ¥æ»¡è¶³è¿™ä¸ªéœ€æ±‚ã€‚

==ç›¸å½“äºæŠ½å–æ¯æ¡¶é‡Œçš„éƒ¨åˆ†æ•°æ®==

â€‹												ğŸ‘‡æŸ¥çœ‹çš„æ¡¶æ•°

**è¯­æ³•:** `TABLESAMPLE(BUCKET x OUT OF y)` 

â€‹																  ğŸ‘†æ€»æ¡¶æ•°

==æ³¨æ„ï¼šxçš„å€¼å¿…é¡»å°äºç­‰äºyçš„å€¼==



~~~sql
hive (default)> select * from stu_bucket tablesample(bucket 1 out of 4 on id);
~~~







# å…«ã€å‡½æ•°





## å†…ç½®å‡½æ•°



**æŸ¥çœ‹ç³»ç»Ÿè‡ªå¸¦çš„å‡½æ•°**  `show functions;`

**æ˜¾ç¤ºè‡ªå¸¦çš„å‡½æ•°çš„ç”¨æ³•**   `desc function upper;`

**è¯¦ç»†æ˜¾ç¤ºè‡ªå¸¦çš„å‡½æ•°çš„ç”¨æ³•**   `desc function extended upper;`





### ç©ºå­—æ®µèµ‹å€¼



**ğŸ¤”æ•°æ®å€¼ä¸º0å’Œä¸ºç©ºä»£è¡¨çš„æ„ä¹‰**



**è¯­æ³•ï¼š**`NVL( valueï¼Œdefault_value)`

å®ƒçš„åŠŸèƒ½æ˜¯å¦‚æœvalueä¸ºNULLï¼Œåˆ™NVLå‡½æ•°è¿”å›default_valueçš„å€¼ï¼Œå¦åˆ™è¿”å›valueçš„å€¼ï¼Œå¦‚æœä¸¤ä¸ªå‚æ•°éƒ½ä¸ºNULL ï¼Œåˆ™è¿”å›NULLã€‚



~~~sql
æŸ¥è¯¢ï¼šå¦‚æœå‘˜å·¥çš„commä¸ºNULLï¼Œåˆ™ç”¨-1ä»£æ›¿
hive (default)> select comm,nvl(comm, -1) from emp;

æŸ¥è¯¢ï¼šå¦‚æœå‘˜å·¥çš„commä¸ºNULLï¼Œåˆ™ç”¨é¢†å¯¼idä»£æ›¿
hive (default)> select comm, nvl(comm,mgr) from emp;
~~~







###  CASE WHEN THEN ELSE END

â€‹		==[  ]	é‡Œçš„å†…å®¹å¯æœ‰å¯æ— ==																								

**è¯­æ³•ï¼š**`case   a  when  b  then  c  [when d then e ]    [else   f]    end`

â€‹																				ğŸ‘†å¯æœ‰Nå¤šä¸ª



> æ¡ˆä¾‹
>

**æ•°æ®**

| name | dept_id | sex  |
| ---- | ------- | ---- |
| æ‚Ÿç©º | A       | ç”·   |
| å¤§æµ· | A       | ç”·   |
| å®‹å®‹ | B       | ç”·   |
| å‡¤å§ | A       | å¥³   |
| å©·å§ | B       | å¥³   |
| å©·å©· | B       | å¥³   |

**éœ€æ±‚**
dept_Id   ç”·      å¥³
 A        	 2       1
 B        	 1       2



**SQL**



å®ç°ä¸€

~~~sql
select 
  dept_Id, 
  sum(case sex when 'ç”·' then 1 else 0 end )   man, 
  sum(case sex when 'å¥³' then 1 else 0 end )  female
from
  emp_sex 
group by dept_Id ;
~~~



å®ç°äºŒ

**ğŸ“¢if   æ–¹æ³•åªé€‚ç”¨äºä¸¤ç§å€¼åˆ¤æ–­**

~~~sql

select 
  dept_Id, 
  sum(if(sex='ç”·',1,0))   man,    ğŸ‘ˆ
  sum(if(sex='å¥³',1,0))  female	 ğŸ‘ˆ
from
  emp_sex 
group by dept_Id    

~~~



### è¡Œè½¬åˆ—



**ç›¸å…³å‡½æ•°:** 

`concat()`: å­—ç¬¦ä¸²æ‹¼æ¥
`concat_ws()` :å­—ç¬¦ä¸²æ‹¼æ¥ , CONCAT_WS must be "string or array<string>"
`collect_set()`: å»é‡æ±‡æ€»äº§ç”Ÿarrayç±»å‹å­—æ®µã€‚
`collect_list()`: æ±‡æ€»





> æ¡ˆä¾‹

ğŸ“Š

| name   | constellation | blood_type |
| ------ | ------------- | ---------- |
| å­™æ‚Ÿç©º | ç™½ç¾Šåº§        | A          |
| å¤§æµ·   | å°„æ‰‹åº§        | A          |
| å®‹å®‹   | ç™½ç¾Šåº§        | B          |
| çŒªå…«æˆ’ | ç™½ç¾Šåº§        | A          |
| å‡¤å§   | å°„æ‰‹åº§        | A          |
| è‹è€å¸ˆ | ç™½ç¾Šåº§        | B          |
| å¤§æµ·   | å°„æ‰‹åº§        | A          |



éœ€æ±‚
å°„æ‰‹åº§,A            å¤§æµ·|å‡¤å§
ç™½ç¾Šåº§,A            å­™æ‚Ÿç©º|çŒªå…«æˆ’
ç™½ç¾Šåº§,B            å®‹å®‹|è‹è€å¸ˆ



**SQL**

~~~sql
åˆ›å»ºhiveè¡¨å¹¶å¯¼å…¥æ•°æ®
create table hjc(
name string, 
constellation string, 
blood_type string) 
row format delimited fields terminated by "\t";

load data local inpath "/tmp/hjc.txt" into table hjc;


å°†  constellation  å’Œ blood_type æ‹¼æ¥åˆ°ä¸€èµ·

select
  name, 
  concat_ws(',',constellation,blood_type) c_b
from 
  person_info  =>t1


æŒ‰ç…§ c_båˆ†ç»„ï¼Œåœ¨ç»„å†…å°†nameè¿›è¡Œå»é‡æ±‡æ€»
select							
  t1.c_b,  concat_ws('|',collect_set(t1.name)) names
from 
 (select
  name, 
  concat_ws(',',constellation,blood_type) c_b
from 
  hjc)t1
group by t1.c_b 
+---------+----------+
| t1.c_b  |  names   |
+---------+----------+
| å°„æ‰‹åº§,A   | å¤§æµ·|å‡¤å§    |
| ç™½ç¾Šåº§,A   | çŒªå…«æˆ’|å­™æ‚Ÿç©º  |
| ç™½ç¾Šåº§,B   | è‹è€å¸ˆ|å®‹å®‹   |
+---------+----------+



#æŒ‰ç…§ c_båˆ†ç»„ï¼Œåœ¨ç»„å†…å°†nameè¿›è¡Œä¸å»é‡æ±‡æ€»
select							
  t1.c_b,  concat_ws('|',collect_list(t1.name)) names
from 
 (select
  name, 
  concat_ws(',',constellation,blood_type) c_b
from 
  hjc)t1
group by t1.c_b 

+---------+-----------+
| t1.c_b  |   names   |
+---------+-----------+ 
| å°„æ‰‹åº§,A   | å¤§æµ·|å‡¤å§|å¤§æµ·  | ğŸ‘ˆ
| ç™½ç¾Šåº§,A   | å­™æ‚Ÿç©º|çŒªå…«æˆ’   |
| ç™½ç¾Šåº§,B   | å®‹å®‹|è‹è€å¸ˆ    |
+---------+-----------+

~~~





### åˆ—è½¬è¡Œ

------



- **ç›¸å…³å‡½æ•°**
        `explode()`: å°†æ•°ç»„æˆ–è€…mapæ‹†åˆ†æˆå¤šè¡Œ
        `LATERAL VIEW` : ä¾§å†™è¡¨(è™šæ‹Ÿè¡¨)

- **ç”¨æ³•ï¼š**`LATERAL VIEW udtf(expression) tableAlias AS columnAlias`

- **è§£é‡Š**ï¼šç”¨äºå’Œsplit, explodeç­‰UDTFä¸€èµ·ä½¿ç”¨ï¼Œå®ƒèƒ½å¤Ÿå°†ä¸€åˆ—æ•°æ®æ‹†æˆå¤šè¡Œæ•°æ®ï¼Œ**åœ¨æ­¤åŸºç¡€ä¸Šå¯ä»¥å¯¹æ‹†åˆ†åçš„æ•°æ®è¿›è¡Œèšåˆã€‚**




> æ¡ˆä¾‹



| movie         | category                 |
| ------------- | ------------------------ |
| ã€Šç–‘çŠ¯è¿½è¸ªã€‹  | æ‚¬ç–‘,åŠ¨ä½œ,ç§‘å¹»,å‰§æƒ…      |
| ã€ŠLie to meã€‹ | æ‚¬ç–‘,è­¦åŒª,åŠ¨ä½œ,å¿ƒç†,å‰§æƒ… |
| ã€Šæˆ˜ç‹¼2ã€‹     | æˆ˜äº‰,åŠ¨ä½œ,ç¾éš¾           |



Need

ã€Šç–‘çŠ¯è¿½è¸ªã€‹      æ‚¬ç–‘
ã€Šç–‘çŠ¯è¿½è¸ªã€‹      åŠ¨ä½œ
ã€Šç–‘çŠ¯è¿½è¸ªã€‹      ç§‘å¹»
ã€Šç–‘çŠ¯è¿½è¸ªã€‹      å‰§æƒ…
ã€ŠLie to meã€‹   æ‚¬ç–‘
ã€ŠLie to meã€‹   è­¦åŒª
ã€ŠLie to meã€‹   åŠ¨ä½œ
ã€ŠLie to meã€‹   å¿ƒç†
ã€ŠLie to meã€‹   å‰§æƒ…
ã€Šæˆ˜ç‹¼2ã€‹        æˆ˜äº‰
ã€Šæˆ˜ç‹¼2ã€‹        åŠ¨ä½œ
ã€Šæˆ˜ç‹¼2ã€‹        ç¾éš¾



**SQL**

~~~sql
select
   movie , 
   category_name
from 
   movie_info 
LATERAL view  explode(split(category,','))  movie_info_tmp  as category_name ;

~~~





### ğŸš©çª—å£å‡½æ•°ï¼ˆå¼€çª—å‡½æ•°ï¼‰



| ç›¸å…³å‡½æ•°è¯´æ˜             |                                                              |
| ------------------------ | ------------------------------------------------------------ |
| OVER()                   | æŒ‡å®šåˆ†æå‡½æ•°å·¥ä½œçš„æ•°æ®çª—å£å¤§å°ï¼Œè¿™ä¸ªæ•°æ®çª—å£å¤§å°å¯èƒ½ä¼šéšç€è¡Œçš„æ”¹å˜è€Œå˜åŒ–ã€‚ |
| CURRENT ROW              | å½“å‰è¡Œ                                                       |
| n PRECEDING              | å¾€å‰nè¡Œæ•°æ®                                                  |
| n FOLLOWING              | å¾€ånè¡Œæ•°æ®                                                  |
| UNBOUNDED                | èµ·ç‚¹                                                         |
| UNBOUNDED PRECEDING      | è¡¨ç¤ºä»å‰é¢çš„èµ·ç‚¹ï¼Œ                                           |
| UNBOUNDED FOLLOWING      | è¡¨ç¤ºåˆ°åé¢çš„ç»ˆç‚¹                                             |
| LAG(col,n,default_val)   | å¾€å‰ç¬¬nè¡Œæ•°æ®                                                |
| LEAD(col,n, default_val) | å¾€åç¬¬nè¡Œæ•°æ®                                                |
| NTILE(n)                 | æŠŠæœ‰åºçª—å£çš„è¡Œåˆ†å‘åˆ°æŒ‡å®šæ•°æ®çš„ç»„ä¸­ï¼Œå„ä¸ªç»„æœ‰ç¼–å·ï¼Œç¼–å·ä»1å¼€å§‹ï¼Œå¯¹äºæ¯ä¸€è¡Œï¼ŒNTILEè¿”å›æ­¤è¡Œæ‰€å±çš„ç»„çš„ç¼–å·ã€‚==æ³¨æ„ï¼šnå¿…é¡»ä¸ºintç±»å‹ã€‚== |



> æ¡ˆä¾‹



~~~sql
éœ€æ±‚äºŒ: æŸ¥è¯¢é¡¾å®¢çš„è´­ä¹°æ˜ç»†åŠæ‰€æœ‰é¡¾å®¢çš„æœˆè´­ä¹°æ€»é¢
 
select
  name, 
  orderdate, 
  cost , 
  sum(cost) over(partition by substring(orderdate,0,7))  month_cost
from
  business 
+-------+-------------+-------+-------------+
| name  |  orderdate  | cost  | month_cost  |
+-------+-------------+-------+-------------+
| jack  | 2017-01-01  | 10    | 205         |
| jack  | 2017-01-08  | 55    | 205         |
| tony  | 2017-01-07  | 50    | 205         |
| jack  | 2017-01-05  | 46    | 205         |
| tony  | 2017-01-04  | 29    | 205         |
| tony  | 2017-01-02  | 15    | 205         |
| jack  | 2017-02-03  | 23    | 23          |
| mart  | 2017-04-13  | 94    | 341         |
| jack  | 2017-04-06  | 42    | 341         |
| mart  | 2017-04-11  | 75    | 341         |
| mart  | 2017-04-09  | 68    | 341         |
| mart  | 2017-04-08  | 62    | 341         |
| neil  | 2017-05-10  | 12    | 12          |
| neil  | 2017-06-12  | 80    | 80          |
+-------+-------------+-------+-------------+


éœ€æ±‚äºŒ: æŸ¥è¯¢é¡¾å®¢çš„è´­ä¹°æ˜ç»†åŠæ¯ä¸ªé¡¾å®¢çš„æœˆè´­ä¹°æ€»é¢

select
  name, 
  orderdate,
  cost, 
  sum(cost) over(partition by name, substring(orderdate,0,7)) name_month_cost
from 
  business
+-------+-------------+-------+------------------+
| name  |  orderdate  | cost  | name_month_cost  |
+-------+-------------+-------+------------------+
| jack  | 2017-01-05  | 46    | 111              |
| jack  | 2017-01-08  | 55    | 111              |
| jack  | 2017-01-01  | 10    | 111              |
| jack  | 2017-02-03  | 23    | 23               |
| jack  | 2017-04-06  | 42    | 42               |
| mart  | 2017-04-13  | 94    | 299              |
| mart  | 2017-04-11  | 75    | 299              |
| mart  | 2017-04-09  | 68    | 299              |
| mart  | 2017-04-08  | 62    | 299              |
| neil  | 2017-05-10  | 12    | 12               |
| neil  | 2017-06-12  | 80    | 80               |
| tony  | 2017-01-04  | 29    | 94               |
| tony  | 2017-01-02  | 15    | 94               |
| tony  | 2017-01-07  | 50    | 94               |
+-------+-------------+-------+------------------+



éœ€æ±‚ä¸‰: å°†æ¯ä¸ªé¡¾å®¢çš„costæŒ‰ç…§æ—¥æœŸè¿›è¡Œç´¯åŠ 

select 
  name, 
  orderdate,
  cost,													ğŸ‘‡
  sum(cost) over(partition by name order by orderdate rows between UNBOUNDED PRECEDING and CURRENT ROW) lj
from 
  business 

select 
  name, 
  orderdate,
  cost,
  sum(cost) over(partition by name order by orderdate ) lj
from 
  business   
+-------+-------------+-------+------+
| name  |  orderdate  | cost  |  lj  |
+-------+-------------+-------+------+
| jack  | 2017-01-01  | 10    | 10   |
| jack  | 2017-01-05  | 46    | 56   |
| jack  | 2017-01-08  | 55    | 111  |
| jack  | 2017-02-03  | 23    | 134  |
| jack  | 2017-04-06  | 42    | 176  |
| mart  | 2017-04-08  | 62    | 62   |
| mart  | 2017-04-09  | 68    | 130  |
| mart  | 2017-04-11  | 75    | 205  |
| mart  | 2017-04-13  | 94    | 299  |
| neil  | 2017-05-10  | 12    | 12   |
| neil  | 2017-06-12  | 80    | 92   |
| tony  | 2017-01-02  | 15    | 15   |
| tony  | 2017-01-04  | 29    | 44   |
| tony  | 2017-01-07  | 50    | 94   |
+-------+-------------+-------+------+


éœ€æ±‚ä¸‰: å°†æ‰€æœ‰é¡¾å®¢çš„costæŒ‰ç…§æ—¥æœŸè¿›è¡Œç´¯åŠ 

select 
  name, 
  orderdate,
  cost,
  sum(cost) over(order by orderdate ) lj
from 
  business 
+-------+-------------+-------+------+
| name  |  orderdate  | cost  |  lj  |
+-------+-------------+-------+------+
| jack  | 2017-01-01  | 10    | 10   |
| tony  | 2017-01-02  | 15    | 25   |
| tony  | 2017-01-04  | 29    | 54   |
| jack  | 2017-01-05  | 46    | 100  |
| tony  | 2017-01-07  | 50    | 150  |
| jack  | 2017-01-08  | 55    | 205  |
| jack  | 2017-02-03  | 23    | 228  |
| jack  | 2017-04-06  | 42    | 270  |
| mart  | 2017-04-08  | 62    | 332  |
| mart  | 2017-04-09  | 68    | 400  |
| mart  | 2017-04-11  | 75    | 475  |
| mart  | 2017-04-13  | 94    | 569  |
| neil  | 2017-05-10  | 12    | 581  |
| neil  | 2017-06-12  | 80    | 661  |
+-------+-------------+-------+------+

éœ€æ±‚ä¸‰: æ±‚æ‰€æœ‰é¡¾å®¢çš„è´­ä¹°æ˜ç»†åŠæŒ‰ç…§æ—¥æœŸè¿›è¡Œæ’åºå
       æ±‚ æ‰€æœ‰é¡¾å®¢çš„cost  ç¬¬ä¸€è¡Œ åˆ° å½“å‰è¡Œ ç´¯åŠ 
          æ‰€æœ‰é¡¾å®¢çš„cost ä¸Šä¸€è¡Œ åˆ° å½“å‰è¡Œ çš„ç´¯åŠ å’Œ 
          æ‰€æœ‰é¡¾å®¢çš„cost ä¸Šä¸€è¡Œ åˆ° ä¸‹ä¸€è¡Œ çš„ç´¯åŠ å’Œ
          æ‰€æœ‰é¡¾å®¢çš„cost å½“å‰è¡Œ åˆ° ä¸‹ä¸€è¡Œ çš„ç´¯åŠ å’Œ
          æ‰€æœ‰é¡¾å®¢çš„cost å½“å‰è¡Œ åˆ° æœ€åä¸€è¡Œçš„ç´¯åŠ å’Œ


select
  name,
  orderdate,
  cost,
  sum(cost) over(order by orderdate rows between UNBOUNDED PRECEDING and CURRENT ROW) f_c,
  sum(cost) over(order by orderdate rows between 1 PRECEDING and CURRENT ROW ) p_c,
  sum(cost) over(order by orderdate rows between 1 PRECEDING and 1 FOLLOWING ) p_n,
  sum(cost) over(order by orderdate rows between CURRENT ROW and 1 FOLLOWING ) c_n,
  sum(cost) over(order by orderdate rows between CURRENT ROW and UNBOUNDED FOLLOWING ) c_l
from
  business
+-------+-------------+-------+------+------+------+------+------+
| name  |  orderdate  | cost  | f_c  | p_c  | p_n  | c_n  | c_l  |
+-------+-------------+-------+------+------+------+------+------+
| jack  | 2017-01-01  | 10    | 10   | 10   | 25   | 25   | 661  |
| tony  | 2017-01-02  | 15    | 25   | 25   | 54   | 44   | 651  |
| tony  | 2017-01-04  | 29    | 54   | 44   | 90   | 75   | 636  |
| jack  | 2017-01-05  | 46    | 100  | 75   | 125  | 96   | 607  |
| tony  | 2017-01-07  | 50    | 150  | 96   | 151  | 105  | 561  |
| jack  | 2017-01-08  | 55    | 205  | 105  | 128  | 78   | 511  |
| jack  | 2017-02-03  | 23    | 228  | 78   | 120  | 65   | 456  |
| jack  | 2017-04-06  | 42    | 270  | 65   | 127  | 104  | 433  |
| mart  | 2017-04-08  | 62    | 332  | 104  | 172  | 130  | 391  |
| mart  | 2017-04-09  | 68    | 400  | 130  | 205  | 143  | 329  |
| mart  | 2017-04-11  | 75    | 475  | 143  | 237  | 169  | 261  |
| mart  | 2017-04-13  | 94    | 569  | 169  | 181  | 106  | 186  |
| neil  | 2017-05-10  | 12    | 581  | 106  | 186  | 92   | 92   |
| neil  | 2017-06-12  | 80    | 661  | 92   | 92   | 80   | 80   |
+-------+-------------+-------+------+------+------+------+------+


éœ€æ±‚å››: æŸ¥è¯¢æ¯ä¸ªé¡¾å®¢ä¸Šæ¬¡çš„è´­ä¹°æ—¶é—´ åŠ ä¸‹ä¸€æ¬¡çš„è´­ä¹°æ—¶é—´ 
select
   name,
   cost, 
   orderdate c_orderdate,
   lag(orderdate ,1 ,'1970-01-01') over(partition by name  order by orderdate) p_orderdate,
   lead(orderdate ,1 ,'9999-01-01') over(partition by name  order by orderdate) p_orderdate
from 
  business

+-------+-------+--------------+--------------+--------------+
| name  | cost  | c_orderdate  | p_orderdate  | p_orderdate  |
+-------+-------+--------------+--------------+--------------+
| jack  | 10    | 2017-01-01   | 1970-01-01   | 2017-01-05   |
| jack  | 46    | 2017-01-05   | 2017-01-01   | 2017-01-08   |
| jack  | 55    | 2017-01-08   | 2017-01-05   | 2017-02-03   |
| jack  | 23    | 2017-02-03   | 2017-01-08   | 2017-04-06   |
| jack  | 42    | 2017-04-06   | 2017-02-03   | 9999-01-01   |
| mart  | 62    | 2017-04-08   | 1970-01-01   | 2017-04-09   |
| mart  | 68    | 2017-04-09   | 2017-04-08   | 2017-04-11   |
| mart  | 75    | 2017-04-11   | 2017-04-09   | 2017-04-13   |
| mart  | 94    | 2017-04-13   | 2017-04-11   | 9999-01-01   |
| neil  | 12    | 2017-05-10   | 1970-01-01   | 2017-06-12   |
| neil  | 80    | 2017-06-12   | 2017-05-10   | 9999-01-01   |
| tony  | 15    | 2017-01-02   | 1970-01-01   | 2017-01-04   |
| tony  | 29    | 2017-01-04   | 2017-01-02   | 2017-01-07   |
| tony  | 50    | 2017-01-07   | 2017-01-04   | 9999-01-01   |
+-------+-------+--------------+--------------+--------------+


éœ€æ±‚äº”: æŸ¥è¯¢å‰20%æ—¶é—´çš„è®¢å•ä¿¡æ¯

select 
  t1.name, 
  t1.orderdate,
  t1.cost ,
  t1.gid
from 
(select
  name, 
  orderdate,
  cost, 
  ntile(5) over(order by orderdate ) gid
from 
  business) t1
where t1.gid = 1 ; 

+----------+---------------+----------+---------+
| t1.name  | t1.orderdate  | t1.cost  | t1.gid  |
+----------+---------------+----------+---------+
| jack     | 2017-01-01    | 10       | 1       |
| tony     | 2017-01-02    | 15       | 1       |
| tony     | 2017-01-04    | 29       | 1       |
+----------+---------------+----------+---------+



~~~



#### æ€»ç»“

| å‡½æ•°                                                         | æ¦‚è¿°                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| over()                                                       | ä¼šä¸ºæ¯æ¡æ•°æ®éƒ½å¼€å¯ä¸€ä¸ªçª—å£. é»˜è®¤çš„çª—å£å¤§å°å°±æ˜¯å½“å‰æ•°æ®é›†çš„å¤§å°. |
| over(partition by ....)                                      | ä¼šæŒ‰ç…§æŒ‡å®šçš„å­—æ®µè¿›è¡Œåˆ†åŒºï¼Œ å°†åˆ†åŒºå­—æ®µçš„å€¼ç›¸åŒçš„æ•°æ®åˆ’åˆ†åˆ°ç›¸åŒçš„åŒºã€‚<br/>                          æ¯ä¸ªåŒºä¸­çš„æ¯æ¡æ•°æ®éƒ½ä¼šå¼€å¯ä¸€ä¸ªçª—å£.æ¯æ¡æ•°æ®çš„çª—å£å¤§å°é»˜è®¤ä¸ºå½“å‰åˆ†åŒºæ•°æ®é›†çš„å¤§å°. |
| over(order by ....)                                          | ä¼šåœ¨çª—å£ä¸­æŒ‰ç…§æŒ‡å®šçš„å­—æ®µå¯¹æ•°æ®è¿›è¡Œæ’åº. <br/>                      ä¼šä¸ºæ¯æ¡æ•°æ®éƒ½å¼€å¯ä¸€ä¸ªçª—å£,é»˜è®¤çš„çª—å£å¤§å°ä¸ºä»æ•°æ®é›†å¼€å§‹åˆ°å½“å‰è¡Œ. |
| over(partition by .... order by ....)                        | ä¼šæŒ‰ç…§æŒ‡å®šçš„å­—æ®µè¿›è¡Œåˆ†åŒºï¼Œ å°†åˆ†åŒºå­—æ®µçš„å€¼ç›¸åŒçš„æ•°æ®åˆ’åˆ†åˆ°ç›¸åŒçš„åŒº,<br/>                                       åœ¨æ¯ä¸ªåŒºä¸­ä¼šæŒ‰ç…§æŒ‡å®šçš„å­—æ®µè¿›è¡Œæ’åº. <br/>                                       ä¼šä¸ºæ¯æ¡æ•°æ®éƒ½å¼€å¯ä¸€ä¸ªçª—å£,é»˜è®¤çš„çª—å£å¤§å°ä¸ºå½“å‰åˆ†åŒºä¸­ä»æ•°æ®é›†å¼€å§‹åˆ°å½“å‰è¡Œ. |
| over(partition by ... order by ... rows between ... and ....) | æŒ‡å®šæ¯æ¡æ•°æ®çš„çª—å£å¤§å°.                                      |



#### å…³é”®å­—



| å…³é”®å­—         |                               |
| -------------- | ----------------------------- |
| order by       | å…¨å±€æ’åº æˆ–è€… çª—å£å‡½æ•°ä¸­æ’åº. |
| distribute by  | åˆ†åŒº                          |
| sort by        | åŒºå†…æ’åº                      |
| cluster by     | åˆ†åŒºæ’åº                      |
| partition by   | çª—å£å‡½æ•°ä¸­åˆ†åŒº                |
| partitioned by | å»ºè¡¨æŒ‡å®šåˆ†åŒºå­—æ®µ              |
| clustered by   | å»ºè¡¨æŒ‡å®šåˆ†æ¡¶å­—æ®µ              |







### æ’åå‡½æ•°



| å‡½æ•°è¯´æ˜     |                                             |
| ------------ | ------------------------------------------- |
| RANK()       | æ’åºç›¸åŒæ—¶ä¼šé‡å¤ï¼Œæ€»æ•°ä¸ä¼šå˜  1ï¼Œ2ï¼Œ2ï¼Œ4    |
| DENSE_RANK() | æ’åºç›¸åŒæ—¶ä¼šé‡å¤ï¼Œæ€»æ•°ä¼šå‡å°‘  1ï¼Œ2ï¼Œ2ï¼Œ3ï¼Œ4 |
| ROW_NUMBER() | ä¼šæ ¹æ®é¡ºåºè®¡ç®—                              |



> æ¡ˆä¾‹
>



| name   | subject | score |
| ------ | ------- | ----- |
| å­™æ‚Ÿç©º | è¯­æ–‡    | 87    |
| å­™æ‚Ÿç©º | æ•°å­¦    | 95    |
| å­™æ‚Ÿç©º | è‹±è¯­    | 68    |
| å¤§æµ·   | è¯­æ–‡    | 94    |
| å¤§æµ·   | æ•°å­¦    | 56    |
| å¤§æµ·   | è‹±è¯­    | 84    |
| å®‹å®‹   | è¯­æ–‡    | 64    |
| å®‹å®‹   | æ•°å­¦    | 86    |
| å®‹å®‹   | è‹±è¯­    | 84    |
| å©·å©·   | è¯­æ–‡    | 65    |
| å©·å©·   | æ•°å­¦    | 85    |
| å©·å©·   | è‹±è¯­    | 78    |

~~~sql
ğŸ‰éœ€æ±‚: æŒ‰ç…§å­¦ç§‘è¿›è¡Œæ’å
select
  name, 
  subject,
  score,
  rank() over(partition by subject order by score desc ) rk,
  dense_rank() over(partition by subject order by score desc ) drk ,
  row_number() over(partition by subject order by score desc ) rn
from
  score 
+-------+----------+--------+-----+------+-----+
| name  | subject  | score  | rk  | drk  | rn  |
+-------+----------+--------+-----+------+-----+
| å­™æ‚Ÿç©º  | æ•°å­¦       | 95     | 1   | 1    | 1   |
| å®‹å®‹    | æ•°å­¦       | 86     | 2   | 2    | 2   |
| å©·å©·    | æ•°å­¦       | 85     | 3   | 3    | 3   |
| å¤§æµ·    | æ•°å­¦       | 56     | 4   | 4    | 4   |
| å®‹å®‹    | è‹±è¯­       | 84     | 1   | 1    | 1   |
| å¤§æµ·    | è‹±è¯­       | 84     | 1   | 1    | 2   |
| å©·å©·    | è‹±è¯­       | 78     | 3   | 2    | 3   |
| å­™æ‚Ÿç©º  | è‹±è¯­       | 68     | 4   | 3    | 4   |
| å¤§æµ·    | è¯­æ–‡       | 94     | 1   | 1    | 1   |
| å­™æ‚Ÿç©º  | è¯­æ–‡       | 87     | 2   | 2    | 2   |
| å©·å©·    | è¯­æ–‡       | 65     | 3   | 3    | 3   |
| å®‹å®‹    | è¯­æ–‡       | 64     | 4   | 4    | 4   |
+-------+----------+--------+-----+------+-----+
~~~



### å¸¸ç”¨å‡½æ•°

#### å¸¸ç”¨æ—¥æœŸå‡½æ•°

~~~sql

1. unix_timestamp:è¿”å›å½“å‰æˆ–æŒ‡å®šæ—¶é—´çš„æ—¶é—´æˆ³	
select unix_timestamp();
select unix_timestamp("2020-10-28",'yyyy-MM-dd');

2. from_unixtimeï¼šå°†æ—¶é—´æˆ³è½¬ä¸ºæ—¥æœŸæ ¼å¼
select from_unixtime(1603843200);

3. current_dateï¼šå½“å‰æ—¥æœŸ
select current_date;

4. current_timestampï¼šå½“å‰çš„æ—¥æœŸåŠ æ—¶é—´
select current_timestamp;

5. to_dateï¼šæŠ½å–æ—¥æœŸéƒ¨åˆ†
select to_date('2020-10-28 12:12:12');

6. yearï¼šè·å–å¹´
select year('2020-10-28 12:12:12');

7. monthï¼šè·å–æœˆ
select month('2020-10-28 12:12:12');

8. dayï¼šè·å–æ—¥
select day('2020-10-28 12:12:12');

9. hourï¼šè·å–æ—¶
select hour('2020-10-28 12:13:14');

10. minuteï¼šè·å–åˆ†
select minute('2020-10-28 12:13:14');

11. secondï¼šè·å–ç§’
select second('2020-10-28 12:13:14');

12. weekofyearï¼šå½“å‰æ—¶é—´æ˜¯ä¸€å¹´ä¸­çš„ç¬¬å‡ å‘¨
select weekofyear('2020-10-28 12:12:12');

13. dayofmonthï¼šå½“å‰æ—¶é—´æ˜¯ä¸€ä¸ªæœˆä¸­çš„ç¬¬å‡ å¤©
select dayofmonth('2020-10-28 12:12:12');

14. months_betweenï¼š ä¸¤ä¸ªæ—¥æœŸé—´çš„æœˆä»½
select months_between('2020-04-01','2020-10-28');

15. add_monthsï¼šæ—¥æœŸåŠ å‡æœˆ
select add_months('2020-10-28',-3);

16. datediffï¼šä¸¤ä¸ªæ—¥æœŸç›¸å·®çš„å¤©æ•°
select datediff('2020-11-04','2020-10-28');

17. date_addï¼šæ—¥æœŸåŠ å¤©æ•°
select date_add('2020-10-28',4);

18. date_subï¼šæ—¥æœŸå‡å¤©æ•°
select date_sub('2020-10-28',-4);

19. last_dayï¼šæ—¥æœŸçš„å½“æœˆçš„æœ€åä¸€å¤©
select last_day('2020-02-30');

20. date_format(): æ ¼å¼åŒ–æ—¥æœŸ
select date_format('2020-10-28 12:12:12','yyyy/MM/dd HH:mm:ss');
~~~



#### å¸¸ç”¨å–æ•´å‡½æ•°

~~~sql
21. roundï¼š å››èˆäº”å…¥
select round(3.14);
select round(3.54);

22. ceilï¼š  å‘ä¸Šå–æ•´
select ceil(3.14);
select ceil(3.54);

floorï¼š å‘ä¸‹å–æ•´
23. select floor(3.14);
select floor(3.54);
~~~



#### å¸¸ç”¨å­—ç¬¦ä¸²æ“ä½œå‡½æ•°

~~~sql
24. upperï¼š è½¬å¤§å†™
select upper('low');

25. lowerï¼š è½¬å°å†™
select lower('low');

26. lengthï¼š é•¿åº¦
select length("atguigu");

27. trimï¼š  å‰åå»ç©ºæ ¼
select trim(" atguigu ");

28. lpadï¼š å‘å·¦è¡¥é½ï¼Œåˆ°æŒ‡å®šé•¿åº¦
select lpad('atguigu',9,'g');

29. rpadï¼š  å‘å³è¡¥é½ï¼Œåˆ°æŒ‡å®šé•¿åº¦
select rpad('atguigu',9,'g');

30. regexp_replaceï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ç›®æ ‡å­—ç¬¦ä¸²ï¼ŒåŒ¹é…æˆåŠŸåæ›¿æ¢ï¼
SELECT regexp_replace('2020/10/25', '/', '-');
~~~



#### é›†åˆæ“ä½œ

~~~sql
31. sizeï¼š é›†åˆä¸­å…ƒç´ çš„ä¸ªæ•°
select size(friends) from test3;

32. map_keysï¼š è¿”å›mapä¸­çš„key
select map_keys(children) from test3;

33. map_values: è¿”å›mapä¸­çš„value
select map_values(children) from test3;

34. array_contains: åˆ¤æ–­arrayä¸­æ˜¯å¦åŒ…å«æŸä¸ªå…ƒç´ 
select array_contains(friends,'bingbing') from test3;

35. sort_arrayï¼š å°†arrayä¸­çš„å…ƒç´ æ’åº
select sort_array(friends) from test3;
~~~

#### å¤šç»´åˆ†æ

```sql
grouping sets:å¤šç»´åˆ†æ
		å»ºè¡¨
create table testgrouping (
  id int, 
  name string, 
  sex string, 
  deptno int 
)
row format delimited fields terminated by ',';

1001,zhangsan,man,10
1002,xiaohua,female,10
1003,lisi,man,20
1004,xiaohong,female,20

éœ€æ±‚:  ç»Ÿè®¡æ¯ä¸ªéƒ¨é—¨å„å¤šå°‘äººï¼Œ 
      ç”·å¥³å„å¤šå°‘äººï¼Œ 
      æ¯ä¸ªéƒ¨é—¨ä¸­ç”·å¥³å„å¤šå°‘äºº 

select deptno, sex ,count(id) from testgrouping group by deptno,sex  grouping sets( (deptno,sex), sex , deptno )
```

==ç›¸å½“äºå¤šä¸ªgroup   by  çš„èšåˆ==







## è‡ªå®šä¹‰å‡½æ•°



[æ–‡æ¡£](https://cwiki.apache.org/confluence/display/Hive/HivePlugins)

ğŸŒ´Hive è‡ªå¸¦äº†ä¸€äº›å‡½æ•°ï¼Œæ¯”å¦‚ï¼šmax/minç­‰ï¼Œä½†æ˜¯æ•°é‡æœ‰é™ï¼Œè‡ªå·±å¯ä»¥é€šè¿‡è‡ªå®šä¹‰UDFæ¥æ–¹ä¾¿çš„æ‰©å±•ã€‚



ğŸŒ´å½“Hiveæä¾›çš„å†…ç½®å‡½æ•°æ— æ³•æ»¡è¶³ä½ çš„ä¸šåŠ¡å¤„ç†éœ€è¦æ—¶ï¼Œæ­¤æ—¶å°±å¯ä»¥è€ƒè™‘ä½¿ç”¨ç”¨æˆ·è‡ªå®šä¹‰å‡½æ•°ï¼ˆUDFï¼šuser-defined functionï¼‰ã€‚





| ç”¨æˆ·è‡ªå®šä¹‰å‡½æ•°ç±»åˆ«                              | æ¦‚è¿°                                       |
| ----------------------------------------------- | ------------------------------------------ |
| UDFï¼ˆUser-Defined-Functionï¼‰                    | ä¸€è¿›ä¸€å‡º                                   |
| UDAFï¼ˆUser-Defined Aggregation Functionï¼‰       | èšé›†å‡½æ•°ï¼Œå¤šè¿›ä¸€å‡º   ç±»ä¼¼äºï¼šcount/max/min |
| UDTFï¼ˆUser-Defined Table-Generating Functionsï¼‰ | ä¸€è¿›å¤šå‡º  å¦‚lateral view explode()         |



~~~sql
ï¼ˆ1ï¼‰ç»§æ‰¿Hiveæä¾›çš„ç±»
 		org.apache.hadoop.hive.ql.udf.generic.GenericUDF  
		org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;
ï¼ˆ2ï¼‰å®ç°ç±»ä¸­çš„æŠ½è±¡æ–¹æ³•

ï¼ˆ3ï¼‰åœ¨hiveçš„å‘½ä»¤è¡Œçª—å£åˆ›å»ºå‡½æ•°
		æ·»åŠ jar
		add jar linux_jar_path
		åˆ›å»ºfunction
		ä¸´æ—¶åŠ temporary  æ°¸ä¹…çš„è¯jaråŒ…æ”¾libé‡å¯hive
		create [temporary] function [dbname.]function_nameï¼ˆå‡½æ•°åï¼‰ AS class_name;
		
ï¼ˆ4ï¼‰åœ¨hiveçš„å‘½ä»¤è¡Œçª—å£åˆ é™¤å‡½æ•°
		drop [temporary] function [if exists] [dbname.]function_name;

~~~





## è‡ªå®šä¹‰UDFå‡½æ•°



> **éœ€æ±‚:**è‡ªå®šä¹‰ä¸€ä¸ªUDFå®ç°è®¡ç®—ç»™å®šå­—ç¬¦ä¸²çš„é•¿åº¦ï¼Œ



**åˆ›å»ºä¸€ä¸ªMavenå·¥ç¨‹Hive**

**ğŸ”—å¯¼å…¥ä¾èµ–**

~~~XML
<dependencies>
		<dependency>
			<groupId>org.apache.hive</groupId>
			<artifactId>hive-exec</artifactId>
			<version>3.1.2</version>
		</dependency>
</dependencies>
~~~



**ğŸ”¨ä¸šåŠ¡ä»£ç **

~~~java

/**
 * è‡ªå®šä¹‰UDFå‡½æ•°ï¼Œéœ€è¦ç»§æ‰¿GenericUDFç±»
 * éœ€æ±‚: è®¡ç®—æŒ‡å®šå­—ç¬¦ä¸²çš„é•¿åº¦
 */
public class MyStringLength extends GenericUDF {
    /**
     *
     * @param arguments è¾“å…¥å‚æ•°ç±»å‹çš„é‰´åˆ«å™¨å¯¹è±¡
     * @return è¿”å›å€¼ç±»å‹çš„é‰´åˆ«å™¨å¯¹è±¡
     * @throws UDFArgumentException
     */
    @Override
    public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
        // åˆ¤æ–­è¾“å…¥å‚æ•°çš„ä¸ªæ•°
        if(arguments.length !=1){
            throw new UDFArgumentLengthException("Input Args Length Error!!!");
        }
        // åˆ¤æ–­è¾“å…¥å‚æ•°çš„ç±»å‹
        if(!arguments[0].getCategory().equals(ObjectInspector.Category.PRIMITIVE)){
            throw new UDFArgumentTypeException(0,"Input Args Type Error!!!");
        }
        //å‡½æ•°æœ¬èº«è¿”å›å€¼ä¸ºintï¼Œéœ€è¦è¿”å›intç±»å‹çš„é‰´åˆ«å™¨å¯¹è±¡
        return PrimitiveObjectInspectorFactory.javaIntObjectInspector;
    }

    /**
     * å‡½æ•°çš„é€»è¾‘å¤„ç†
     * @param arguments è¾“å…¥çš„å‚æ•°
     * @return è¿”å›å€¼
     * @throws HiveException
     */
    @Override
    public Object evaluate(DeferredObject[] arguments) throws HiveException {
       if(arguments[0].get() == null){
           return 0 ;
       }
       return arguments[0].get().toString().length();
    }

    @Override
    public String getDisplayString(String[] children) {
        return "";
    }
}
~~~



**ğŸ‘‡NEXT** 

~~~sql
1ï¼‰æ‰“æˆjaråŒ…ä¸Šä¼ åˆ°æœåŠ¡å™¨/opt/module/hive/datas/myudf.jar

2ï¼‰å°†jaråŒ…æ·»åŠ åˆ°hiveçš„classpath
hive (default)> add jar /opt/module/hive/datas/myudf.jar;

3ï¼‰åˆ›å»ºä¸´æ—¶å‡½æ•°ä¸å¼€å‘å¥½çš„java classå…³è”
hive (default)> create temporary function my_len as "com.atguigu.
MyStringLength";

4ï¼‰å³å¯åœ¨hqlä¸­ä½¿ç”¨è‡ªå®šä¹‰çš„å‡½æ•° 
hive (default)> select ename,my_len(ename) ename_len from emp;
~~~





## è‡ªå®šä¹‰UDTFå‡½æ•°



> **éœ€æ±‚**  è‡ªå®šä¹‰ä¸€ä¸ªUDTFå®ç°å°†ä¸€ä¸ªä»»æ„åˆ†å‰²ç¬¦çš„å­—ç¬¦ä¸²åˆ‡å‰²æˆç‹¬ç«‹çš„å•è¯ï¼Œ

ğŸ‘‡

~~~sql
hive(default)> select myudtf("hello,world,hadoop,hive", ",");

hello
world
hadoop
hive
~~~



**ä¸šåŠ¡ä»£ç **

~~~sql
public class MyUDTF extends GenericUDTF {

    private ArrayList<String> outList = new ArrayList<>();

    @Override
    public StructObjectInspector initialize(StructObjectInspector argOIs) throws UDFArgumentException {


        //1.å®šä¹‰è¾“å‡ºæ•°æ®çš„åˆ—åå’Œç±»å‹
        List<String> fieldNames = new ArrayList<>();
        List<ObjectInspector> fieldOIs = new ArrayList<>();

        //2.æ·»åŠ è¾“å‡ºæ•°æ®çš„åˆ—åå’Œç±»å‹
        fieldNames.add("lineToWord");
        fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);

        return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);
    }

    @Override
    public void process(Object[] args) throws HiveException {
        
        //1.è·å–åŸå§‹æ•°æ®
        String arg = args[0].toString();

        //2.è·å–æ•°æ®ä¼ å…¥çš„ç¬¬äºŒä¸ªå‚æ•°ï¼Œæ­¤å¤„ä¸ºåˆ†éš”ç¬¦
        String splitKey = args[1].toString();

        //3.å°†åŸå§‹æ•°æ®æŒ‰ç…§ä¼ å…¥çš„åˆ†éš”ç¬¦è¿›è¡Œåˆ‡åˆ†
        String[] fields = arg.split(splitKey);

        //4.éå†åˆ‡åˆ†åçš„ç»“æœï¼Œå¹¶å†™å‡º
        for (String field : fields) {

            //é›†åˆä¸ºå¤ç”¨çš„ï¼Œé¦–å…ˆæ¸…ç©ºé›†åˆ
            outList.clear();

            //å°†æ¯ä¸€ä¸ªå•è¯æ·»åŠ è‡³é›†åˆ
            outList.add(field);

            //å°†é›†åˆå†…å®¹å†™å‡º
            forward(outList);
        }
    }

    @Override
    public void close() throws HiveException {

    }
}
~~~



**ğŸ‘‡NEXT**

~~~sql
ğŸŒµæ‰“æˆjaråŒ…ä¸Šä¼ åˆ°æœåŠ¡å™¨/opt/module/hive/data/myudtf.jar
ğŸŒµå°†jaråŒ…æ·»åŠ åˆ°hiveçš„classpathä¸‹
hive (default)> add jar /opt/module/hive/data/myudtf.jar;

ğŸŒµåˆ›å»ºä¸´æ—¶å‡½æ•°ä¸å¼€å‘å¥½çš„java classå…³è”
hive (default)> create temporary function myudtf as 
"com.atguigu.hive.MyUDTF";

ğŸŒµä½¿ç”¨è‡ªå®šä¹‰çš„å‡½æ•°
hive (default)> select myudtf("hello,world,hadoop,hive",",") ;
~~~





# ä¹ã€å‹ç¼©å’Œå­˜å‚¨



MRæ”¯æŒçš„å‹ç¼©ç¼–ç 

| å‹ç¼©æ ¼å¼ | ç®—æ³•    | æ–‡ä»¶æ‰©å±•å | æ˜¯å¦å¯åˆ‡åˆ† |
| -------- | ------- | ---------- | ---------- |
| DEFLATE  | DEFLATE | .deflate   | å¦         |
| Gzip     | DEFLATE | .gz        | å¦         |
| bzip2    | bzip2   | .bz2       | æ˜¯         |
| LZO      | LZO     | .lzo       | æ˜¯         |
| Snappy   | Snappy  | .snappy    | å¦         |



| å‹ç¼©æ ¼å¼ | å¯¹åº”çš„ç¼–ç /è§£ç å™¨                          |
| -------- | ------------------------------------------ |
| DEFLATE  | org.apache.hadoop.io.compress.DefaultCodec |
| gzip     | org.apache.hadoop.io.compress.GzipCodec    |
| bzip2    | org.apache.hadoop.io.compress.BZip2Codec   |
| LZO      | com.hadoop.compression.lzo.LzopCodec       |
| Snappy   | org.apache.hadoop.io.compress.SnappyCodec  |



è¦åœ¨Hadoopä¸­å¯ç”¨å‹ç¼©ï¼Œå¯ä»¥é…ç½®å¦‚ä¸‹å‚æ•°ï¼ˆ`mapred-site.xml`æ–‡ä»¶ä¸­ï¼‰

| å‚æ•°                                                 | é»˜è®¤å€¼                                                       | é˜¶æ®µ        | å»ºè®®                                         |
| ---------------------------------------------------- | ------------------------------------------------------------ | ----------- | -------------------------------------------- |
| io.compression.codecs    ï¼ˆåœ¨core-site.xmlä¸­é…ç½®ï¼‰   | org.apache.hadoop.io.compress.DefaultCodec,  org.apache.hadoop.io.compress.GzipCodec,  org.apache.hadoop.io.compress.BZip2Codec,  org.apache.hadoop.io.compress.Lz4Codec | è¾“å…¥å‹ç¼©    | Hadoopä½¿ç”¨æ–‡ä»¶æ‰©å±•ååˆ¤æ–­æ˜¯å¦æ”¯æŒæŸç§ç¼–è§£ç å™¨ |
| ==mapreduce.map.output.compress==                    | false                                                        | mapperè¾“å‡º  | è¿™ä¸ªå‚æ•°è®¾ä¸ºtrueå¯ç”¨å‹ç¼©                     |
| ==mapreduce.map.output.compress.codec==              | org.apache.hadoop.io.compress.DefaultCodec                   | mapperè¾“å‡º  | ä½¿ç”¨LZOã€LZ4æˆ–snappyç¼–è§£ç å™¨åœ¨æ­¤é˜¶æ®µå‹ç¼©æ•°æ® |
| ==mapreduce.output.fileoutputformat.compress==       | false                                                        | reducerè¾“å‡º | è¿™ä¸ªå‚æ•°è®¾ä¸ºtrueå¯ç”¨å‹ç¼©                     |
| ==mapreduce.output.fileoutputformat.compress.codec== | org.apache.hadoop.io.compress. DefaultCodec                  | reducerè¾“å‡º | ä½¿ç”¨æ ‡å‡†å·¥å…·æˆ–è€…ç¼–è§£ç å™¨ï¼Œå¦‚gzipå’Œbzip2      |
| mapreduce.output.fileoutputformat.compress.type      | RECORD                                                       | reducerè¾“å‡º | SequenceFileè¾“å‡ºä½¿ç”¨çš„å‹ç¼©ç±»å‹ï¼šNONEå’ŒBLOCK  |



### Mapè¾“å‡ºé˜¶æ®µå‹ç¼©-ğŸ˜ƒ

~~~sql
ï¼ˆ1ï¼‰å¼€å¯hiveä¸­é—´ä¼ è¾“æ•°æ®å‹ç¼©åŠŸèƒ½
hive (default)>set hive.exec.compress.intermediate=true;

ï¼ˆ2ï¼‰å¼€å¯mapreduceä¸­mapè¾“å‡ºå‹ç¼©åŠŸèƒ½
hive (default)>set mapreduce.map.output.compress=true;

ï¼ˆ3ï¼‰è®¾ç½®mapreduceä¸­mapè¾“å‡ºæ•°æ®çš„å‹ç¼©æ–¹å¼
hive (default)>set mapreduce.map.output.compress.codec=
 org.apache.hadoop.io.compress.SnappyCodec;
 
ï¼ˆ4ï¼‰æ‰§è¡ŒæŸ¥è¯¢è¯­å¥
	hive (default)> select count(ename) name from emp;

~~~



### Reduceè¾“å‡ºé˜¶æ®µå‹ç¼©-ğŸ˜€



~~~sql
ï¼ˆ1ï¼‰å¼€å¯hiveæœ€ç»ˆè¾“å‡ºæ•°æ®å‹ç¼©åŠŸèƒ½
hive (default)>set hive.exec.compress.output=true;

ï¼ˆ2ï¼‰å¼€å¯mapreduceæœ€ç»ˆè¾“å‡ºæ•°æ®å‹ç¼©
hive (default)>set mapreduce.output.fileoutputformat.compress=true;

ï¼ˆ3ï¼‰è®¾ç½®mapreduceæœ€ç»ˆæ•°æ®è¾“å‡ºå‹ç¼©æ–¹å¼
hive (default)> set mapreduce.output.fileoutputformat.compress.codec =
 org.apache.hadoop.io.compress.SnappyCodec;
 
ï¼ˆ4ï¼‰è®¾ç½®mapreduceæœ€ç»ˆæ•°æ®è¾“å‡ºå‹ç¼©ä¸ºå—å‹ç¼©
hive (default)> set mapreduce.output.fileoutputformat.compress.type=BLOCK;

ï¼ˆ5ï¼‰æµ‹è¯•ä¸€ä¸‹è¾“å‡ºç»“æœæ˜¯å¦æ˜¯å‹ç¼©æ–‡ä»¶
hive (default)> insert overwrite local directory
 '/opt/module/hive/datas/distribute-result' select * from emp distribute by deptno sort by empno desc;
~~~



## æ–‡ä»¶å­˜å‚¨æ ¼å¼



â€‹                                   							ğŸ‘‡æ–‡æœ¬æ ¼å¼      ğŸ‘‡KVæ ¼å¼

Hiveæ”¯æŒçš„å­˜å‚¨æ•°æ®çš„æ ¼å¼ä¸»è¦æœ‰ï¼šTEXTFILE ã€SEQUENCEFILEã€==ORC==ã€==PARQUET==ã€‚



### åˆ—å¼å­˜å‚¨å’Œè¡Œå¼å­˜å‚¨



**è¡Œå­˜å‚¨çš„ç‰¹ç‚¹**

æŸ¥è¯¢æ»¡è¶³æ¡ä»¶çš„ä¸€æ•´è¡Œæ•°æ®çš„æ—¶å€™ï¼Œåˆ—å­˜å‚¨åˆ™éœ€è¦å»æ¯ä¸ªèšé›†çš„å­—æ®µæ‰¾åˆ°å¯¹åº”çš„æ¯ä¸ªåˆ—çš„å€¼ï¼Œè¡Œå­˜å‚¨åªéœ€è¦æ‰¾åˆ°å…¶ä¸­ä¸€ä¸ªå€¼ï¼Œå…¶ä½™çš„å€¼éƒ½åœ¨ç›¸é‚»åœ°æ–¹ï¼Œæ‰€ä»¥æ­¤æ—¶è¡Œå­˜å‚¨æŸ¥è¯¢çš„é€Ÿåº¦æ›´å¿«ã€‚



**åˆ—å­˜å‚¨çš„ç‰¹ç‚¹**

å› ä¸ºæ¯ä¸ªå­—æ®µçš„æ•°æ®èšé›†å­˜å‚¨ï¼Œåœ¨æŸ¥è¯¢åªéœ€è¦å°‘æ•°å‡ ä¸ªå­—æ®µçš„æ—¶å€™ï¼Œèƒ½å¤§å¤§å‡å°‘è¯»å–çš„æ•°æ®é‡ï¼›æ¯ä¸ªå­—æ®µçš„æ•°æ®ç±»å‹ä¸€å®šæ˜¯ç›¸åŒçš„ï¼Œåˆ—å¼å­˜å‚¨å¯ä»¥é’ˆå¯¹æ€§çš„è®¾è®¡æ›´å¥½çš„è®¾è®¡å‹ç¼©ç®—æ³•ã€‚

`TEXTFILEå’ŒSEQUENCEFILEçš„å­˜å‚¨æ ¼å¼éƒ½æ˜¯åŸºäºè¡Œå­˜å‚¨çš„ï¼›`

`ORCå’ŒPARQUETæ˜¯åŸºäºåˆ—å¼å­˜å‚¨çš„ã€‚`





| æ ¼å¼         | æ¦‚è¿°                                                         |
| ------------ | ------------------------------------------------------------ |
| TextFileæ ¼å¼ | é»˜è®¤æ ¼å¼ï¼Œæ•°æ®ä¸åšå‹ç¼©ï¼Œç£ç›˜å¼€é”€å¤§ï¼Œæ•°æ®è§£æå¼€é”€å¤§ã€‚å¯ç»“åˆGzipã€Bzip2ä½¿ç”¨ï¼Œä½†ä½¿ç”¨Gzipè¿™ç§æ–¹å¼ï¼Œhiveä¸ä¼šå¯¹æ•°æ®è¿›è¡Œåˆ‡åˆ†ï¼Œä»è€Œæ— æ³•å¯¹æ•°æ®è¿›è¡Œå¹¶è¡Œæ“ä½œã€‚ |
| Orcæ ¼å¼      | æ¯ä¸ªOrcæ–‡ä»¶ç”±1ä¸ªæˆ–å¤šä¸ªstripeç»„æˆï¼Œæ¯ä¸ªstripeä¸€èˆ¬ä¸ºHDFSçš„å—å¤§å° |
| Parquetæ ¼å¼  | Parquetæ–‡ä»¶æ˜¯ä»¥äºŒè¿›åˆ¶æ–¹å¼å­˜å‚¨çš„ï¼Œæ‰€ä»¥æ˜¯ä¸å¯ä»¥ç›´æ¥è¯»å–çš„ï¼Œæ–‡ä»¶ä¸­åŒ…æ‹¬è¯¥æ–‡ä»¶çš„æ•°æ®å’Œå…ƒæ•°æ®ï¼Œå› æ­¤Parquetæ ¼å¼æ–‡ä»¶æ˜¯è‡ªè§£æçš„ |



å­˜å‚¨æ–‡ä»¶çš„å¤§å°å¯¹æ¯”æ€»ç»“ï¼š

==ORC > Parquet > textFile==

æŸ¥è¯¢é€Ÿåº¦ç›¸è¿‘



## å­˜å‚¨å’Œå‹ç¼©ç»“åˆ



[OCRæ–‡æ¡£](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC)



**ORCå­˜å‚¨æ–¹å¼çš„å‹ç¼©ï¼š**

| Key                      | Default     | Notes                                                        |
| ------------------------ | ----------- | ------------------------------------------------------------ |
| orc.compress             | ZLIB        | high level  compression (one of NONE, ZLIB, SNAPPY)          |
| orc.compress.size        | 262,144     | number of bytes  in each compression chunk                   |
| orc.stripe.size          | 268,435,456 | number of bytes in each stripe                               |
| orc.row.index.stride     | 10,000      | number of rows between index entries (must be >= 1000)       |
| orc.create.index         | true        | whether to create  row indexes                               |
| orc.bloom.filter.columns | ""          | comma separated list of column names for which bloom filter should be  created |
| orc.bloom.filter.fpp     | 0.05        | false positive probability for bloom filter (must >0.0 and <1.0) |



==æ³¨æ„ï¼š==æ‰€æœ‰å…³äºORCFileçš„å‚æ•°éƒ½æ˜¯åœ¨HQLè¯­å¥çš„`TBLPROPERTIES`å­—æ®µé‡Œé¢å‡ºç°

â€‹										==OCRé»˜è®¤zlib==

åˆ›å»ºä¸€ä¸ªZLIBå‹ç¼©çš„ORCå­˜å‚¨æ–¹å¼

~~~sql
å»ºè¡¨è¯­å¥
create table log_orc_zlib(
track_time string,
url string,
session_id string,
referer string,
ip string,
end_user_id string,
city_id string
)
row format delimited fields terminated by '\t'
stored as orc
ğŸ‘‰    tblproperties("orc.compress"="ZLIB");		ğŸ‘ˆ

ï¼ˆ2ï¼‰æ’å…¥æ•°æ®
insert into log_orc_zlib select * from log_text;

ï¼ˆ3ï¼‰æŸ¥çœ‹æ’å…¥åæ•°æ®
hive (default)> dfs -du -h /user/hive/warehouse/log_orc_zlib/ ;
~~~





**å­˜å‚¨æ–¹å¼å’Œå‹ç¼©æ€»ç»“**



åœ¨å®é™…çš„é¡¹ç›®å¼€å‘å½“ä¸­ï¼Œhiveè¡¨çš„æ•°æ®å­˜å‚¨æ ¼å¼ä¸€èˆ¬é€‰æ‹©ï¼š**orcæˆ–parquetã€‚å‹ç¼©æ–¹å¼ä¸€èˆ¬é€‰æ‹©snappyï¼Œlzoã€‚**







# åã€è°ƒä¼˜





## æ‰§è¡Œè®¡åˆ’

**è¯­æ³•**

`EXPLAIN [EXTENDED | DEPENDENCY | AUTHORIZATION] query`



`explain  SQLè¯­å¥`ï¼šæŸ¥çœ‹ç®€ç•¥æ‰§è¡Œè®¡åˆ’

`explain    extended   SQLè¯­å¥`ï¼šæŸ¥çœ‹è¯¦ç»†æ‰§è¡Œè®¡åˆ’



## FetchæŠ“å–



FetchæŠ“å–æ˜¯æŒ‡==Hiveä¸­å¯¹æŸäº›æƒ…å†µçš„æŸ¥è¯¢å¯ä»¥ä¸å¿…ä½¿ç”¨MapReduceè®¡ç®—==

ä¾‹å¦‚ï¼šSELECT * FROM employees;åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒHiveå¯ä»¥ç®€å•åœ°è¯»å–employeeå¯¹åº”çš„å­˜å‚¨ç›®å½•ä¸‹çš„æ–‡ä»¶ï¼Œç„¶åè¾“å‡ºæŸ¥è¯¢ç»“æœåˆ°æ§åˆ¶å°ã€‚



åœ¨`hive-default.xml.template`æ–‡ä»¶ä¸­`hive.fetch.task.conversion`é»˜è®¤æ˜¯moreï¼Œè€ç‰ˆæœ¬hiveé»˜è®¤æ˜¯minimalï¼Œè¯¥å±æ€§ä¿®æ”¹ä¸ºmoreä»¥åï¼Œåœ¨å…¨å±€æŸ¥æ‰¾ã€å­—æ®µæŸ¥æ‰¾ã€limitæŸ¥æ‰¾ç­‰éƒ½ä¸èµ°mapreduceã€‚

~~~xml

<property>
    <name>hive.fetch.task.conversion</name>
    <value>more</value>
    <description>
      Expects one of [none, minimal, more].
      Some select queries can be converted to single FETCH task minimizing latency.
      Currently the query should be single sourced not having any subquery and should not have any aggregations or distincts (which incurs RS), lateral views and joins.
      0. none : disable hive.fetch.task.conversion
      1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only
      2. more  : SELECT, FILTER, LIMIT only (support TABLESAMPLE and virtual columns)
    </description>
</property>

~~~





## æœ¬åœ°æ¨¡å¼



ğŸ‘‰ä¸»è¦é’ˆå¯¹çš„æ˜¯å°æ–‡ä»¶çš„å¤„ç†

==Hiveå¯ä»¥é€šè¿‡æœ¬åœ°æ¨¡å¼åœ¨å•å°æœºå™¨ä¸Šå¤„ç†æ‰€æœ‰çš„ä»»åŠ¡ã€‚å¯¹äºå°æ•°æ®é›†ï¼Œæ‰§è¡Œæ—¶é—´å¯ä»¥æ˜æ˜¾è¢«ç¼©çŸ­ã€‚==



**ğŸ”—æœ¬åœ°æ¨¡å¼ç›¸å…³è®¾ç½®**

~~~sql
set hive.exec.mode.local.auto=true;  //å¼€å¯æœ¬åœ°mr
//è®¾ç½®local mrçš„æœ€å¤§è¾“å…¥æ•°æ®é‡ï¼Œå½“è¾“å…¥æ•°æ®é‡å°äºè¿™ä¸ªå€¼æ—¶é‡‡ç”¨local  mrçš„æ–¹å¼ï¼Œé»˜è®¤ä¸º134217728ï¼Œå³128M

set hive.exec.mode.local.auto.inputbytes.max=50000000;
//è®¾ç½®local mrçš„æœ€å¤§è¾“å…¥æ–‡ä»¶ä¸ªæ•°ï¼Œå½“è¾“å…¥æ–‡ä»¶ä¸ªæ•°å°äºè¿™ä¸ªå€¼æ—¶é‡‡ç”¨local mrçš„æ–¹å¼ï¼Œé»˜è®¤ä¸º4

set hive.exec.mode.local.auto.input.files.max=10;
~~~

**ğŸ“¢å¼€å¯**

~~~sql
hive (default)> set hive.exec.mode.local.auto=true; 
~~~

**ğŸ“¢å…³é—­**

~~~sql
hive (default)> set hive.exec.mode.local.auto=false; 
~~~



## è¡¨çš„ä¼˜åŒ–





### å°è¡¨å¤§è¡¨Join(MapJoin)



==æ–°ç‰ˆçš„hiveå·²ç»å¯¹å°è¡¨JOINå¤§è¡¨å’Œå¤§è¡¨JOINå°è¡¨è¿›è¡Œäº†ä¼˜åŒ–ã€‚å°è¡¨æ”¾åœ¨å·¦è¾¹å’Œå³è¾¹å·²ç»æ²¡æœ‰æ˜æ˜¾åŒºåˆ«ã€‚==



**MapJoinå‚æ•°è®¾ç½®**

**ğŸ“¢å¼€å¯**

~~~sql
set hive.auto.convert.join = true; é»˜è®¤ä¸ºtrue
~~~

**ğŸ“¢å¤§è¡¨å°è¡¨çš„é˜ˆå€¼è®¾ç½®ï¼ˆé»˜è®¤25Mä»¥ä¸‹è®¤ä¸ºæ˜¯å°è¡¨ï¼‰**

~~~sql
set hive.mapjoin.smalltable.filesize = 25000000;
~~~







### å¤§è¡¨Joinå¤§è¡¨



> **ç©ºKEYè¿‡æ»¤**

==ç©ºkeyå¯¹æ•°æ®åŠæ— ç”¨==

ä¸è¿‡æ»¤ ç©ºkey  çš„è¯ç©ºkeyè¿›å…¥ä¸€ä¸ªreduce  é€ æˆæ•°æ®å€¾æ–œ



**ğŸ”¨æµ‹è¯•è¿‡æ»¤ç©ºid**

~~~sql
hive (default)> insert overwrite table jointable select n.* from (select * from nullidtable where id is not null ) n  left join bigtable o on n.id = o.id;												ğŸ‘†
~~~

æŸ¥çœ‹jobhistory

**âš™é…ç½®mapred-site.xml**

~~~xml
<property>
<name>mapreduce.jobhistory.address</name>
<value>hadoop102:10020</value>
</property>
<property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>hadoop102:19888</value>
</property>

å¯åŠ¨å†å²æœåŠ¡å™¨
sbin/mr-jobhistory-daemon.sh start historyserver

http://hadoop102:19888/jobhistory
~~~



> **ç©ºkeyè½¬æ¢**

==ç©ºkeyå¯¹æ•°æ®é›†æœ‰ç”¨==



å°†è¡¨aä¸­keyä¸ºç©ºçš„å­—æ®µèµ‹ä¸€ä¸ªéšæœºçš„å€¼ï¼Œä½¿å¾—æ•°æ®éšæœºå‡åŒ€åœ°åˆ†ä¸åˆ°ä¸åŒçš„reducerä¸Š





> **SMB(Sort Merge Bucket join)**

æ¡¶join

**ğŸ“¢ä¸€å¼ è¡¨ä¸­äº‹å…ˆè®¾å®šåˆ†æ¡¶è§„åˆ™ä½¿å¾—æ¯ä¸ªæ¡¶ä¸­çš„æ•°æ®ä¸ºæœ‰è§„å¾‹çš„é›†åˆ   å¦ä¸€å¼ è¡¨ä¹Ÿæ˜¯å¦‚æ­¤  åœ¨joinæ—¶ ä¼šæ ¹æ®ä¸¤å¼ è¡¨ç›¸å¯¹åº”çš„æ¡¶å»join  ä¸ä¼šå»joinæ•´ä¸ªæ•°æ®  ä»è€Œæé«˜æ•ˆç‡**



**ğŸš©**é›†ç¾¤æ»¡è´Ÿè·è¿è½¬æ—¶ æ¡¶ä¸ªæ•°=CPUæ•°â€”2[^Reduceå’ŒAppmaster]



**ğŸ“¢Joinå‰è®¾ç½®å‚æ•°**

~~~sql
set hive.optimize.bucketmapjoin = true;								/*æ¡¶joinä¼˜åŒ–*/
set hive.optimize.bucketmapjoin.sortedmerge = true;						/*æ’åºå½’å¹¶*/
set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;		/*è¯»æ•°æ®æ–¹å¼ä¸ºæ¡¶*/
~~~



> æ¡ˆä¾‹



~~~sql
ğŸ“Œåˆ›å»ºåˆ†æ¡¶è¡¨1,æ¡¶çš„ä¸ªæ•°ä¸è¦è¶…è¿‡å¯ç”¨CPUçš„æ ¸æ•°
create table bigtable_buck1(
    id bigint,
    t bigint,
    uid string,
    keyword string,
    url_rank int,
    click_num int,
    click_url string)
clustered by(id) 
sorted by(id)
into 6 buckets
row format delimited fields terminated by '\t';

insert into bigtable_buck1 select * from bigtable; 

ğŸ“Œåˆ›å»ºåˆ†é€šè¡¨2,æ¡¶çš„ä¸ªæ•°ä¸è¦è¶…è¿‡å¯ç”¨CPUçš„æ ¸æ•°
create table bigtable_buck2(
    id bigint,
    t bigint,
    uid string,
    keyword string,
    url_rank int,
    click_num int,
    click_url string)
clustered by(id)
sorted by(id) 
into 6 buckets
row format delimited fields terminated by '\t';

insert into bigtable_buck2 select * from bigtable;
~~~





### Group By



å¹¶ä¸æ˜¯æ‰€æœ‰çš„èšåˆæ“ä½œéƒ½éœ€è¦åœ¨Reduceç«¯å®Œæˆï¼Œå¾ˆå¤šèšåˆæ“ä½œéƒ½å¯ä»¥**å…ˆåœ¨Mapç«¯è¿›è¡Œéƒ¨åˆ†èšåˆï¼Œæœ€ååœ¨Reduceç«¯å¾—å‡ºæœ€ç»ˆç»“æœã€‚**



**ğŸ“¢Mapç«¯èšåˆå‚æ•°è®¾ç½®**

~~~sql
ğŸŒ³æ˜¯å¦åœ¨Mapç«¯è¿›è¡Œèšåˆï¼Œé»˜è®¤ä¸ºTrue
set hive.map.aggr = true

ğŸŒ³åœ¨Mapç«¯è¿›è¡Œèšåˆæ“ä½œçš„æ¡ç›®æ•°ç›®
set hive.groupby.mapaggr.checkinterval = 100000

ğŸŒ³æœ‰æ•°æ®å€¾æ–œçš„æ—¶å€™è¿›è¡Œè´Ÿè½½å‡è¡¡ï¼ˆé»˜è®¤æ˜¯falseï¼‰
set hive.groupby.skewindata = true
~~~

å½“é€‰é¡¹è®¾å®šä¸º trueï¼Œç”Ÿæˆçš„æŸ¥è¯¢è®¡åˆ’ä¼šæœ‰ä¸¤ä¸ªMR Jobã€‚ç¬¬ä¸€ä¸ªMR Jobä¸­ï¼ŒMapçš„è¾“å‡ºç»“æœä¼šéšæœºåˆ†å¸ƒåˆ°Reduceä¸­ï¼Œæ¯ä¸ªReduceåšéƒ¨åˆ†èšåˆæ“ä½œï¼Œå¹¶è¾“å‡ºç»“æœï¼Œè¿™æ ·å¤„ç†çš„ç»“æœæ˜¯ç›¸åŒçš„Group By Keyæœ‰å¯èƒ½è¢«åˆ†å‘åˆ°ä¸åŒçš„Reduceä¸­ï¼Œä»è€Œè¾¾åˆ°è´Ÿè½½å‡è¡¡çš„ç›®çš„ï¼›ç¬¬äºŒä¸ªMR Jobå†æ ¹æ®é¢„å¤„ç†çš„æ•°æ®ç»“æœæŒ‰ç…§Group By Keyåˆ†å¸ƒåˆ°Reduceä¸­ï¼ˆè¿™ä¸ªè¿‡ç¨‹å¯ä»¥ä¿è¯ç›¸åŒçš„Group By Keyè¢«åˆ†å¸ƒåˆ°åŒä¸€ä¸ªReduceä¸­ï¼‰ï¼Œæœ€åå®Œæˆæœ€ç»ˆçš„èšåˆæ“ä½œã€‚







### Count(Distinct) å»é‡ç»Ÿè®¡



æ•°æ®é‡å°çš„æ—¶å€™æ— æ‰€è°“ï¼Œæ•°æ®é‡å¤§çš„æƒ…å†µä¸‹ï¼Œç”±äºCOUNT DISTINCTæ“ä½œéœ€è¦ç”¨ä¸€ä¸ªReduce Taskæ¥å®Œæˆï¼Œè¿™ä¸€ä¸ªReduceéœ€è¦å¤„ç†çš„æ•°æ®é‡å¤ªå¤§ï¼Œå°±ä¼šå¯¼è‡´æ•´ä¸ªJobå¾ˆéš¾å®Œæˆï¼Œ==ä¸€èˆ¬COUNT DISTINCTä½¿ç”¨å…ˆGROUP BYå†COUNTçš„æ–¹å¼æ›¿æ¢==,ä½†æ˜¯éœ€è¦æ³¨æ„group byé€ æˆçš„æ•°æ®å€¾æ–œé—®é¢˜.



è®¾ç½®reduceä¸ªæ•°

~~~sql
set mapreduce.job.reduces = 5;
~~~

æ‰§è¡Œå»é‡idæŸ¥è¯¢

~~~sql
hive (default)> select count(distinct id) from bigtable;


Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 7.12 sec   HDFS Read: 
				  ğŸ‘†		 ğŸ‘†
120741990 HDFS Write: 7 SUCCESS
Total MapReduce CPU Time Spent: 7 seconds 120 msec
OK
c0
100001
Time taken: 23.607 seconds, Fetched: 1 row(s)

~~~

é‡‡ç”¨GROUP byå»é‡id

~~~sql
hive (default)> select count(id) from (select id from bigtable group by id) a;
							ğŸ‘‡
Stage-Stage-1: Map: 1  Reduce: 5   Cumulative CPU: 17.53 sec   HDFS Read: 120752703 HDFS Write: 580 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 4.29 sec2   HDFS Read: 9409 HDFS Write: 7 SUCCESS
Total MapReduce CPU Time Spent: 21 seconds 820 msec
OK
_c0
100001
Time taken: 50.795 seconds, Fetched: 1 row(s)

/*å¤šç”¨ä¸€ä¸ªJobæ¥å®Œæˆï¼Œä½†åœ¨æ•°æ®é‡å¤§çš„æƒ…å†µä¸‹ï¼Œè¿™ä¸ªç»å¯¹æ˜¯å€¼å¾—çš„ã€‚*/
~~~





### å°½é‡é¿å…ç¬›å¡å°”ç§¯

å°½é‡é¿å…ç¬›å¡å°”ç§¯ï¼Œjoinçš„æ—¶å€™ä¸åŠ onæ¡ä»¶ï¼Œæˆ–è€…æ— æ•ˆçš„onæ¡ä»¶ï¼ŒHiveåªèƒ½ä½¿ç”¨1ä¸ªreduceræ¥å®Œæˆç¬›å¡å°”ç§¯ã€‚



### è¡Œåˆ—è¿‡æ»¤

åˆ—å¤„ç†ï¼šåœ¨SELECTä¸­ï¼Œåªæ‹¿éœ€è¦çš„åˆ—ï¼Œå¦‚æœæœ‰åˆ†åŒºï¼Œå°½é‡ä½¿ç”¨åˆ†åŒºè¿‡æ»¤ï¼Œå°‘ç”¨SELECT *ã€‚

è¡Œå¤„ç†ï¼šåœ¨åˆ†åŒºå‰ªè£ä¸­ï¼Œå½“ä½¿ç”¨å¤–å…³è”æ—¶ï¼Œå¦‚æœå°†å‰¯è¡¨çš„è¿‡æ»¤æ¡ä»¶å†™åœ¨Whereåé¢ï¼Œé‚£ä¹ˆå°±ä¼šå…ˆå…¨è¡¨å…³è”ï¼Œä¹‹åå†è¿‡æ»¤



### åˆ†åŒºåˆ†æ¡¶



è§ä¸ŠğŸ‘†



## åˆç†è®¾ç½®MapåŠReduceæ•°





> å¤æ‚æ–‡ä»¶å¢åŠ Mapæ•°

å½“inputçš„æ–‡ä»¶éƒ½å¾ˆå¤§ï¼Œä»»åŠ¡é€»è¾‘å¤æ‚ï¼Œmapæ‰§è¡Œéå¸¸æ…¢çš„æ—¶å€™ï¼Œå¯ä»¥è€ƒè™‘å¢åŠ Mapæ•°ï¼Œæ¥ä½¿å¾—æ¯ä¸ªmapå¤„ç†çš„æ•°æ®é‡å‡å°‘ï¼Œä»è€Œæé«˜ä»»åŠ¡çš„æ‰§è¡Œæ•ˆç‡ã€‚

å¢åŠ mapçš„æ–¹æ³•ä¸ºï¼šæ ¹æ®

computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128Må…¬å¼ï¼Œè°ƒæ•´maxSizeæœ€å¤§å€¼ã€‚è®©maxSizeæœ€å¤§å€¼ä½äºblocksizeå°±å¯ä»¥å¢åŠ mapçš„ä¸ªæ•°ã€‚



>  å°æ–‡ä»¶è¿›è¡Œåˆå¹¶



~~~sql
ğŸŒ³åœ¨mapæ‰§è¡Œå‰åˆå¹¶å°æ–‡ä»¶ï¼Œå‡å°‘mapæ•°ï¼šCombineHiveInputFormatå…·æœ‰å¯¹å°æ–‡ä»¶è¿›è¡Œåˆå¹¶çš„åŠŸèƒ½ï¼ˆç³»ç»Ÿé»˜è®¤çš„æ ¼å¼ï¼‰ã€‚HiveInputFormatæ²¡æœ‰å¯¹å°æ–‡ä»¶åˆå¹¶åŠŸèƒ½ã€‚
set hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
ğŸŒ³åœ¨Map-Reduceçš„ä»»åŠ¡ç»“æŸæ—¶åˆå¹¶å°æ–‡ä»¶çš„è®¾ç½®ï¼š
åœ¨map-onlyä»»åŠ¡ç»“æŸæ—¶åˆå¹¶å°æ–‡ä»¶ï¼Œé»˜è®¤true
SET hive.merge.mapfiles = true;
åœ¨map-reduceä»»åŠ¡ç»“æŸæ—¶åˆå¹¶å°æ–‡ä»¶ï¼Œé»˜è®¤false
SET hive.merge.mapredfiles = true;
åˆå¹¶æ–‡ä»¶çš„å¤§å°ï¼Œé»˜è®¤256M
SET hive.merge.size.per.task = 268435456;
å½“è¾“å‡ºæ–‡ä»¶çš„å¹³å‡å¤§å°å°äºè¯¥å€¼æ—¶ï¼Œå¯åŠ¨ä¸€ä¸ªç‹¬ç«‹çš„map-reduceä»»åŠ¡è¿›è¡Œæ–‡ä»¶merge
SET hive.merge.smallfiles.avgsize = 16777216;
~~~



> åˆç†è®¾ç½®Reduceæ•°



**è°ƒæ•´reduceä¸ªæ•°æ–¹æ³•ä¸€**

~~~sql
ï¼ˆ1ï¼‰æ¯ä¸ªReduceå¤„ç†çš„æ•°æ®é‡é»˜è®¤æ˜¯256MB
hive.exec.reducers.bytes.per.reducer=256000000ï¼ˆå‚æ•°ä¸€ï¼‰
ï¼ˆ2ï¼‰æ¯ä¸ªä»»åŠ¡æœ€å¤§çš„reduceæ•°ï¼Œé»˜è®¤ä¸º1009
hive.exec.reducers.max=1009		ï¼ˆå‚æ•°äºŒï¼‰
ï¼ˆ3ï¼‰è®¡ç®—reduceræ•°çš„å…¬å¼
N=min(å‚æ•°2ï¼Œæ€»è¾“å…¥æ•°æ®é‡/å‚æ•°1)
~~~



**è°ƒæ•´reduceä¸ªæ•°æ–¹æ³•äºŒ**

~~~sql
åœ¨hadoopçš„mapred-default.xmlæ–‡ä»¶ä¸­ä¿®æ”¹
è®¾ç½®æ¯ä¸ªjobçš„Reduceä¸ªæ•°
set mapreduce.job.reduces = 15;
~~~



## å¹¶è¡Œæ‰§è¡Œ



**ğŸ“¢æ‰“å¼€ä»»åŠ¡å¹¶è¡Œæ‰§è¡Œ**

~~~sql
set hive.exec.parallel=true; 
~~~



**ğŸ“¢åŒä¸€ä¸ªsqlå…è®¸æœ€å¤§å¹¶è¡Œåº¦ï¼Œé»˜è®¤ä¸º8ã€‚**

~~~sql
set hive.exec.parallel.thread.number=16; 
~~~





## ä¸¥æ ¼æ¨¡å¼



**1**ï¼‰åˆ†åŒºè¡¨ä¸ä½¿ç”¨åˆ†åŒºè¿‡æ»¤

  å°†hive.strict.checks.no.partition.filterè®¾ç½®ä¸ºtrueæ—¶ï¼Œå¯¹äºåˆ†åŒºè¡¨ï¼Œé™¤éwhereè¯­å¥ä¸­å«æœ‰åˆ†åŒºå­—æ®µè¿‡æ»¤æ¡ä»¶æ¥é™åˆ¶èŒƒå›´ï¼Œå¦åˆ™ä¸å…è®¸æ‰§è¡Œã€‚æ¢å¥è¯è¯´ï¼Œå°±æ˜¯ç”¨æˆ·ä¸å…è®¸æ‰«ææ‰€æœ‰åˆ†åŒºã€‚è¿›è¡Œè¿™ä¸ªé™åˆ¶çš„åŸå› æ˜¯ï¼Œé€šå¸¸åˆ†åŒºè¡¨éƒ½æ‹¥æœ‰éå¸¸å¤§çš„æ•°æ®é›†ï¼Œè€Œä¸”æ•°æ®å¢åŠ è¿…é€Ÿã€‚æ²¡æœ‰è¿›è¡Œåˆ†åŒºé™åˆ¶çš„æŸ¥è¯¢å¯èƒ½ä¼šæ¶ˆè€—ä»¤äººä¸å¯æ¥å—çš„å·¨å¤§èµ„æºæ¥å¤„ç†è¿™ä¸ªè¡¨ã€‚
 **2**ï¼‰ä½¿ç”¨order byæ²¡æœ‰limitè¿‡æ»¤

 å°†hive.strict.checks.orderby.no.limitè®¾ç½®ä¸ºtrueæ—¶ï¼Œå¯¹äºä½¿ç”¨äº†order byè¯­å¥çš„æŸ¥è¯¢ï¼Œè¦æ±‚å¿…é¡»ä½¿ç”¨limitè¯­å¥ã€‚å› ä¸ºorder byä¸ºäº†æ‰§è¡Œæ’åºè¿‡ç¨‹ä¼šå°†æ‰€æœ‰çš„ç»“æœæ•°æ®åˆ†å‘åˆ°åŒä¸€ä¸ªReducerä¸­è¿›è¡Œå¤„ç†ï¼Œå¼ºåˆ¶è¦æ±‚ç”¨æˆ·å¢åŠ è¿™ä¸ªLIMITè¯­å¥å¯ä»¥é˜²æ­¢Reduceré¢å¤–æ‰§è¡Œå¾ˆé•¿ä¸€æ®µæ—¶é—´ã€‚

**3**ï¼‰ç¬›å¡å°”ç§¯

 å°†hive.strict.checks.cartesian.productè®¾ç½®ä¸ºtrueæ—¶ï¼Œä¼šé™åˆ¶ç¬›å¡å°”ç§¯çš„æŸ¥è¯¢ã€‚å¯¹å…³ç³»å‹æ•°æ®åº“éå¸¸äº†è§£çš„ç”¨æˆ·å¯èƒ½æœŸæœ›åœ¨ æ‰§è¡ŒJOINæŸ¥è¯¢çš„æ—¶å€™ä¸ä½¿ç”¨ONè¯­å¥è€Œæ˜¯ä½¿ç”¨whereè¯­å¥ï¼Œè¿™æ ·å…³ç³»æ•°æ®åº“çš„æ‰§è¡Œä¼˜åŒ–å™¨å°±å¯ä»¥é«˜æ•ˆåœ°å°†WHEREè¯­å¥è½¬åŒ–æˆé‚£ä¸ªONè¯­å¥ã€‚ä¸å¹¸çš„æ˜¯ï¼ŒHiveå¹¶ä¸ä¼šæ‰§è¡Œè¿™ç§ä¼˜åŒ–ï¼Œå› æ­¤ï¼Œå¦‚æœè¡¨è¶³å¤Ÿå¤§ï¼Œé‚£ä¹ˆè¿™ä¸ªæŸ¥è¯¢å°±ä¼šå‡ºç°ä¸å¯æ§çš„æƒ…





## JVMé‡ç”¨å’Œå‹ç¼©



ğŸ“¢JVMé‡ç”¨ç®€å•æ¥è¯´å°±æ˜¯ä¸€ç›´å¼€å¯å®¹å™¨ï¼Œé¿å…åå¤é‡å¯æ‰€å¢åŠ çš„JOBæ—¶é—´



ğŸ“¢Uberæ¨¡å¼ï¼š

åˆ¤æ–­Jobçš„å¤§å°å¦‚æœæ˜¯å°Jobåªéœ€å¼€å¯ä¸€ä¸ªå®¹å™¨  é¿å…èµ„æºæµªè´¹

ğŸ”—å¼€å¯Uberæ¨¡å¼

~~~sql
set mapreduce.job.ubertask.enable	/*é»˜è®¤false*/
~~~

**è¿›è¡Œä¸‰ä¸ªåˆ¤æ–­ğŸ‘‡**

Jobçš„Mapæ•°åˆ¤æ–­

~~~sql
mapreduce.job.ubertask.maxmaps     /*é»˜è®¤<=9  åªèƒ½å‘ä¸‹æ›´æ”¹*/
~~~

Jobçš„Reduceæ•°åˆ¤æ–­

~~~sql
mapreduce.job.ubertask.maxreduces		  /*é»˜è®¤<=1 åªèƒ½å‘ä¸‹æ›´æ”¹*/
~~~

Jobè®¡ç®—ä»»åŠ¡çš„æ•°æ®é‡åˆ¤æ–­

~~~sql
mapreduce.job.ubertask.maxbytes		  /*é»˜è®¤å—å¤§å°  */
~~~



å‹ç¼©è§ä¸Š













